,url,repository_url,labels_url,comments_url,events_url,html_url,id,node_id,number,title,labels,state,locked,assignee,assignees,milestone,comments,created_at,updated_at,closed_at,author_association,body,repo_name,owner,user.login,user.id,user.node_id,user.avatar_url,user.gravatar_id,user.url,user.html_url,user.followers_url,user.following_url,user.gists_url,user.starred_url,user.subscriptions_url,user.organizations_url,user.repos_url,user.events_url,user.received_events_url,user.type,user.site_admin,pull_request.url,pull_request.html_url,pull_request.diff_url,pull_request.patch_url
0,https://api.github.com/repos/apache/spark/issues/27002,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/27002/labels{/name},https://api.github.com/repos/apache/spark/issues/27002/comments,https://api.github.com/repos/apache/spark/issues/27002/events,https://github.com/apache/spark/pull/27002,542079733,MDExOlB1bGxSZXF1ZXN0MzU2NTkzNjY3,27002,[SPARK-30346]Improve logging when events dropped,[],open,False,,[],,2,2019-12-24T10:24:15Z,2019-12-24T10:29:45Z,,NONE,"
### What changes were proposed in this pull request?

1. Make logging events dropping every 60s works fine, the orignal implementaion some times not working due to susequent events comming and updating the DroppedEventCounter
2. Logging thread dump when the events dropping was logged every 60s.

### Why are the changes needed?

Currenly, the logging may be skipped and delayed a long time under high concurrency, that make debugging hard. So This PR will try to fix it.

Also this PR added logging for thread dump of dispatcher thread to help debugging performance issue that may causing events dropped.

### Does this PR introduce any user-facing change?

No


### How was this patch tested?

NA
",spark,apache,liupc,6747355,MDQ6VXNlcjY3NDczNTU=,https://avatars2.githubusercontent.com/u/6747355?v=4,,https://api.github.com/users/liupc,https://github.com/liupc,https://api.github.com/users/liupc/followers,https://api.github.com/users/liupc/following{/other_user},https://api.github.com/users/liupc/gists{/gist_id},https://api.github.com/users/liupc/starred{/owner}{/repo},https://api.github.com/users/liupc/subscriptions,https://api.github.com/users/liupc/orgs,https://api.github.com/users/liupc/repos,https://api.github.com/users/liupc/events{/privacy},https://api.github.com/users/liupc/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/27002,https://github.com/apache/spark/pull/27002,https://github.com/apache/spark/pull/27002.diff,https://github.com/apache/spark/pull/27002.patch
1,https://api.github.com/repos/apache/spark/issues/27001,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/27001/labels{/name},https://api.github.com/repos/apache/spark/issues/27001/comments,https://api.github.com/repos/apache/spark/issues/27001/events,https://github.com/apache/spark/pull/27001,542074866,MDExOlB1bGxSZXF1ZXN0MzU2NTg5NTU4,27001,[SPARK-30345][SQL] Fix intermittent test failure (ConnectException) on ThriftServerQueryTestSuite/ThriftServerWithSparkContextSuite,[],open,False,,[],,3,2019-12-24T10:08:25Z,2019-12-24T10:14:24Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch fixes the intermittent test failure on ThriftServerQueryTestSuite/ThriftServerWithSparkContextSuite, getting ConnectException when querying to thrift server.
(https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115646/testReport/)

The relevant unit test log messages are following:

```
19/12/23 13:33:01.875 pool-1-thread-1 INFO AbstractService: Service:ThriftBinaryCLIService is started.
19/12/23 13:33:01.875 pool-1-thread-1 INFO AbstractService: Service:HiveServer2 is started.
...
19/12/23 13:33:01.888 pool-1-thread-1 INFO ThriftServerWithSparkContextSuite: HiveThriftServer2 started successfully
...
19/12/23 13:33:01.909 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO ThriftServerWithSparkContextSuite:

===== TEST OUTPUT FOR o.a.s.sql.hive.thriftserver.ThriftServerWithSparkContextSuite: 'SPARK-29911: Uncache cached tables when session closed' =====

...
19/12/23 13:33:02.017 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO Utils: Supplied authorities: localhost:15441
19/12/23 13:33:02.018 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO Utils: Resolved authority: localhost:15441
19/12/23 13:33:02.078 HiveServer2-Background-Pool: Thread-213 INFO BaseSessionStateBuilder$$anon$2: Optimization rule 'org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation' is excluded from the optimizer.
19/12/23 13:33:02.078 HiveServer2-Background-Pool: Thread-213 INFO BaseSessionStateBuilder$$anon$2: Optimization rule 'org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation' is excluded from the optimizer.
19/12/23 13:33:02.121 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite WARN HiveConnection: Failed to connect to localhost:15441
19/12/23 13:33:02.124 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO ThriftServerWithSparkContextSuite:

===== FINISHED o.a.s.sql.hive.thriftserver.ThriftServerWithSparkContextSuite: 'SPARK-29911: Uncache cached tables when session closed' =====

19/12/23 13:33:02.143 Thread-35 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 15441 with 5...500 worker threads
19/12/23 13:33:02.327 pool-1-thread-1 INFO HiveServer2: Shutting down HiveServer2
19/12/23 13:33:02.328 pool-1-thread-1 INFO ThriftCLIService: Thrift server has stopped
```
(Here the error is logged as `WARN HiveConnection: Failed to connect to localhost:15441` - the actual stack trace can be seen on Jenkins test summary.)

The reason of test failure: Thrift(Binary|Http)CLIService prepare and launch the service asynchronously (in new thread), which suites are not waiting for completion and just start running tests, ends up with race condition.

That can be easily reproduced, via adding artificial sleep in `ThriftBinaryCLIService.run()` here:
https://github.com/apache/spark/blob/ba3f6330dd2b6054988f1f6f0ffe014fc4969088/sql/hive-thriftserver/v2.3/src/main/java/org/apache/hive/service/cli/thrift/ThriftBinaryCLIService.java#L49

(Note that `sleep` should be added before initializing server socket. E.g. Line 57)

This patch changes the test initialization logic to try executing simple query to wait until the service is available. The patch also refactors the code to apply the change both ThriftServerQueryTestSuite and ThriftServerWithSparkContextSuite easily.

### Why are the changes needed?

This patch fixes the intermittent failure observed here:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115646/testReport/

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Artificially made the test fail consistently (by the approach described above), and confirmed the patch fixed the test.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/27001,https://github.com/apache/spark/pull/27001,https://github.com/apache/spark/pull/27001.diff,https://github.com/apache/spark/pull/27001.patch
2,https://api.github.com/repos/apache/spark/issues/27000,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/27000/labels{/name},https://api.github.com/repos/apache/spark/issues/27000/comments,https://api.github.com/repos/apache/spark/issues/27000/events,https://github.com/apache/spark/pull/27000,542066640,MDExOlB1bGxSZXF1ZXN0MzU2NTgyODI2,27000,[SPARK-29224][ML]Implement Factorization Machines as a ml-pipeline component,[],open,False,,[],,2,2019-12-24T09:43:37Z,2019-12-24T10:23:35Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?

Implement Factorization Machines as a ml-pipeline component

1. loss function supports: logloss, mse
2. optimizer: GD, adamW


### Why are the changes needed?

Factorization Machines is widely used in advertising and recommendation system to estimate CTR(click-through rate).
Advertising and recommendation system usually has a lot of data, so we need Spark to estimate the CTR, and Factorization Machines are common ml model to estimate CTR.
References:

1. S. Rendle, ‚ÄúFactorization machines,‚Äù in Proceedings of IEEE International Conference on Data Mining (ICDM), pp. 995‚Äì1000, 2010.
https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf


### Does this PR introduce any user-facing change?

No

### How was this patch tested?

run unit tests
",spark,apache,mob-ai,55372231,MDQ6VXNlcjU1MzcyMjMx,https://avatars2.githubusercontent.com/u/55372231?v=4,,https://api.github.com/users/mob-ai,https://github.com/mob-ai,https://api.github.com/users/mob-ai/followers,https://api.github.com/users/mob-ai/following{/other_user},https://api.github.com/users/mob-ai/gists{/gist_id},https://api.github.com/users/mob-ai/starred{/owner}{/repo},https://api.github.com/users/mob-ai/subscriptions,https://api.github.com/users/mob-ai/orgs,https://api.github.com/users/mob-ai/repos,https://api.github.com/users/mob-ai/events{/privacy},https://api.github.com/users/mob-ai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/27000,https://github.com/apache/spark/pull/27000,https://github.com/apache/spark/pull/27000.diff,https://github.com/apache/spark/pull/27000.patch
3,https://api.github.com/repos/apache/spark/issues/26999,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26999/labels{/name},https://api.github.com/repos/apache/spark/issues/26999/comments,https://api.github.com/repos/apache/spark/issues/26999/events,https://github.com/apache/spark/pull/26999,542063357,MDExOlB1bGxSZXF1ZXN0MzU2NTgwMTU5,26999,[SPARK-29914][ML][FOLLOWUP] fix SQLTransformer & VectorSizeHint toString method,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-12-24T09:33:26Z,2019-12-24T09:39:21Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
1,modify the toString in SQLTransformer & VectorSizeHint
2,add toString in RegexTokenizer

### Why are the changes needed?
in SQLTransformer & VectorSizeHint , `toString` methods directly call getter of param without default values.
This will cause `java.util.NoSuchElementException` in REPL:
```scala
scala> val vs = new VectorSizeHint()
java.util.NoSuchElementException: Failed to find a default value for size
  at org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:780)
  at scala.Option.getOrElse(Option.scala:189)

```


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
existing testsuites
",spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26999,https://github.com/apache/spark/pull/26999,https://github.com/apache/spark/pull/26999.diff,https://github.com/apache/spark/pull/26999.patch
4,https://api.github.com/repos/apache/spark/issues/26998,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26998/labels{/name},https://api.github.com/repos/apache/spark/issues/26998/comments,https://api.github.com/repos/apache/spark/issues/26998/events,https://github.com/apache/spark/pull/26998,542053516,MDExOlB1bGxSZXF1ZXN0MzU2NTcyMTQ1,26998,[SPARK-25855][CORE][FOLLOW-UP] Format config name to follow the other boolean conf naming convention ,[],open,False,,[],,2,2019-12-24T09:02:52Z,2019-12-24T09:49:22Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Change config name from `spark.eventLog.allowErasureCoding` to `spark.eventLog.allowErasureCoding.enabled`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

To follow the other boolean conf naming convention.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No, it's newly added in Spark 3.0.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Tested manually and pass Jenkins.
",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26998,https://github.com/apache/spark/pull/26998,https://github.com/apache/spark/pull/26998.diff,https://github.com/apache/spark/pull/26998.patch
5,https://api.github.com/repos/apache/spark/issues/26997,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26997/labels{/name},https://api.github.com/repos/apache/spark/issues/26997/comments,https://api.github.com/repos/apache/spark/issues/26997/events,https://github.com/apache/spark/pull/26997,542037894,MDExOlB1bGxSZXF1ZXN0MzU2NTU5NDc3,26997,[SPARK-30343][SQL] Skip unnecessary checks in RewriteDistinctAggregates,[],open,False,,[],,1,2019-12-24T08:10:25Z,2019-12-24T08:15:07Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pr intends to skip the unnecessary checks that most aggregate quries don't need in RewriteDistinctAggregates.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For minor optimization.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing tests.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26997,https://github.com/apache/spark/pull/26997,https://github.com/apache/spark/pull/26997.diff,https://github.com/apache/spark/pull/26997.patch
6,https://api.github.com/repos/apache/spark/issues/26996,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26996/labels{/name},https://api.github.com/repos/apache/spark/issues/26996/comments,https://api.github.com/repos/apache/spark/issues/26996/events,https://github.com/apache/spark/pull/26996,542007433,MDExOlB1bGxSZXF1ZXN0MzU2NTM0NDcz,26996,[SPARK-30342][SQL][DOC]Update LIST FILE/JAR command Documentation,[],open,False,,[],,6,2019-12-24T06:17:24Z,2019-12-24T08:55:06Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Updated the document for LIST FILE/JAR command.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
LIST FILE/JAR can take multiple filenames as argument and it returns the files which were added as resources.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
Yes. Documentation updated for LIST FILE/JAR command
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Manually
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,iRakson,15366835,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26996,https://github.com/apache/spark/pull/26996,https://github.com/apache/spark/pull/26996.diff,https://github.com/apache/spark/pull/26996.patch
7,https://api.github.com/repos/apache/spark/issues/26995,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26995/labels{/name},https://api.github.com/repos/apache/spark/issues/26995/comments,https://api.github.com/repos/apache/spark/issues/26995/events,https://github.com/apache/spark/pull/26995,541978594,MDExOlB1bGxSZXF1ZXN0MzU2NTExNDQ4,26995,[SPARK-30341][SQL] Overflow check for interval arithmetic operations,[],open,False,,[],,9,2019-12-24T03:48:48Z,2019-12-24T08:38:02Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

1. For the interval arithmetic functions, e.g. `add`/`subtract`/`negative`/`multiply`/`divide`, enable overflow check when `ANSI` is on. This behavior fits the numeric type operations.

2. For `add`/`subtract`/`negative`, fix non-checked overflow to result in `NULL` when an overflow happens and `ANSI` is off as same as `multiply`/`divide`.

```sql
select interval '-2147483648 months' - interval '1 months';
178956970 years 7 months
```
```sql
select negative(interval '-2147483648 months');
-178956970 years -8 months
```
### Why are the changes needed?

1. bug fix
2. `ANSI` support

### Does this PR introduce any user-facing change?
yes,r `add`/`subtract`/`negative` result in `NULL`  if overflow


### How was this patch tested?

add unit tests",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26995,https://github.com/apache/spark/pull/26995,https://github.com/apache/spark/pull/26995.diff,https://github.com/apache/spark/pull/26995.patch
8,https://api.github.com/repos/apache/spark/issues/26994,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26994/labels{/name},https://api.github.com/repos/apache/spark/issues/26994/comments,https://api.github.com/repos/apache/spark/issues/26994/events,https://github.com/apache/spark/pull/26994,541965103,MDExOlB1bGxSZXF1ZXN0MzU2NTAxMjkz,26994,[SPARK-30339][SQL] Avoid to fail twice in function lookup,[],open,False,,[],,4,2019-12-24T02:38:15Z,2019-12-24T07:15:05Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Currently if function lookup fails, spark will give it a second change by casting decimal type to double type. But for cases where decimal type doesn't exist, it's meaningless to lookup again and causes extra cost like unnecessary metastore access. We should throw exceptions directly in these cases.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Covered by existing tests.",spark,apache,wzhfy,10878553,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26994,https://github.com/apache/spark/pull/26994,https://github.com/apache/spark/pull/26994.diff,https://github.com/apache/spark/pull/26994.patch
9,https://api.github.com/repos/apache/spark/issues/26993,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26993/labels{/name},https://api.github.com/repos/apache/spark/issues/26993/comments,https://api.github.com/repos/apache/spark/issues/26993/events,https://github.com/apache/spark/pull/26993,541961202,MDExOlB1bGxSZXF1ZXN0MzU2NDk4Mjk2,26993,[WIP][SPARK-30338][SQL] Avoid unnecessary InternalRow copies in ParquetRowConverter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-24T02:19:01Z,2019-12-24T10:01:47Z,,CONTRIBUTOR,"‚ö†Ô∏è üì£ This PR is a work in progress; I'm opening it early for discussion / feedback. I may continue to add additional unit test cases. I think there's also some existing-but-outdated code comments that I might want to clean up.

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR modifies `ParquetRowConverter` to remove unnecessary `InternalRow.copy()` calls for structs that are directly nested in other structs. 

### Why are the changes needed?

These changes  can significantly improve performance when reading Parquet files that contain deeply-nested structs with many fields.

The `ParquetRowConverter` uses per-field `Converter`s for handling individual fields. Internally, these converters may have mutable state and may return mutable objects. In most cases, each `converter` is only invoked once per Parquet record (this is true for top-level fields, for example). However, arrays and maps may call their child element converters multiple times per Parquet record: in these cases we must be careful to copy any mutable outputs returned by child converters.

In the existing code, `InternalRow`s are copied whenever they are stored into _any_ parent container (not just maps and arrays). This copying can be especially expensive for deeply-nested fields, since a deep copy is performed at every level of nesting.

This PR modifies the code to avoid copies for structs that are directly nested in structs; see inline code comments for an argument for why this is safe. 

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

**Correctness**:  I added new test cases to `ParquetIOSuite` to increase coverage of nested structs, including structs nested in arrays: previously this suite didn't test that case, so we used to lack mutation coverage of this `copy()` code (the suite's tests still passed if I incorrectly removed the `.copy()` in all cases).

**Performance**: I put together a simple local benchmark demonstrating the performance problems:

First, construct a nested schema:

```scala
  case class Inner(
    f1: Int,
    f2: Long,
    f3: String,
    f4: Int,
    f5: Long,
    f6: String,
    f7: Int,
    f8: Long,
    f9: String,
    f10: Int
  )
  
  case class Wrapper1(inner: Inner)
  case class Wrapper2(wrapper1: Wrapper1)
  case class Wrapper3(wrapper2: Wrapper2)
```

`Wrapper3`'s schema looks like:

```
root
 |-- wrapper2: struct (nullable = true)
 |    |-- wrapper1: struct (nullable = true)
 |    |    |-- inner: struct (nullable = true)
 |    |    |    |-- f1: integer (nullable = true)
 |    |    |    |-- f2: long (nullable = true)
 |    |    |    |-- f3: string (nullable = true)
 |    |    |    |-- f4: integer (nullable = true)
 |    |    |    |-- f5: long (nullable = true)
 |    |    |    |-- f6: string (nullable = true)
 |    |    |    |-- f7: integer (nullable = true)
 |    |    |    |-- f8: long (nullable = true)
 |    |    |    |-- f9: string (nullable = true)
 |    |    |    |-- f10: integer (nullable = true)
```

Next, generate some fake data:

```scala
  val data = spark.range(1, 1000 * 1000 * 25, 1, 1).map { i =>
    Wrapper3(Wrapper2(Wrapper1(Inner(
      i.toInt,
      i * 2,
      (i * 3).toString,
      (i * 4).toInt,
      i * 5,
      (i * 6).toString,
      (i * 7).toInt,
      i * 8,
      (i * 9).toString,
      (i * 10).toInt
    ))))
  }

  data.write.mode(""overwrite"").parquet(""/tmp/parquet-test"")
```

I then ran a simple benchmark consisting of

```
spark.read.parquet(""/tmp/parquet-test"").selectExpr(""hash(*)"").rdd.count()
```

where the `hash(*)` is designed to force decoding of all Parquet fields but avoids `RowEncoder` costs in the `.rdd.count()` stage.

In the old code, expensive copying takes place at every level of nesting; this is apparent in the following flame graph:

![image](https://user-images.githubusercontent.com/50748/71389014-88a15380-25af-11ea-9537-3e87a2aef179.png)

After this PR's changes, the above toy benchmark runs ~30% faster.",spark,apache,JoshRosen,50748,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26993,https://github.com/apache/spark/pull/26993,https://github.com/apache/spark/pull/26993.diff,https://github.com/apache/spark/pull/26993.patch
10,https://api.github.com/repos/apache/spark/issues/26992,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26992/labels{/name},https://api.github.com/repos/apache/spark/issues/26992/comments,https://api.github.com/repos/apache/spark/issues/26992/events,https://github.com/apache/spark/pull/26992,541948980,MDExOlB1bGxSZXF1ZXN0MzU2NDg4ODYz,26992,[SPARK-30337][SQL][SS] Change case class with vars to normal class in spark-sql-kafka module,[],open,False,,[],,1,2019-12-24T01:06:24Z,2019-12-24T02:48:11Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch changes case classes with vars to normal classes in spark-sql-kafka module, as it is allowed to use var in case class but discouraged. 

Quoting https://docs.scala-lang.org/tour/case-classes.html 

> You can‚Äôt reassign message1.sender because it is a val (i.e. immutable). It is possible to use vars in case classes but this is discouraged.

### Why are the changes needed?

That was from review comment (https://github.com/apache/spark/pull/22138#discussion_r302304877 / https://github.com/apache/spark/pull/22138#discussion_r302305192) and we have TODO comments to address later. It's time to address it and remove TODO comments.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26992,https://github.com/apache/spark/pull/26992,https://github.com/apache/spark/pull/26992.diff,https://github.com/apache/spark/pull/26992.patch
11,https://api.github.com/repos/apache/spark/issues/26991,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26991/labels{/name},https://api.github.com/repos/apache/spark/issues/26991/comments,https://api.github.com/repos/apache/spark/issues/26991/events,https://github.com/apache/spark/pull/26991,541944552,MDExOlB1bGxSZXF1ZXN0MzU2NDg1NDU5,26991,[SPARK-30336][SQL][SS] Move Kafka consumer-related classes to its own package,[],open,False,,[],,2,2019-12-24T00:36:20Z,2019-12-24T01:26:05Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

There're too many classes placed in a single package ""org.apache.spark.sql.kafka010"" which classes can be grouped by purpose.

As a part of change in SPARK-21869 (#26845), we moved out producer related classes to ""org.apache.spark.sql.kafka010.producer"" and only expose necessary classes/methods to the outside of package. This patch applies the same to consumer related classes.

### Why are the changes needed?

Described above.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26991,https://github.com/apache/spark/pull/26991,https://github.com/apache/spark/pull/26991.diff,https://github.com/apache/spark/pull/26991.patch
12,https://api.github.com/repos/apache/spark/issues/26990,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26990/labels{/name},https://api.github.com/repos/apache/spark/issues/26990/comments,https://api.github.com/repos/apache/spark/issues/26990/events,https://github.com/apache/spark/pull/26990,541943984,MDExOlB1bGxSZXF1ZXN0MzU2NDg1MDM0,26990,[SPARK-18409][ML][FOLLOWUP] LSH approxNearestNeighbors optimization,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-12-24T00:32:39Z,2019-12-24T09:46:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
compute count and quantile on one pass

### Why are the changes needed?
to avoid extra pass


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
existing testsuites
",spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26990,https://github.com/apache/spark/pull/26990,https://github.com/apache/spark/pull/26990.diff,https://github.com/apache/spark/pull/26990.patch
13,https://api.github.com/repos/apache/spark/issues/26987,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26987/labels{/name},https://api.github.com/repos/apache/spark/issues/26987/comments,https://api.github.com/repos/apache/spark/issues/26987/events,https://github.com/apache/spark/pull/26987,541841027,MDExOlB1bGxSZXF1ZXN0MzU2NDAwNTkx,26987,[SPARK-30334][SQL] Introduce as_json for marking a column as JSON data,[],open,False,,[],,1,2019-12-23T17:25:20Z,2019-12-23T23:18:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Semi-structured data is used widely in the data industry for reporting events in a wide variety of formats. Click events in product analytics can be stored as json. Some application logs can be in the form of delimited key=value text. Some data may be in xml.

The goal of this project is to be able to signal Spark that such a column exists. This will then enable Spark to ""auto-parse"" these columns on the fly. The proposal is to store this information as part of the column metadata, in the fields:
 - format: The format of the semi-structured column, e.g. json, xml, avro
 - options: Options for parsing these columns

This PR introduces the function ""as_json"", which accomplishes this for JSON columns.

### Why are the changes needed?

Simplify the handling of semi-structured columns in Spark, initially for JSON data.

### Does this PR introduce any user-facing change?

Introduces a new function called as_json

### How was this patch tested?

Unit tests",spark,apache,brkyvz,5243515,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26987,https://github.com/apache/spark/pull/26987,https://github.com/apache/spark/pull/26987.diff,https://github.com/apache/spark/pull/26987.patch
14,https://api.github.com/repos/apache/spark/issues/26984,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26984/labels{/name},https://api.github.com/repos/apache/spark/issues/26984/comments,https://api.github.com/repos/apache/spark/issues/26984/events,https://github.com/apache/spark/pull/26984,541684646,MDExOlB1bGxSZXF1ZXN0MzU2MjY5NTE5,26984,[SPARK-27641][CORE] Fix MetricsSystem to remove metrics of the specif‚Ä¶,[],open,False,,[],,1,2019-12-23T10:17:31Z,2019-12-23T20:39:06Z,,NONE,"## What changes were proposed in this pull request?

* Only remove the registered metrics  of the specific source when perform `MetricsSystem.removeSource`

## How was this patch tested?
* add new unit tests",spark,apache,chummyhe89,2225583,MDQ6VXNlcjIyMjU1ODM=,https://avatars3.githubusercontent.com/u/2225583?v=4,,https://api.github.com/users/chummyhe89,https://github.com/chummyhe89,https://api.github.com/users/chummyhe89/followers,https://api.github.com/users/chummyhe89/following{/other_user},https://api.github.com/users/chummyhe89/gists{/gist_id},https://api.github.com/users/chummyhe89/starred{/owner}{/repo},https://api.github.com/users/chummyhe89/subscriptions,https://api.github.com/users/chummyhe89/orgs,https://api.github.com/users/chummyhe89/repos,https://api.github.com/users/chummyhe89/events{/privacy},https://api.github.com/users/chummyhe89/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26984,https://github.com/apache/spark/pull/26984,https://github.com/apache/spark/pull/26984.diff,https://github.com/apache/spark/pull/26984.patch
15,https://api.github.com/repos/apache/spark/issues/26983,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26983/labels{/name},https://api.github.com/repos/apache/spark/issues/26983/comments,https://api.github.com/repos/apache/spark/issues/26983/events,https://github.com/apache/spark/pull/26983,541666143,MDExOlB1bGxSZXF1ZXN0MzU2MjU0MjY4,26983,[SPARK-30331][SQL]  Set isFinalPlan to true before posting the final AdaptiveSparkPlan event,[],open,False,,[],,4,2019-12-23T09:31:43Z,2019-12-24T08:40:42Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Set `isFinalPlan=true` before posting the final AdaptiveSparkPlan event (`SparkListenerSQLAdaptiveExecutionUpdate`)

### Why are the changes needed?

Otherwise, any attempt to listen on the final event by pattern matching `isFinalPlan=true` would fail

### Does this PR introduce any user-facing change?

No.


### How was this patch tested?

All tests in `AdaptiveQueryExecSuite` are exteneded with a verification that a `SparkListenerSQLAdaptiveExecutionUpdate` event with `isFinalPlan=True` exists
",spark,apache,manuzhang,1191767,MDQ6VXNlcjExOTE3Njc=,https://avatars3.githubusercontent.com/u/1191767?v=4,,https://api.github.com/users/manuzhang,https://github.com/manuzhang,https://api.github.com/users/manuzhang/followers,https://api.github.com/users/manuzhang/following{/other_user},https://api.github.com/users/manuzhang/gists{/gist_id},https://api.github.com/users/manuzhang/starred{/owner}{/repo},https://api.github.com/users/manuzhang/subscriptions,https://api.github.com/users/manuzhang/orgs,https://api.github.com/users/manuzhang/repos,https://api.github.com/users/manuzhang/events{/privacy},https://api.github.com/users/manuzhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26983,https://github.com/apache/spark/pull/26983,https://github.com/apache/spark/pull/26983.diff,https://github.com/apache/spark/pull/26983.patch
16,https://api.github.com/repos/apache/spark/issues/26982,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26982/labels{/name},https://api.github.com/repos/apache/spark/issues/26982/comments,https://api.github.com/repos/apache/spark/issues/26982/events,https://github.com/apache/spark/pull/26982,541620673,MDExOlB1bGxSZXF1ZXN0MzU2MjE2NDEw,26982,[SPARK-30329][ML] add iterator/foreach methods for Vectors,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-12-23T07:32:11Z,2019-12-24T04:02:53Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
1, add new foreach-like methods: foreach/foreachNonZero
2, add iterator: iterator/activeIterator/nonZeroIterator

### Why are the changes needed?
see the [ticke](https://issues.apache.org/jira/browse/SPARK-30329) for details
foreach/foreachNonZero: for both convenience and performace (SparseVector.foreach should be faster than current traversal method)
iterator/activeIterator/nonZeroIterator: add the three iterators, so that we can futuremore add/change some impls based on those iterators for both ml and mllib sides, to avoid vector conversions.

### Does this PR introduce any user-facing change?
Yes, new methods are added

### How was this patch tested?
added testsuites",spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26982,https://github.com/apache/spark/pull/26982,https://github.com/apache/spark/pull/26982.diff,https://github.com/apache/spark/pull/26982.patch
17,https://api.github.com/repos/apache/spark/issues/26981,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26981/labels{/name},https://api.github.com/repos/apache/spark/issues/26981/comments,https://api.github.com/repos/apache/spark/issues/26981/events,https://github.com/apache/spark/pull/26981,541554595,MDExOlB1bGxSZXF1ZXN0MzU2MTYyMzUx,26981,[SPARK-26389][SS][FOLLOW-UP]Format config name to follow the other boolean conf naming convention,[],open,False,,[],,15,2019-12-23T03:29:26Z,2019-12-24T10:19:13Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Rename `spark.sql.streaming.forceDeleteTempCheckpointLocation` to `spark.sql.streaming.forceDeleteTempCheckpointLocation.enabled`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

To follow the other boolean conf naming convention.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No, as this config is newly added in 3.0.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Pass Jenkins.",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26981,https://github.com/apache/spark/pull/26981,https://github.com/apache/spark/pull/26981.diff,https://github.com/apache/spark/pull/26981.patch
18,https://api.github.com/repos/apache/spark/issues/26980,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26980/labels{/name},https://api.github.com/repos/apache/spark/issues/26980/comments,https://api.github.com/repos/apache/spark/issues/26980/events,https://github.com/apache/spark/pull/26980,541468506,MDExOlB1bGxSZXF1ZXN0MzU2MDk4NzE1,26980,[SPARK-27348][Core] HeartbeatReceiver should remove lost executors from CoarseGrainedSchedulerBackend,[],open,False,,[],,14,2019-12-22T15:59:43Z,2019-12-24T08:43:07Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Remove it from `CoarseGrainedSchedulerBackend` when `HeartbeatReceiver` recognizes a lost executor.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Currently, an application may hang if we don't remove a lost executor from `CoarseGrainedSchedulerBackend` as it may happens due to:

1) In `expireDeadHosts()`, `HeartbeatReceiver` calls `scheduler.executorLost()`;

2) Before `HeartbeatReceiver` calls `sc.killAndReplaceExecutor()`(which would mark the lost executor as ""pendingToRemove"") in a separate thread,  `CoarseGrainedSchedulerBackend` may begins to launch tasks on that executor without realizing it has been lost indeed.

3) If that lost executor doesn't shut down gracefully, `CoarseGrainedSchedulerBackend ` may never receive a disconnect event. As a result, tasks launched on that lost executor become orphans. While at the same time, driver just thinks that those tasks are still running and waits forever.

Removing the lost executor from `CoarseGrainedSchedulerBackend` would let `TaskSetManager` mark those tasks as failed which avoids app hang. Furthermore, it cleans up records in `executorDataMap`, which may never be removed in such case.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Updated existed tests.

Close #24350.",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26980,https://github.com/apache/spark/pull/26980,https://github.com/apache/spark/pull/26980.diff,https://github.com/apache/spark/pull/26980.patch
19,https://api.github.com/repos/apache/spark/issues/26978,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26978/labels{/name},https://api.github.com/repos/apache/spark/issues/26978/comments,https://api.github.com/repos/apache/spark/issues/26978/events,https://github.com/apache/spark/pull/26978,541410574,MDExOlB1bGxSZXF1ZXN0MzU2MDU3NDIy,26978,[SPARK-29721][SQL] Prune unnecessary nested fields from Generate without Project,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-22T05:33:13Z,2019-12-24T04:37:11Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This patch proposes to prune unnecessary nested fields from Generate which has no Project on top of it.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

In Optimizer, we can prune nested columns from Project(projectList, Generate). However, unnecessary columns could still possibly be read in Generate, if no Project on top of it. We should prune it too.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit test.
",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26978,https://github.com/apache/spark/pull/26978,https://github.com/apache/spark/pull/26978.diff,https://github.com/apache/spark/pull/26978.patch
20,https://api.github.com/repos/apache/spark/issues/26977,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26977/labels{/name},https://api.github.com/repos/apache/spark/issues/26977/comments,https://api.github.com/repos/apache/spark/issues/26977/events,https://github.com/apache/spark/pull/26977,541389583,MDExOlB1bGxSZXF1ZXN0MzU2MDQyNTY3,26977,[SPARK-30326][SQL] Raise exception if analyzer exceed max iterations,[],open,False,,[],,4,2019-12-22T00:32:32Z,2019-12-23T20:39:05Z,,NONE,"### What changes were proposed in this pull request?
Enhance RuleExecutor strategy to take different actions when exceeding max iterations. And raise exception if analyzer exceed max iterations.

### Why are the changes needed?
Currently, both analyzer and optimizer just log warning message if rule execution exceed max iterations. They should have different behavior. Analyzer should raise exception to indicates the plan is not fixed after max iterations, while optimizer just log warning to keep the current plan. This is more feasible after SPARK-30138 was introduced.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
AnalyzerRuleStrategySuite added to test this change.
",spark,apache,Eric5553,10626956,MDQ6VXNlcjEwNjI2OTU2,https://avatars1.githubusercontent.com/u/10626956?v=4,,https://api.github.com/users/Eric5553,https://github.com/Eric5553,https://api.github.com/users/Eric5553/followers,https://api.github.com/users/Eric5553/following{/other_user},https://api.github.com/users/Eric5553/gists{/gist_id},https://api.github.com/users/Eric5553/starred{/owner}{/repo},https://api.github.com/users/Eric5553/subscriptions,https://api.github.com/users/Eric5553/orgs,https://api.github.com/users/Eric5553/repos,https://api.github.com/users/Eric5553/events{/privacy},https://api.github.com/users/Eric5553/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26977,https://github.com/apache/spark/pull/26977,https://github.com/apache/spark/pull/26977.diff,https://github.com/apache/spark/pull/26977.patch
21,https://api.github.com/repos/apache/spark/issues/26975,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26975/labels{/name},https://api.github.com/repos/apache/spark/issues/26975/comments,https://api.github.com/repos/apache/spark/issues/26975/events,https://github.com/apache/spark/pull/26975,541306266,MDExOlB1bGxSZXF1ZXN0MzU1OTg2Mzkx,26975,[SPARK-30325][CORE] Stage retry and executor crash cause app hung up forever,[],open,False,,[],,15,2019-12-21T09:29:53Z,2019-12-24T10:17:56Z,,NONE,"### **What changes were proposed in this pull request?**
 Fix a bug can cause app hung.
The bug code analysis and discuss here:
https://issues.apache.org/jira/browse/SPARK-30325
The bugs occurs in the corer case as follows:
1. The stage occurs for fetchFailed and some task hasn't finished, scheduler will resubmit a new stage as retry with those unfinished tasks.
2. The unfinished task in origin stage finished and the same task on the new retry stage hasn't finished, it will mark the task partition on the new retry stage as succesuful. 
3. The executor running those 'successful task' crashed, it cause taskSetManager run executorLost to rescheduler the task on the executor, here will cause copiesRunning decreate 1 twice, beause those 'successful task' are not finished, it only status is successful but finished time undefined for the origin stage mark it status only, the variable copiesRunning will decreate to -1 as result.
4. 'dequeueTaskFromList' will use copiesRunning equal 0 as reschedule basis when rescheduler tasks, and now it is -1, can't to reschedule, and the app will hung forever.

Kill tasks which succeeded in origin stage when new retry stage has started the same task and hasn't finished can alse reduce stage execution time and resouce cost, too.

### **Why are the changes needed?**
This will cause app hung up.

### **Does this PR introduce any user-facing change?**
No

### **How was this patch tested?**",spark,apache,seayoun,45163307,MDQ6VXNlcjQ1MTYzMzA3,https://avatars3.githubusercontent.com/u/45163307?v=4,,https://api.github.com/users/seayoun,https://github.com/seayoun,https://api.github.com/users/seayoun/followers,https://api.github.com/users/seayoun/following{/other_user},https://api.github.com/users/seayoun/gists{/gist_id},https://api.github.com/users/seayoun/starred{/owner}{/repo},https://api.github.com/users/seayoun/subscriptions,https://api.github.com/users/seayoun/orgs,https://api.github.com/users/seayoun/repos,https://api.github.com/users/seayoun/events{/privacy},https://api.github.com/users/seayoun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26975,https://github.com/apache/spark/pull/26975,https://github.com/apache/spark/pull/26975.diff,https://github.com/apache/spark/pull/26975.patch
22,https://api.github.com/repos/apache/spark/issues/26974,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26974/labels{/name},https://api.github.com/repos/apache/spark/issues/26974/comments,https://api.github.com/repos/apache/spark/issues/26974/events,https://github.com/apache/spark/pull/26974,541158925,MDExOlB1bGxSZXF1ZXN0MzU1ODY4MjM3,26974,[SPARK-30324][SQL] Simplify JSON field access through dot notation,[],open,False,,[],,4,2019-12-20T19:24:24Z,2019-12-24T09:24:30Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Allows JSON strings to be parsed through dot and bracket notation instead of having to write ""get_json_object"". We add a feature flag to control this behavior.

The column needs to be a JSON object for this to work. JSON arrays are not supported and would return nulls. If a column is a JSON array, e.g. `[{""a"":1},{""a"":2},...]`, `explode(from_json(colName, expr(""array<string>"")))` can be used to first explode the array into the JSON objects.

While the JSON column can be resolved case insensitively, the JSON fields are case sensitive.
For example, imagine the column `foo`:
```
{""a"":""bar"", ""b"":{""field1"":""v1""}}
```

`foo.b.field1`, `FOO.b.field1` would work, but `foo.B.FIELD1` would return a null value.

### Why are the changes needed?

get_json_object is a hard to use and verbose API. This API would greatly help data engineers store their data as raw json columns, and data analysts access the fields that they care about, without having to deal with complicated UDFs, and JSON parsing code.

### Does this PR introduce any user-facing change?

Yes, StringType fields that contain JSON now will be accessible through dot notation.


### How was this patch tested?

Unit tests",spark,apache,brkyvz,5243515,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26974,https://github.com/apache/spark/pull/26974,https://github.com/apache/spark/pull/26974.diff,https://github.com/apache/spark/pull/26974.patch
23,https://api.github.com/repos/apache/spark/issues/26973,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26973/labels{/name},https://api.github.com/repos/apache/spark/issues/26973/comments,https://api.github.com/repos/apache/spark/issues/26973/events,https://github.com/apache/spark/pull/26973,541151591,MDExOlB1bGxSZXF1ZXN0MzU1ODYyMTE1,26973,[SPARK-30323][SQL] Support filters pushdown in CSV datasource,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-12-20T19:03:37Z,2019-12-24T10:28:51Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

In the PR, I propose to support pushed down filters in CSV datasource. The reason of pushing a filter up to `UnivocityParser` is to apply the filter as soon as all its attributes become available i.e. converted from CSV fields to desired values according to the schema. This allows to skip conversions of other values if the filter returns `false`. This can improve performance when pushed filters are highly selective and conversion of CSV string fields to desired values are comparably expensive ( for example, conversion to `TIMESTAMP` values).

Here are details of the implementation:
- `UnivocityParser.convert()` converts parsed CSV tokens one-by-one sequentially starting from index 0 up to `parsedSchema.length - 1`. At current index `i`, it applies filters that refer to attributes at row fields indexes `0..i`. If any filter returns `false`, it skips conversions of other input tokens.
- Pushed filters are converted to expressions. The expressions are bound to row positions according to `CSVFilters.readSchema`. The expressions are compiled to predicates via generating Java code.
- To be able to apply predicates to partially initialized rows, the predicates are grouped, and combined via the `And` expression. Final predicate at index `N` can refer to row fields at the positions `0..N`, and can be applied to a row even if other fields at the positions `N+1..readSchema.lenght-1` are not set.

### Why are the changes needed?
The changes improve performance on synthetic benchmarks more **than 9 times** (on JDK 8 & 11):
```
OpenJDK 64-Bit Server VM 11.0.5+10 on Mac OS X 10.15.2
Intel(R) Core(TM) i7-4850HQ CPU @ 2.30GHz
Filters pushdown:                         Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
w/o filters                                       11889          11945          52          0.0      118893.1       1.0X
pushdown disabled                                 11790          11860         115          0.0      117902.3       1.0X
w/ filters                                         1240           1278          33          0.1       12400.8       9.6X
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- Added new test suite `CSVFiltersSuite`
- Added tests to `CSVSuite` and `UnivocityParserSuite`",spark,apache,MaxGekk,1580697,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26973,https://github.com/apache/spark/pull/26973,https://github.com/apache/spark/pull/26973.diff,https://github.com/apache/spark/pull/26973.patch
24,https://api.github.com/repos/apache/spark/issues/26972,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26972/labels{/name},https://api.github.com/repos/apache/spark/issues/26972/comments,https://api.github.com/repos/apache/spark/issues/26972/events,https://github.com/apache/spark/pull/26972,541073384,MDExOlB1bGxSZXF1ZXN0MzU1Nzk1Nzc5,26972,[SPARK-30321][Ml] Log weightSum in Algo that has weights support,[],open,False,,[],,6,2019-12-20T16:15:30Z,2019-12-24T01:54:14Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
add instr.logSumOfWeights in the Algo that has weightCol support


### Why are the changes needed?
Many algorithms support weightCol now. I think weightsum is useful info to add to the log. 


### Does this PR introduce any user-facing change?
no


### How was this patch tested?
manually tested
",spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26972,https://github.com/apache/spark/pull/26972,https://github.com/apache/spark/pull/26972.diff,https://github.com/apache/spark/pull/26972.patch
25,https://api.github.com/repos/apache/spark/issues/26971,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26971/labels{/name},https://api.github.com/repos/apache/spark/issues/26971/comments,https://api.github.com/repos/apache/spark/issues/26971/events,https://github.com/apache/spark/pull/26971,541069220,MDExOlB1bGxSZXF1ZXN0MzU1NzkyMzA2,26971,[SPARK-30320][SQL] Fix insert overwrite to DataSource table with dynamic partition error,[],open,False,,[],,3,2019-12-20T16:06:29Z,2019-12-23T10:57:38Z,,NONE,"### What changes were proposed in this pull request?
Task attempt id is append to dynamic partition staging dir, commitTask in HadoopMapReduceCommitProtocol carries attempt id in TaskCommitMessage

### Why are the changes needed?
This PR fix insert overwrite to DataSource table with dynamic partition error when running multiple task attempts. suppose there are one task attempt and one speculative task attempt, the speculative would raise FileAlreadyExistsException because of same staging dir attempt tasks commit

### Does this PR introduce any user-facing change?
This PR add a configuration 'spark.max.local.task.failures' to set max failure time in LOCAL mode, default 1

### How was this patch tested?
Added UT
",spark,apache,WinkerDu,11454734,MDQ6VXNlcjExNDU0NzM0,https://avatars0.githubusercontent.com/u/11454734?v=4,,https://api.github.com/users/WinkerDu,https://github.com/WinkerDu,https://api.github.com/users/WinkerDu/followers,https://api.github.com/users/WinkerDu/following{/other_user},https://api.github.com/users/WinkerDu/gists{/gist_id},https://api.github.com/users/WinkerDu/starred{/owner}{/repo},https://api.github.com/users/WinkerDu/subscriptions,https://api.github.com/users/WinkerDu/orgs,https://api.github.com/users/WinkerDu/repos,https://api.github.com/users/WinkerDu/events{/privacy},https://api.github.com/users/WinkerDu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26971,https://github.com/apache/spark/pull/26971,https://github.com/apache/spark/pull/26971.diff,https://github.com/apache/spark/pull/26971.patch
26,https://api.github.com/repos/apache/spark/issues/26970,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26970/labels{/name},https://api.github.com/repos/apache/spark/issues/26970/comments,https://api.github.com/repos/apache/spark/issues/26970/events,https://github.com/apache/spark/pull/26970,541063590,MDExOlB1bGxSZXF1ZXN0MzU1Nzg3NTkx,26970,[SPARK-28825][SQL][DOC] Documentation for Explain Command,[],open,False,,[],,2,2019-12-20T15:54:33Z,2019-12-23T23:56:54Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Document Explain statement in SQL Reference Guide.

## Why are the changes needed?
Adding documentation for SQL reference.

## Does this PR introduce any user-facing change?
yes

Before:
There was no documentation for this.
After:
![image (15)](https://user-images.githubusercontent.com/51401130/71266718-fdd21600-236e-11ea-801d-698fbcdc2e71.png)
![image (14)](https://user-images.githubusercontent.com/51401130/71266720-fe6aac80-236e-11ea-86e9-777a594c9004.png)
![image (13)](https://user-images.githubusercontent.com/51401130/71266721-fe6aac80-236e-11ea-9454-65fbfc70c455.png)
![image (12)](https://user-images.githubusercontent.com/51401130/71266722-fe6aac80-236e-11ea-84e3-1a2431228b23.png)
![image (11)](https://user-images.githubusercontent.com/51401130/71266724-ff034300-236e-11ea-9095-e5e0115c5d75.png)

## How was this patch tested?
Used jekyll build and serve to verify.



",spark,apache,PavithraRamachandran,51401130,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26970,https://github.com/apache/spark/pull/26970,https://github.com/apache/spark/pull/26970.diff,https://github.com/apache/spark/pull/26970.patch
27,https://api.github.com/repos/apache/spark/issues/26969,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26969/labels{/name},https://api.github.com/repos/apache/spark/issues/26969/comments,https://api.github.com/repos/apache/spark/issues/26969/events,https://github.com/apache/spark/pull/26969,541046962,MDExOlB1bGxSZXF1ZXN0MzU1NzczNzI5,26969,[SPARK-30319][SQL] Add a stricter version of `as[T]`,[],open,False,,[],,1,2019-12-20T15:19:42Z,2019-12-20T16:17:06Z,,NONE,"### What changes were proposed in this pull request?
Some aspects of `as[T]` are not intuitive and expected behaviour is not provided elsewhere:
* Extra columns that are not part of the type `T` are not dropped.
* Order of columns is not aligned with schema of `T`.
* Columns are not cast to the types of `T`'s fields. They have to be cast explicitly.

**This PR adds a stricter version of `as[T]` to `Dataset`.** 

### Why are the changes needed?
The behaviour of `as[T]` is not intuitive when you read code like `df.as[T].write.csv(""data.csv"")`. The result depends on the actual schema of `df`, where `def as[T](): Dataset[T]` should be agnostic to the schema of `df`.

A method that enforces schema of `T` on a given Dataset would be very convenient and allows to articulate and guarantee above assumptions about your data with the native Spark Dataset API. This method plays a more explicit and enforcing role than `as[T]` with respect to columns, column order and column type.

### Does this PR introduce any user-facing change?
Yes, it adds a new method to `Dataset`. It does not touch the existing `as[T]`.

Possible naming of a stricter version of `as[T]`:
* `as[T](strict = true)`
* `toDS[T]` (as in `toDF`)
* `selectAs[T]` (as this is merely selecting the columns of schema `T`)

This PR chooses the `toDS[T]` naming.

### How was this patch tested?
Existing tests for `as[T]` have been extended to assert the actual schema and emphasize the differences between `as[T]` and `toDS[T]`. Tests for `toDS[T]` are based on the `as[T]` tests.",spark,apache,EnricoMi,44700269,MDQ6VXNlcjQ0NzAwMjY5,https://avatars1.githubusercontent.com/u/44700269?v=4,,https://api.github.com/users/EnricoMi,https://github.com/EnricoMi,https://api.github.com/users/EnricoMi/followers,https://api.github.com/users/EnricoMi/following{/other_user},https://api.github.com/users/EnricoMi/gists{/gist_id},https://api.github.com/users/EnricoMi/starred{/owner}{/repo},https://api.github.com/users/EnricoMi/subscriptions,https://api.github.com/users/EnricoMi/orgs,https://api.github.com/users/EnricoMi/repos,https://api.github.com/users/EnricoMi/events{/privacy},https://api.github.com/users/EnricoMi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26969,https://github.com/apache/spark/pull/26969,https://github.com/apache/spark/pull/26969.diff,https://github.com/apache/spark/pull/26969.patch
28,https://api.github.com/repos/apache/spark/issues/26965,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26965/labels{/name},https://api.github.com/repos/apache/spark/issues/26965/comments,https://api.github.com/repos/apache/spark/issues/26965/events,https://github.com/apache/spark/pull/26965,540900037,MDExOlB1bGxSZXF1ZXN0MzU1NjQ5NzYw,26965,[SPARK-30330][SQL]Support single quotes json parsing for get_json_object and json_tuple,[],open,False,,[],,10,2019-12-20T09:51:49Z,2019-12-24T10:01:51Z,,NONE,"### What changes were proposed in this pull request?

I execute some query as` select get_json_object(ytag, '$.y1') AS y1 from t4`; SparkSQL return null but  Hive return correct results.
In my production environment, ytag is a json wrapped by single quotes,as follows
```
{'y1': 'shuma', 'y2': 'shuma:shouji'}
{'y1': 'jiaoyu', 'y2': 'jiaoyu:gaokao'}
{'y1': 'yule', 'y2': 'yule:mingxing'}
```
Then l realized some functions including get_json_object and json_tuple does not support  single quotes json parsing.
So l provide this PR to resolve the question.

### Why are the changes needed?

Enabled for Hive compatibility

### Does this PR introduce any user-facing change?

NO

### How was this patch tested?

NEW TESTS
",spark,apache,wenfang6,39544641,MDQ6VXNlcjM5NTQ0NjQx,https://avatars3.githubusercontent.com/u/39544641?v=4,,https://api.github.com/users/wenfang6,https://github.com/wenfang6,https://api.github.com/users/wenfang6/followers,https://api.github.com/users/wenfang6/following{/other_user},https://api.github.com/users/wenfang6/gists{/gist_id},https://api.github.com/users/wenfang6/starred{/owner}{/repo},https://api.github.com/users/wenfang6/subscriptions,https://api.github.com/users/wenfang6/orgs,https://api.github.com/users/wenfang6/repos,https://api.github.com/users/wenfang6/events{/privacy},https://api.github.com/users/wenfang6/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26965,https://github.com/apache/spark/pull/26965,https://github.com/apache/spark/pull/26965.diff,https://github.com/apache/spark/pull/26965.patch
29,https://api.github.com/repos/apache/spark/issues/26961,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26961/labels{/name},https://api.github.com/repos/apache/spark/issues/26961/comments,https://api.github.com/repos/apache/spark/issues/26961/events,https://github.com/apache/spark/pull/26961,540833684,MDExOlB1bGxSZXF1ZXN0MzU1NTkyMjQx,26961,[SPARK-29708][SQL] Correct aggregated values when grouping sets are duplicated,[],open,False,,[],,5,2019-12-20T07:51:40Z,2019-12-20T22:48:18Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pr intends to fix wrong aggregated values in `GROUPING SETS` when there are duplicated grouping sets in a query (e.g., `GROUPING SETS ((k1),(k1))`).

For example;
```
scala> spark.table(""t"").show()
+---+---+---+
| k1| k2|  v|
+---+---+---+
|  0|  0|  3|
+---+---+---+

scala> sql(""""""select grouping_id(), k1, k2, sum(v) from t group by grouping sets ((k1),(k1,k2),(k2,k1),(k1,k2))"""""").show()
+-------------+---+----+------+                                                 
|grouping_id()| k1|  k2|sum(v)|
+-------------+---+----+------+
|            0|  0|   0|     9| <---- wrong aggregate value and the correct answer is `3`
|            1|  0|null|     3|
+-------------+---+----+------+

// PostgreSQL case
postgres=#  select k1, k2, sum(v) from t group by grouping sets ((k1),(k1,k2),(k2,k1),(k1,k2));
 k1 |  k2  | sum 
----+------+-----
  0 |    0 |   3
  0 |    0 |   3
  0 |    0 |   3
  0 | NULL |   3
(4 rows)

// Hive case
hive> select GROUPING__ID, k1, k2, sum(v) from t group by k1, k2 grouping sets ((k1),(k1,k2),(k2,k1),(k1,k2));
1	0	NULL	3
0	0	0	3
```
This pr adds code to dedupe grouping sets in the Analyzer.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To fix bugs.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
The existing tests.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26961,https://github.com/apache/spark/pull/26961,https://github.com/apache/spark/pull/26961.diff,https://github.com/apache/spark/pull/26961.patch
30,https://api.github.com/repos/apache/spark/issues/26959,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26959/labels{/name},https://api.github.com/repos/apache/spark/issues/26959/comments,https://api.github.com/repos/apache/spark/issues/26959/events,https://github.com/apache/spark/pull/26959,540742781,MDExOlB1bGxSZXF1ZXN0MzU1NTEzMTg0,26959,[SPARK-30315][SQL] Add adaptive execution context,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-12-20T04:08:05Z,2019-12-21T22:35:56Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This is a minor code refactoring PR. It creates an adaptive execution context class to wrap objects shared across main query and sub-queries.

### Why are the changes needed?
This refactoring will improve code readability and reduce the number of parameters used to initialize `AdaptiveSparkPlanExec`.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Passed existing UTs.",spark,apache,maryannxue,4171904,MDQ6VXNlcjQxNzE5MDQ=,https://avatars3.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26959,https://github.com/apache/spark/pull/26959,https://github.com/apache/spark/pull/26959.diff,https://github.com/apache/spark/pull/26959.patch
31,https://api.github.com/repos/apache/spark/issues/26957,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26957/labels{/name},https://api.github.com/repos/apache/spark/issues/26957/comments,https://api.github.com/repos/apache/spark/issues/26957/events,https://github.com/apache/spark/pull/26957,540694965,MDExOlB1bGxSZXF1ZXN0MzU1NDcxNTkx,26957,[WIP][SPARK-30314] Add identifier and catalog information to DataSourceV2Relation,[],open,False,,[],,4,2019-12-20T01:56:33Z,2019-12-24T05:29:36Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Add identifier and catalog information in DataSourceV2Relation so it would be possible to do richer checks in checkAnalysis step.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,yuchenhuo,37087310,MDQ6VXNlcjM3MDg3MzEw,https://avatars2.githubusercontent.com/u/37087310?v=4,,https://api.github.com/users/yuchenhuo,https://github.com/yuchenhuo,https://api.github.com/users/yuchenhuo/followers,https://api.github.com/users/yuchenhuo/following{/other_user},https://api.github.com/users/yuchenhuo/gists{/gist_id},https://api.github.com/users/yuchenhuo/starred{/owner}{/repo},https://api.github.com/users/yuchenhuo/subscriptions,https://api.github.com/users/yuchenhuo/orgs,https://api.github.com/users/yuchenhuo/repos,https://api.github.com/users/yuchenhuo/events{/privacy},https://api.github.com/users/yuchenhuo/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26957,https://github.com/apache/spark/pull/26957,https://github.com/apache/spark/pull/26957.diff,https://github.com/apache/spark/pull/26957.patch
32,https://api.github.com/repos/apache/spark/issues/26956,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26956/labels{/name},https://api.github.com/repos/apache/spark/issues/26956/comments,https://api.github.com/repos/apache/spark/issues/26956/events,https://github.com/apache/spark/pull/26956,540603304,MDExOlB1bGxSZXF1ZXN0MzU1MzkwNjM4,26956,[SPARK-30312][SQL] Preserve path attributes including permission when truncate table,[],open,False,,[],,3,2019-12-19T22:12:43Z,2019-12-20T04:01:08Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This patch proposes to preserve few attributes including owner/group/permission/acls of paths when truncate table/partition.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

When Spark SQL truncates table, it deletes the paths of table/partitions, then re-create new ones. If some attributes such as permission/acls are set on the paths, the attributes will be deleted.

We should preserve the permission/acls if possible.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

Yes. When truncate table/partition, Spark will keep attributes including owner/group/permission/acls of paths.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit test.",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26956,https://github.com/apache/spark/pull/26956,https://github.com/apache/spark/pull/26956.diff,https://github.com/apache/spark/pull/26956.patch
33,https://api.github.com/repos/apache/spark/issues/26955,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26955/labels{/name},https://api.github.com/repos/apache/spark/issues/26955/comments,https://api.github.com/repos/apache/spark/issues/26955/events,https://github.com/apache/spark/pull/26955,540592494,MDExOlB1bGxSZXF1ZXN0MzU1MzgxMDQz,26955,[SPARK-30310] [Core] Resolve missing match case in SparkUncaughtExceptionHandler and added tests,[],open,False,,[],,1,2019-12-19T21:53:59Z,2019-12-23T20:42:15Z,,NONE,"### What changes were proposed in this pull request?
1) Added missing match case to SparkUncaughtExceptionHandler, so that it would not halt the process when the exception doesn't match any of the match case statements.
2) Added log message before halting process.  During debugging it wasn't obvious why the Worker process would DEAD (until we set SPARK_NO_DAEMONIZE=1) due to the shell-scripts puts the process into background and essentially absorbs the exit code.
3) Added SparkUncaughtExceptionHandlerSuite.  Basically we create a Spark exception-throwing application with SparkUncaughtExceptionHandler and then check its exit code.

### Why are the changes needed?
SPARK-30310, because the process would halt unexpectedly.

### How was this patch tested?
All unit tests (mvn test) were ran and OK.",spark,apache,tinhto-000,46789425,MDQ6VXNlcjQ2Nzg5NDI1,https://avatars2.githubusercontent.com/u/46789425?v=4,,https://api.github.com/users/tinhto-000,https://github.com/tinhto-000,https://api.github.com/users/tinhto-000/followers,https://api.github.com/users/tinhto-000/following{/other_user},https://api.github.com/users/tinhto-000/gists{/gist_id},https://api.github.com/users/tinhto-000/starred{/owner}{/repo},https://api.github.com/users/tinhto-000/subscriptions,https://api.github.com/users/tinhto-000/orgs,https://api.github.com/users/tinhto-000/repos,https://api.github.com/users/tinhto-000/events{/privacy},https://api.github.com/users/tinhto-000/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26955,https://github.com/apache/spark/pull/26955,https://github.com/apache/spark/pull/26955.diff,https://github.com/apache/spark/pull/26955.patch
34,https://api.github.com/repos/apache/spark/issues/26953,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26953/labels{/name},https://api.github.com/repos/apache/spark/issues/26953/comments,https://api.github.com/repos/apache/spark/issues/26953/events,https://github.com/apache/spark/pull/26953,540376240,MDExOlB1bGxSZXF1ZXN0MzU1MTk3MTYx,26953,[SPARK-30306][CORE][PYTHON] Instrument Python UDF execution time and metrics using Spark Metrics system,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-12-19T15:16:26Z,2019-12-20T10:30:44Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This proposes to extend Spark instrumentation to add metrics aimed at drilling down on the performance of Python code called by Spark: via UDF, Pandas UDF or with MapPartittions. Relevant performance counters, notably exuction time, are exposed using the Spark Metrics System (based on the Dropwizard library).

### Why are the changes needed?
This allows to easily consume the metrics produced by executors, for example using a performance dashboard (this references to previous work as discucssed in https://db-blog.web.cern.ch/blog/luca-canali/2019-02-performance-dashboard-apache-spark ).
See also the screenshot that compares the existing state (no Python UDF time instrumentation) to the proposed new functionality ![](https://issues.apache.org/jira/secure/attachment/12989201/PandasUDF_Time_Instrumentation_Annotated.png)

### Does this PR introduce any user-facing change?
This PR adds the PythonMetrics source to the Spark Metrics system. The list of the implemented metrics has been added to the Monitoring documentation.

### How was this patch tested?
Added relevant tests
+ manually tested end-to-end on a YARN cluster and using an existing Spark dashboard extended with the metrics proposed here.",spark,apache,LucaCanali,5243162,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26953,https://github.com/apache/spark/pull/26953,https://github.com/apache/spark/pull/26953.diff,https://github.com/apache/spark/pull/26953.patch
35,https://api.github.com/repos/apache/spark/issues/26951,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26951/labels{/name},https://api.github.com/repos/apache/spark/issues/26951/comments,https://api.github.com/repos/apache/spark/issues/26951/events,https://github.com/apache/spark/pull/26951,540364360,MDExOlB1bGxSZXF1ZXN0MzU1MTg3MzU3,26951,"[SPARK-30304][CORE]When the specified shufflemanager is incorrect, print the prompt.","[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-19T14:57:35Z,2019-12-21T07:47:01Z,,CONTRIBUTOR,"
### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
During the instantiation of the specified `ShuffleManager`, if the configuration is wrong, whether the log can print some tips.

before:
```log
java.lang.ClassNotFoundException: hash
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:206)
	at org.apache.spark.SparkEnv$.instantiateClass$1(SparkEnv.scala:274)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:338)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:188)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:462)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:131)
	at org.apache.spark.SortShuffleSuite.$anonfun$new$1(SortShuffleSuite.scala:67)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
```

Now it's not just printing out stack exceptions, but also printing error prompt logs.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
It is convenient to quickly locate problems through logs.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add test in `SortShuffleSuite`.",spark,apache,sev7e0,21102442,MDQ6VXNlcjIxMTAyNDQy,https://avatars1.githubusercontent.com/u/21102442?v=4,,https://api.github.com/users/sev7e0,https://github.com/sev7e0,https://api.github.com/users/sev7e0/followers,https://api.github.com/users/sev7e0/following{/other_user},https://api.github.com/users/sev7e0/gists{/gist_id},https://api.github.com/users/sev7e0/starred{/owner}{/repo},https://api.github.com/users/sev7e0/subscriptions,https://api.github.com/users/sev7e0/orgs,https://api.github.com/users/sev7e0/repos,https://api.github.com/users/sev7e0/events{/privacy},https://api.github.com/users/sev7e0/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26951,https://github.com/apache/spark/pull/26951,https://github.com/apache/spark/pull/26951.diff,https://github.com/apache/spark/pull/26951.patch
36,https://api.github.com/repos/apache/spark/issues/26946,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26946/labels{/name},https://api.github.com/repos/apache/spark/issues/26946/comments,https://api.github.com/repos/apache/spark/issues/26946/events,https://github.com/apache/spark/pull/26946,540152730,MDExOlB1bGxSZXF1ZXN0MzU1MDA4OTAw,26946,[SPARK-30036][SQL] Fix: REPARTITION hint does not work with order by,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2019-12-19T08:18:45Z,2019-12-24T10:01:46Z,,CONTRIBUTOR,"### Why are the changes needed?
`EnsureRequirements` adds `ShuffleExchangeExec` (RangePartitioning) after Sort if `RoundRobinPartitioning` behinds it. This will cause 2 shuffles, and the number of partitions in the final stage is not the number specified by `RoundRobinPartitioning.

**Example SQL**
```
SELECT /*+ REPARTITION(5) */ * FROM test ORDER BY a
```

**BEFORE**
```
== Physical Plan ==
*(1) Sort [a#0 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(a#0 ASC NULLS FIRST, 200), true, [id=#11]
   +- Exchange RoundRobinPartitioning(5), false, [id=#9]
      +- Scan hive default.test [a#0, b#1], HiveTableRelation `default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#0, b#1]
```

**AFTER**
```
== Physical Plan ==
*(1) Sort [a#0 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(a#0 ASC NULLS FIRST, 5), true, [id=#11]
   +- Scan hive default.test [a#0, b#1], HiveTableRelation `default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#0, b#1]
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Run suite Tests and add new test for this.
",spark,apache,stczwd,10897625,MDQ6VXNlcjEwODk3NjI1,https://avatars0.githubusercontent.com/u/10897625?v=4,,https://api.github.com/users/stczwd,https://github.com/stczwd,https://api.github.com/users/stczwd/followers,https://api.github.com/users/stczwd/following{/other_user},https://api.github.com/users/stczwd/gists{/gist_id},https://api.github.com/users/stczwd/starred{/owner}{/repo},https://api.github.com/users/stczwd/subscriptions,https://api.github.com/users/stczwd/orgs,https://api.github.com/users/stczwd/repos,https://api.github.com/users/stczwd/events{/privacy},https://api.github.com/users/stczwd/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26946,https://github.com/apache/spark/pull/26946,https://github.com/apache/spark/pull/26946.diff,https://github.com/apache/spark/pull/26946.patch
37,https://api.github.com/repos/apache/spark/issues/26944,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26944/labels{/name},https://api.github.com/repos/apache/spark/issues/26944/comments,https://api.github.com/repos/apache/spark/issues/26944/events,https://github.com/apache/spark/pull/26944,540100269,MDExOlB1bGxSZXF1ZXN0MzU0OTY0ODk2,26944,[SPARK-30302][SQL] Complete info for show create table for views,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-12-19T06:08:33Z,2019-12-24T02:06:56Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add table/column comments and table properties to the result of show create table of views.


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
When show create table for views, after this patch, the result can contain table/column comments and table properties if they exist.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add new tests",spark,apache,wzhfy,10878553,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26944,https://github.com/apache/spark/pull/26944,https://github.com/apache/spark/pull/26944.diff,https://github.com/apache/spark/pull/26944.patch
38,https://api.github.com/repos/apache/spark/issues/26943,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26943/labels{/name},https://api.github.com/repos/apache/spark/issues/26943/comments,https://api.github.com/repos/apache/spark/issues/26943/events,https://github.com/apache/spark/pull/26943,540099565,MDExOlB1bGxSZXF1ZXN0MzU0OTY0MzE5,26943,[SPARK-30298][SQL] Bucket join should work for self-join with views,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-12-19T06:06:39Z,2019-12-20T23:27:57Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Currently, in the following scenario, bucket join is not utilized:
```scala
val df = (0 until 20).map(i => (i, i)).toDF(""i"", ""j"").as(""df"")
df.write.format(""parquet"").bucketBy(8, ""i"").saveAsTable(""t"")
sql(""CREATE VIEW v AS SELECT * FROM t"")
sql(""SELECT * FROM t a JOIN v b ON a.i = b.i"").explain
```
```
== Physical Plan ==
*(4) SortMergeJoin [i#13], [i#15], Inner
:- *(1) Sort [i#13 ASC NULLS FIRST], false, 0
:  +- *(1) Project [i#13, j#14]
:     +- *(1) Filter isnotnull(i#13)
:        +- *(1) ColumnarToRow
:           +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:..., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
+- *(3) Sort [i#15 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(i#15, 8), true, [id=#64] <----- Exchange node introduced
      +- *(2) Project [i#13 AS i#15, j#14 AS j#16]
         +- *(2) Filter isnotnull(i#13)
            +- *(2) ColumnarToRow
               +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:..., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
```
Notice that `Exchange` is present. This is because `Project` introduces aliases and `outputPartitioning` and `requiredChildDistribution` do not consider aliases while considering bucket join in `EnsureRequirements`. This PR addresses to allow this scenario.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This allows bucket join to be utilized in the above example.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes, now with the fix, the `explain` out is as follows:
```
== Physical Plan ==
*(3) SortMergeJoin [i#13], [i#15], Inner
:- *(1) Sort [i#13 ASC NULLS FIRST], false, 0
:  +- *(1) Project [i#13, j#14]
:     +- *(1) Filter isnotnull(i#13)
:        +- *(1) ColumnarToRow
:           +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:.., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
+- *(2) Sort [i#15 ASC NULLS FIRST], false, 0
   +- *(2) Project [i#13 AS i#15, j#14 AS j#16]
      +- *(2) Filter isnotnull(i#13)
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:.., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
```
Note that the `Exchange` is no longer present.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,imback82,12103644,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26943,https://github.com/apache/spark/pull/26943,https://github.com/apache/spark/pull/26943.diff,https://github.com/apache/spark/pull/26943.patch
39,https://api.github.com/repos/apache/spark/issues/26938,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26938/labels{/name},https://api.github.com/repos/apache/spark/issues/26938/comments,https://api.github.com/repos/apache/spark/issues/26938/events,https://github.com/apache/spark/pull/26938,539728131,MDExOlB1bGxSZXF1ZXN0MzU0NjUyOTY2,26938,[SPARK-30297][CORE] Fix executor lost in net cause app hung upp,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-12-18T14:49:38Z,2019-12-23T11:19:10Z,,NONE,"### **What changes were proposed in this pull request?**

**Backgroud**
The driver can't sense this executor was lost through the network connection disconnection If an executor was lost in the network and it have not responsed rst and close packet to driver, so driver can only sense this executor dead through heartbeat expired.

**Problems**
Heartbeat expiration processing flow as follows:
1. Executor heartbeat expired as above.
2. HeartbeatReceiver will call scheduler executor lost to rescheduler the tasks on this executor.
3. HeartbeatReceiver kill the executor.

The tasks on the dead executor have a chance to rescheduled on this dead executor again if the task rescheduler before the executor has't remove from executorBackend, it will send launch task to this executor again, the executor will not response and the driver can't sense through heartbeat beause the executor has lost in network. This cause those tasks rescheduled on this lost executor can't finish forever, and the app will hung up here forever.
This patch fix this problem, it remove the executor before rescheduler.

### **Why are the changes needed?**
This will cause app hung up.

### **Does this PR introduce any user-facing change?**
NO

### **How was this patch tested?**",spark,apache,seayoun,45163307,MDQ6VXNlcjQ1MTYzMzA3,https://avatars3.githubusercontent.com/u/45163307?v=4,,https://api.github.com/users/seayoun,https://github.com/seayoun,https://api.github.com/users/seayoun/followers,https://api.github.com/users/seayoun/following{/other_user},https://api.github.com/users/seayoun/gists{/gist_id},https://api.github.com/users/seayoun/starred{/owner}{/repo},https://api.github.com/users/seayoun/subscriptions,https://api.github.com/users/seayoun/orgs,https://api.github.com/users/seayoun/repos,https://api.github.com/users/seayoun/events{/privacy},https://api.github.com/users/seayoun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26938,https://github.com/apache/spark/pull/26938,https://github.com/apache/spark/pull/26938.diff,https://github.com/apache/spark/pull/26938.patch
40,https://api.github.com/repos/apache/spark/issues/26937,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26937/labels{/name},https://api.github.com/repos/apache/spark/issues/26937/comments,https://api.github.com/repos/apache/spark/issues/26937/events,https://github.com/apache/spark/pull/26937,539655401,MDExOlB1bGxSZXF1ZXN0MzU0NTkxODAy,26937,[WIP][SPARK-30295][SQL] Remove hive dependencies for the Spark SQL Client,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-12-18T12:37:18Z,2019-12-19T21:12:34Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
#### Removed Hive dependencies from:
- [x] SparkSQLCLIDriver
- [x] SparkSQLDriver
#### Features added :
- [x] Added a SparkSQLCLIArguments
- [x] Added new prompt string `spark-sql ($database)>`
- [x] Added same showString method from Datasets to print String formatted results.

#### TODO:
- [ ] Code documentation
- [ ] Improved Console usage.
- [x] Interruption Handling.
- [ ] IO handling for reading and writtig streams, currently using prinln.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To remove unnecessary hive dependencies for the Spark SQL Client.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
--> 
No


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added test for: 

> SPARK-30049 Should not complaint for quotes in commented lines

Passing current tests",spark,apache,javierivanov,7876890,MDQ6VXNlcjc4NzY4OTA=,https://avatars1.githubusercontent.com/u/7876890?v=4,,https://api.github.com/users/javierivanov,https://github.com/javierivanov,https://api.github.com/users/javierivanov/followers,https://api.github.com/users/javierivanov/following{/other_user},https://api.github.com/users/javierivanov/gists{/gist_id},https://api.github.com/users/javierivanov/starred{/owner}{/repo},https://api.github.com/users/javierivanov/subscriptions,https://api.github.com/users/javierivanov/orgs,https://api.github.com/users/javierivanov/repos,https://api.github.com/users/javierivanov/events{/privacy},https://api.github.com/users/javierivanov/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26937,https://github.com/apache/spark/pull/26937,https://github.com/apache/spark/pull/26937.diff,https://github.com/apache/spark/pull/26937.patch
41,https://api.github.com/repos/apache/spark/issues/26936,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26936/labels{/name},https://api.github.com/repos/apache/spark/issues/26936/comments,https://api.github.com/repos/apache/spark/issues/26936/events,https://github.com/apache/spark/pull/26936,539648747,MDExOlB1bGxSZXF1ZXN0MzU0NTg2MjUz,26936,[SPARK-30296][SQL] Add Dataset diffing feature,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-12-18T12:22:57Z,2019-12-19T21:12:35Z,,NONE,"### What changes were proposed in this pull request?
Adds a `diff` transformation to `Dataset` that computes the differences between the two datasets, i.e. which rows of `this` dataset to _add_, _delete_ or _change_ to get to the given dataset.

With
```
val left = Seq((1, ""one""), (2, ""two""), (3, ""three"")).toDF(""id"", ""value"")
val right = Seq((1, ""one""), (2, ""Two""), (4, ""four"")).toDF(""id"", ""value"")
```
Diffing becomes as easy as:
```
left.diff(right).show()
```
|diff| id|value|
|----|---|-----|
|   N|  1|  one|
|   D|  2|  two|
|   I|  2|  Two|
|   D|  3|three|
|   I|  4| four|

With columns that provide unique identifiers per row (here `id`), the diff looks like:
```
left.diff(right, ""id"").show()
```
|diff| id|left_value|right_value|
|----|---|----------|-----------|
|   N|  1|       one|        one|
|   C|  2|       two|        Two|
|   D|  3|     three|       null|
|   I|  4|      null|       four|


Equivalent alternative is this hand-crafted transformation
```
left.withColumn(""exists"", lit(1)).as(""l"")
  .join(right.withColumn(""exists"", lit(1)).as(""r""),
    $""l.id"" <=> $""r.id"",
    ""fullouter"")
  .withColumn(""diff"",
    when($""l.exists"".isNull, ""I"").
      when($""r.exists"".isNull, ""D"").
      when(!($""l.value"" <=> $""r.value""), ""C"").
      otherwise(""N""))
  .show()
```

Statistics on the differences can be obtained by
```
left.diff(right, ""id"").groupBy(""diff"").count().show()
```

|diff|count|
|----|-----|
|   N|    1|
|   I|    1|
|   D|    1|
|   C|    1|

This `diff` provides the following features:
* id columns are optional
* provides typed `diffAs` transformations
* supports null values in id and non-id columns
* detects null value insertion / deletion
* configurable via `DiffOptions`:
  * diff column name (default: `""diff""`), for diffing datasets that already contain `diff` column
  * diff action labels (defaults: `""N""`, `""I""`, `""D""`, `""C""`), allows custom diff notation,
e.g. Unix diff left-right notation (<, >) or git before-after format (+, -, -+)

### Why are the changes needed?
Your evolving code need frequent regression testing to prove it still produces identical results, or if changes are expected, to investigate those changes. Diffing the results of two code paths provides the confidence you need. Diffing small schemata is easy, but with wide schema the Spark query becomes laborious and error-prone. With a single proven and tested method, diffing becomes easier and a more reliable operation. This has proven to be useful for interactive spark as well as deployed production code.

### Does this PR introduce any user-facing change?
Yes, it provides new transformations added to `Dataset`:

* `def diff(other: Dataset[T], idColumns: String*): DataFrame`
* `def diff(other: Dataset[T], options: DiffOptions, idColumns: String*): DataFrame`
* `def diffAs[U](other: Dataset[T], idColumns: String*)(implicit diffEncoder: Encoder[U]): Dataset[U]`
* `def diffAs[U](other: Dataset[T], options: DiffOptions, idColumns: String*)(implicit diffEncoder: Encoder[U]): Dataset[U]`
* `def diffAs[U](other: Dataset[T], diffEncoder: Encoder[U], idColumns: String*): Dataset[U]`
* `def diffAs[U](other: Dataset[T], options: DiffOptions, diffEncoder: Encoder[U], idColumns: String*): Dataset[U]`

### How was this patch tested?
There is a new suite with plenty of tests: `DiffSuite`.",spark,apache,EnricoMi,44700269,MDQ6VXNlcjQ0NzAwMjY5,https://avatars1.githubusercontent.com/u/44700269?v=4,,https://api.github.com/users/EnricoMi,https://github.com/EnricoMi,https://api.github.com/users/EnricoMi/followers,https://api.github.com/users/EnricoMi/following{/other_user},https://api.github.com/users/EnricoMi/gists{/gist_id},https://api.github.com/users/EnricoMi/starred{/owner}{/repo},https://api.github.com/users/EnricoMi/subscriptions,https://api.github.com/users/EnricoMi/orgs,https://api.github.com/users/EnricoMi/repos,https://api.github.com/users/EnricoMi/events{/privacy},https://api.github.com/users/EnricoMi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26936,https://github.com/apache/spark/pull/26936,https://github.com/apache/spark/pull/26936.diff,https://github.com/apache/spark/pull/26936.patch
42,https://api.github.com/repos/apache/spark/issues/26935,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26935/labels{/name},https://api.github.com/repos/apache/spark/issues/26935/comments,https://api.github.com/repos/apache/spark/issues/26935/events,https://github.com/apache/spark/pull/26935,539548183,MDExOlB1bGxSZXF1ZXN0MzU0NTAzODU5,26935,[SPARK-30294][SS] Explicitly defines read-only StateStore and optimize for HDFSBackedStateStore,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-12-18T09:08:07Z,2019-12-21T00:34:29Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

There's a concept of 'read-only' and 'read+write' state store in Spark which is defined ""implicitly"". Spark doesn't prevent write for 'read-only' state store; Spark just assumes read-only stateful operator will not modify the state store. Given it's not defined explicitly, the instance of state store has to be implemented as 'read+write' even it's being used as 'read-only', which sometimes brings confusion.

For example, abort() in HDFSBackedStateStore - https://github.com/apache/spark/blob/d38f8167483d4d79e8360f24a8c0bffd51460659/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L143-L155

The comment sounds as if statement works differently between 'read-only' and 'read+write', but that's not true as both state store has state initialized as UPDATING (no difference). So 'read-only' state also creates the temporary file, initializes output streams to write to temporary file, closes output streams, and finally deletes the temporary file. This unnecessary operations are being done per batch/partition.

This patch explicitly defines 'read-only' StateStore, and enables state store provider to create 'read-only' StateStore instance if requested. Relevant code paths are modified, as well as 'read-only' StateStore implementation for HDFSBackedStateStore is introduced. The new implementation gets rid of unnecessary operations explained above.

In point of backward-compatibility view, the only thing being changed in public API side is `StateStoreProvider`. The trait `StateStoreProvider` has to be changed to allow requesting 'read-only' StateStore; this patch adds default implementation which leverages 'read+write' StateStore but wrapping with 'write-protected' StateStore instance, so that custom providers don't need to change their code to reflect the change. But if the providers can optimize for read-only workload, they'll be happy to make a change.

Please note that this patch makes ReadOnlyStateStore extend StateStore and being referred as StateStore, as StateStore is being used in so many places and it's not easy to support both traits if we differentiate them. So unfortunately these write methods are still exposed for read-only state; it just throws UnsupportedOperationException.

### Why are the changes needed?

The new API opens the chance to optimize read-only state store instance compared with read+write state store instance. HDFSBackedStateStoreProvider is modified to provide read-only version of state store which doesn't deal with temporary file as well as state machine.

### Does this PR introduce any user-facing change?

Clearly ""no"" for most end users, and also ""no"" for custom state store providers as it doesn't touch trait `StateStore` as well as provides default implementation for added method in trait `StateStoreProvider`.

### How was this patch tested?

Modified UT. Existing UTs ensure the change doesn't break anything.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26935,https://github.com/apache/spark/pull/26935,https://github.com/apache/spark/pull/26935.diff,https://github.com/apache/spark/pull/26935.patch
43,https://api.github.com/repos/apache/spark/issues/26933,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26933/labels{/name},https://api.github.com/repos/apache/spark/issues/26933/comments,https://api.github.com/repos/apache/spark/issues/26933/events,https://github.com/apache/spark/pull/26933,539473913,MDExOlB1bGxSZXF1ZXN0MzU0NDQzNDc3,26933,[SPARK-30292][SQL]Throw Exception when invalid string is cast to decimal in ANSI mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-12-18T06:14:43Z,2019-12-20T00:00:07Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
If spark.sql.ansi.enabled is set,
Throw analysis exception when cast to decimal do not follow the ANSI casting standards.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
ANSI SQL standards do not allow invalid strings to get casted into decimal and throw exception for that. Currently spark sql gives NULL in such cases. 

Before: 
`select cast('str' as decimal)  => NULL`

After :
`select cast('str' as decimal) => invalid input syntax for type numeric: str`

These results are after setting `spark.sql.ansi.enabled=true`
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
Yes. Now when ansi mode is on users will get analysis exception for invalid strings.
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Manually. Test cases will added soon.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,iRakson,15366835,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26933,https://github.com/apache/spark/pull/26933,https://github.com/apache/spark/pull/26933.diff,https://github.com/apache/spark/pull/26933.patch
44,https://api.github.com/repos/apache/spark/issues/26930,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26930/labels{/name},https://api.github.com/repos/apache/spark/issues/26930/comments,https://api.github.com/repos/apache/spark/issues/26930/events,https://github.com/apache/spark/pull/26930,539397992,MDExOlB1bGxSZXF1ZXN0MzU0MzgyNTUx,26930,[SPARK-30290][Core] Count for merged block when fetching continuous blocks in batch,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-18T01:24:18Z,2019-12-24T08:25:08Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

We added shuffle block fetch optimization in SPARK-9853. In ShuffleBlockFetcherIterator, we merge single blocks into batch blocks. During merging, we should count merged blocks for `maxBlocksInFlightPerAddress`, not original single blocks.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

If `maxBlocksInFlightPerAddress` is specified, like set it to 1, it should mean one batch block, not one original single block. Otherwise, it will conflict with batch shuffle fetch.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit test.",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26930,https://github.com/apache/spark/pull/26930,https://github.com/apache/spark/pull/26930.diff,https://github.com/apache/spark/pull/26930.patch
45,https://api.github.com/repos/apache/spark/issues/26929,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26929/labels{/name},https://api.github.com/repos/apache/spark/issues/26929/comments,https://api.github.com/repos/apache/spark/issues/26929/events,https://github.com/apache/spark/pull/26929,539387015,MDExOlB1bGxSZXF1ZXN0MzU0MzczNjgx,26929,[SPARK-30289][SQL] DSv2's partitioning should not accept nested columns,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-18T00:41:39Z,2019-12-19T05:07:43Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
1. `InMemoryTable` used in tests was flatting the nested columns, and then the flatten columns was used to look up the indices which can cause issues. This is fixed in this PR.
 For example, a nested schema of 
 ```
root
 |-- user: struct (nullable = true)
 |    |-- name: string (nullable = true)
 |-- customer: string (nullable = true)
 |    |-- name: string (nullable = true)
 ```
will be flatten into `user`, `name`, `customer`, `name` creating two conflicted `name`. Then we looked up the indices from the top column schema which created hard to debug exception.

In the new test, without the fix, the exception was

```scala
java.lang.IllegalArgumentException: id does not exist. Available: nested
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:303)
```

With fix, the exception is
```
org.apache.spark.sql.AnalysisException: Cannot partition by nested column: nested.id;
```

 2. Add check to `FileTable` as well which currently doesn't support using nested columns as partitioning.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Without the fix, the exception is raised in a very hard to debug way.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing unit tests and new tests.",spark,apache,dbtsai,1134574,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26929,https://github.com/apache/spark/pull/26929,https://github.com/apache/spark/pull/26929.diff,https://github.com/apache/spark/pull/26929.patch
46,https://api.github.com/repos/apache/spark/issues/26927,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26927/labels{/name},https://api.github.com/repos/apache/spark/issues/26927/comments,https://api.github.com/repos/apache/spark/issues/26927/events,https://github.com/apache/spark/pull/26927,539021044,MDExOlB1bGxSZXF1ZXN0MzU0MDY2ODE2,26927,[SPARK-29505][SQL] Make DESC EXTENDED <table name> <column name> case insensitive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-12-17T11:57:16Z,2019-12-24T09:43:27Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
While querying using **desc** , if column name is not entered exactly as per the column name given during the table creation, the colstats are wrong. fetching of col stats has been made case insensitive.

### Why are the changes needed?
functions like **analyze**, etc support case insensitive retrieval of column data.


### Does this PR introduce any user-facing change?
NO


### How was this patch tested?
<!--
Unit test has been rewritten and tested.",spark,apache,PavithraRamachandran,51401130,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26927,https://github.com/apache/spark/pull/26927,https://github.com/apache/spark/pull/26927.diff,https://github.com/apache/spark/pull/26927.patch
47,https://api.github.com/repos/apache/spark/issues/26926,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26926/labels{/name},https://api.github.com/repos/apache/spark/issues/26926/comments,https://api.github.com/repos/apache/spark/issues/26926/events,https://github.com/apache/spark/pull/26926,538987196,MDExOlB1bGxSZXF1ZXN0MzU0MDM4NTIy,26926,[WIP][SPARK-30287][SQL][phase1] Add new module sql/service,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-12-17T10:49:14Z,2019-12-19T21:13:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

With the development of Spark and HiveÔºåin current `sql/hive-thriftserver`, we need to do a lot of work to solve code conflicts between different hive versions. It's an annoying and unending work in current ways. And these issues are troubling us when we develop new features for the SparkThriftServer2. We suppose to implement a new thrift server based on latest v11 `TCLService.thrift` thrift protocol. Implement all API in spark's own code to get rid of hive code .
    Finally, the new thrift server have below feature:
1. support all functions current `hive-thriftserver` support
2. use all code maintained by spark itself
3. realize origin function fit to spark‚Äôs own feature, won't limited by hive's code
4. support running without hive metastore or with hive metastore
5. support user impersonation by Multi-tenant authority separation,  hive authentication and DFS authentication
6. add a new module `spark-jdbc`, with connection url  `jdbc:spark:<host>:<port>/<db>`, all `hive-jdbc` support we will all support
7. support both `hive-jdbc` and `spark-jdbc` client for compatibility with most clients


We have done all these works in our repo, now we plan  merge our code into the master step by step.  

1.  **phase1**  pr about build new module  `spark-service` on folder `sql/service`
2. **phase2**  pr thrift protocol and generated thrift protocol java code 
3. **phase3**  pr with all `spark-service` module code  and description about design, also UT
4. **phase4**  pr about build new module `spark-jdbc` on folder `sql/jdbc`
5. **phase5**  pr with all `spark-jdbc` module code  and UT
6. **phase6**  pr about support thriftserver Impersonation
7. **phase7**   pr about build spark's own beeline client `spark-beeline`
8. **phase8**  pr about spark's own Cli client `Spark SQL CLI` `spark-cli`

### Why are the changes needed?

Build a totally new thrift server base on spark's own code and feature.Don't rely on hive code anymore


### Does this PR introduce any user-facing change?


### How was this patch tested?
Not need  UT now
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26926,https://github.com/apache/spark/pull/26926,https://github.com/apache/spark/pull/26926.diff,https://github.com/apache/spark/pull/26926.patch
48,https://api.github.com/repos/apache/spark/issues/26925,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26925/labels{/name},https://api.github.com/repos/apache/spark/issues/26925/comments,https://api.github.com/repos/apache/spark/issues/26925/events,https://github.com/apache/spark/pull/26925,538955058,MDExOlB1bGxSZXF1ZXN0MzU0MDExNzU5,26925,[MINOR][CORE] Quiet request executor remove message.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-12-17T09:51:53Z,2019-12-21T18:38:06Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Settings to quiet for Class `ExecutorAllocationManager` that request message too verbose. otherwise, this class generates too many messages like 
`INFO spark.ExecutorAllocationManager: Request to remove executorIds: 890`
 when we enabled DRA.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Log level improvement.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,cfmcgrady,8537877,MDQ6VXNlcjg1Mzc4Nzc=,https://avatars1.githubusercontent.com/u/8537877?v=4,,https://api.github.com/users/cfmcgrady,https://github.com/cfmcgrady,https://api.github.com/users/cfmcgrady/followers,https://api.github.com/users/cfmcgrady/following{/other_user},https://api.github.com/users/cfmcgrady/gists{/gist_id},https://api.github.com/users/cfmcgrady/starred{/owner}{/repo},https://api.github.com/users/cfmcgrady/subscriptions,https://api.github.com/users/cfmcgrady/orgs,https://api.github.com/users/cfmcgrady/repos,https://api.github.com/users/cfmcgrady/events{/privacy},https://api.github.com/users/cfmcgrady/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26925,https://github.com/apache/spark/pull/26925,https://github.com/apache/spark/pull/26925.diff,https://github.com/apache/spark/pull/26925.patch
49,https://api.github.com/repos/apache/spark/issues/26924,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26924/labels{/name},https://api.github.com/repos/apache/spark/issues/26924/comments,https://api.github.com/repos/apache/spark/issues/26924/events,https://github.com/apache/spark/pull/26924,538953461,MDExOlB1bGxSZXF1ZXN0MzU0MDEwNDE0,26924,[SPARK-30285][CORE] Fix deadlock between LiveListenerBus#stop and AsyncEventQueue#removeListenerOnError,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2019-12-17T09:48:58Z,2019-12-24T08:50:57Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

There is a deadlock between `LiveListenerBus#stop` and `AsyncEventQueue#removeListenerOnError`.

We can reproduce as follows:

1. Post some events to `LiveListenerBus`
2. Call `LiveListenerBus#stop` and hold the synchronized lock of `bus`(https://github.com/apache/spark/blob/5e92301723464d0876b5a7eec59c15fed0c5b98c/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala#L229), waiting until all the events are processed by listeners, then remove all the queues
3. Event queue would drain out events by posting to its listeners. If a listener is interrupted, it will call `AsyncEventQueue#removeListenerOnError`,  inside it will call `bus.removeListener`(https://github.com/apache/spark/blob/7b1b60c7583faca70aeab2659f06d4e491efa5c0/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala#L207), trying to acquire synchronized lock of bus, resulting in deadlock

This PR  removes the `synchronized` from `LiveListenerBus.stop` because underlying data structures themselves are thread-safe.

### Why are the changes needed?
To fix deadlock.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
New UT.",spark,apache,wangshuo128,4003322,MDQ6VXNlcjQwMDMzMjI=,https://avatars0.githubusercontent.com/u/4003322?v=4,,https://api.github.com/users/wangshuo128,https://github.com/wangshuo128,https://api.github.com/users/wangshuo128/followers,https://api.github.com/users/wangshuo128/following{/other_user},https://api.github.com/users/wangshuo128/gists{/gist_id},https://api.github.com/users/wangshuo128/starred{/owner}{/repo},https://api.github.com/users/wangshuo128/subscriptions,https://api.github.com/users/wangshuo128/orgs,https://api.github.com/users/wangshuo128/repos,https://api.github.com/users/wangshuo128/events{/privacy},https://api.github.com/users/wangshuo128/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26924,https://github.com/apache/spark/pull/26924,https://github.com/apache/spark/pull/26924.diff,https://github.com/apache/spark/pull/26924.patch
50,https://api.github.com/repos/apache/spark/issues/26923,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26923/labels{/name},https://api.github.com/repos/apache/spark/issues/26923/comments,https://api.github.com/repos/apache/spark/issues/26923/events,https://github.com/apache/spark/pull/26923,538930976,MDExOlB1bGxSZXF1ZXN0MzUzOTkyMTI1,26923,[SPARK-30284][SQL] CREATE VIEW should keep the current catalog and namespace,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-17T09:07:21Z,2019-12-19T21:13:07Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Update CREATE VIEW command to store the current catalog and namespace instead of current database in view metadata. Also update analyzer to leverage the catalog and namespace in view metastore to resolve relations inside views.

Note that, this PR still keeps the way we resolve views, by recursively calling Analyzer. This is necessary because view text may contain CTE, window spec, etc. which needs rules outside of the main resolution batch (e.g. `CTESubstitution`)

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To resolve relations inside view correctly.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes, fix a bug. Now tables referred by a view can be resolved correctly even if the current catalog/namespace has been updated.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
a new test",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26923,https://github.com/apache/spark/pull/26923,https://github.com/apache/spark/pull/26923.diff,https://github.com/apache/spark/pull/26923.patch
51,https://api.github.com/repos/apache/spark/issues/26921,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26921/labels{/name},https://api.github.com/repos/apache/spark/issues/26921/comments,https://api.github.com/repos/apache/spark/issues/26921/events,https://github.com/apache/spark/pull/26921,538880441,MDExOlB1bGxSZXF1ZXN0MzUzOTUxNTI4,26921,[SPARK-30282][SQL] UnresolvedV2Relation should be resolved to temp view first,[],open,False,,[],,7,2019-12-17T07:10:16Z,2019-12-19T14:42:04Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is a part of effort to make the relation lookup behavior consistent: [SPARK-2990](https://issues.apache.org/jira/browse/SPARK-29900).

This PR specifically addresses the V2 commands whose logical plan contains `UnresolvedV2Relation` such that if `UnresolvedV2Relation` is resolved to a temp view, those commands should error out with a message that v2 command cannot handle temp views.
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For the following v2 commands, `Analyzer.ResolveTables` does not check against the temp views before resolving `UnresolvedV2Relation`, thus it always resolves `UnresolvedV2Relation` to a table:
```
ALTER TABLE
DESCRIBE TABLE
SHOW TBLPROPERTIES
```
Thus, in the following example, `t` will be resolved to a table, not a temp view:
```
sql(""CREATE TEMPORARY VIEW t AS SELECT 2 AS i"")
sql(""CREATE TABLE testcat.ns.t USING csv AS SELECT 1 AS i"")
sql(""USE testcat.ns"")
sql(""DESCRIBE t"") // 't' is resolved to a table
```
This behavior is inconsistent with other commands which look up temp views first.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes, now the above example will fail as follows:
```
sql(""DESCRIBE t"") // 't' is now resolved to a temp view
org.apache.spark.sql.AnalysisException: Invalid command: 't' is a view not a table.;
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTables$$resolveV2Relation(Analyzer.scala:782)
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added new tests.",spark,apache,imback82,12103644,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26921,https://github.com/apache/spark/pull/26921,https://github.com/apache/spark/pull/26921.diff,https://github.com/apache/spark/pull/26921.patch
52,https://api.github.com/repos/apache/spark/issues/26920,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26920/labels{/name},https://api.github.com/repos/apache/spark/issues/26920/comments,https://api.github.com/repos/apache/spark/issues/26920/events,https://github.com/apache/spark/pull/26920,538869914,MDExOlB1bGxSZXF1ZXN0MzUzOTQyODMw,26920,[SPARK-30281][SS] Consider partitioned/recursive option while verifying archive path on FileStreamSource,[],open,False,,[],,6,2019-12-17T06:42:08Z,2019-12-18T02:02:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch renews the verification logic of archive path for FileStreamSource, as we found the logic doesn't take partitioned/recursive options into account.

Before the patch, it only requires the archive path to have depth more than 2 (two subdirectories from root), leveraging the fact FileStreamSource normally reads the files where the parent directory matches the pattern or the file itself matches the pattern. Given 'archive' operation moves the files to the base archive path with retaining the full path, archive path is tend to be safe if the depth is more than 2, meaning FileStreamSource doesn't re-read archived files as new source files.

WIth partitioned/recursive options, the fact is invalid, as FileStreamSource can read any files in any depth of subdirectories for source pattern. To deal with this correctly, we have to renew the verification logic, which may not intuitive and simple but works for all cases.

The new verification logic prevents both cases:

1) archive path matches with source pattern as ""prefix"" (the depth of archive path > the depth of source pattern)

e.g.
* source pattern: `/hello*/spar?`
* archive path: `/hello/spark/structured/streaming`

Any files in archive path will match with source pattern when recursive option is enabled.

2) source pattern matches with archive path as ""prefix"" (the depth of source pattern > the depth of archive path)

e.g.
* source pattern: `/hello*/spar?/structured/hello2*`
* archive path: `/hello/spark/structured`

Some archive files will not match with source pattern, e.g. file path:  `/hello/spark/structured/hello2`, then final archived path: `/hello/spark/structured/hello/spark/structured/hello2`.

But some other archive files will still match with source pattern, e.g. file path: `/hello2/spark/structured/hello2`, then final archived path: `/hello/spark/structured/hello2/spark/structured/hello2` which matches with source pattern when recursive is enabled.

Implicitly it also prevents archive path matches with source pattern as full match (same depth).

We would want to prevent any source files to be archived and added to new source files again, so the patch takes most restrictive approach to prevent the possible cases.

### Why are the changes needed?

Without this patch, there's a chance archived files are included as new source files when partitioned/recursive option is enabled, as current condition doesn't take these options into account.

### Does this PR introduce any user-facing change?

Only for Spark 3.0.0-preview (only preview 1 for now, but possibly preview 2 as well) - end users are required to provide archive path with ensuring a bit complicated conditions, instead of simply higher than 2 depths.

### How was this patch tested?

New UT.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26920,https://github.com/apache/spark/pull/26920,https://github.com/apache/spark/pull/26920.diff,https://github.com/apache/spark/pull/26920.patch
53,https://api.github.com/repos/apache/spark/issues/26918,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26918/labels{/name},https://api.github.com/repos/apache/spark/issues/26918/comments,https://api.github.com/repos/apache/spark/issues/26918/events,https://github.com/apache/spark/pull/26918,538812889,MDExOlB1bGxSZXF1ZXN0MzUzODk2MTI4,26918,[SPARK-30279][SQL] Support 32 or more grouping attributes for GROUPING_ID ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2019-12-17T03:37:00Z,2019-12-19T23:48:42Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pr intends to support 32 or more grouping attributes for GROUPING_ID. In the current master, an integer overflow can occur to compute grouping IDs;
https://github.com/apache/spark/blob/e75d9afb2f282ce79c9fd8bce031287739326a4f/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala#L613

For example, the query below generates wrong grouping IDs in the master;
```

scala> val numCols = 32 // or, 31
scala> val cols = (0 until numCols).map { i => s""c$i"" }
scala> sql(s""create table test_$numCols (${cols.map(c => s""$c int"").mkString("","")}, v int) using parquet"")
scala> val insertVals = (0 until numCols).map { _ => 1 }.mkString("","")
scala> sql(s""insert into test_$numCols values ($insertVals,3)"")
scala> sql(s""select grouping_id(), sum(v) from test_$numCols group by grouping sets ((${cols.mkString("","")}), (${cols.init.mkString("","")}))"").show(10, false)
scala> sql(s""drop table test_$numCols"")

// numCols = 32
+-------------+------+
|grouping_id()|sum(v)|
+-------------+------+
|0            |3     |
|0            |3     | // Wrong Grouping ID
+-------------+------+

// numCols = 31
+-------------+------+
|grouping_id()|sum(v)|
+-------------+------+
|0            |3     |
|1            |3     |
+-------------+------+
```
To fix this issue, this pr change code to use long values for `GROUPING_ID` instead of int values.
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To support more cases in `GROUPING_ID`.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26918,https://github.com/apache/spark/pull/26918,https://github.com/apache/spark/pull/26918.diff,https://github.com/apache/spark/pull/26918.patch
54,https://api.github.com/repos/apache/spark/issues/26917,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26917/labels{/name},https://api.github.com/repos/apache/spark/issues/26917/comments,https://api.github.com/repos/apache/spark/issues/26917/events,https://github.com/apache/spark/pull/26917,538799510,MDExOlB1bGxSZXF1ZXN0MzUzODg1MzM1,26917,[SPARK-30278][SQL][DOC] Update Spark SQL document menu for new changes,[],open,False,,[],,7,2019-12-17T02:46:23Z,2019-12-18T14:25:08Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Update the Spark SQL document menu and join strategy hints.

### Why are the changes needed?
- Several new changes in the Spark SQL document didn't change the menu-sql.yaml correspondingly.
- Update the demo code for join strategy hints. 


### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Document change only.",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26917,https://github.com/apache/spark/pull/26917,https://github.com/apache/spark/pull/26917.diff,https://github.com/apache/spark/pull/26917.patch
55,https://api.github.com/repos/apache/spark/issues/26913,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26913/labels{/name},https://api.github.com/repos/apache/spark/issues/26913/comments,https://api.github.com/repos/apache/spark/issues/26913/events,https://github.com/apache/spark/pull/26913,538719953,MDExOlB1bGxSZXF1ZXN0MzUzODIxNTA1,26913,[SPARK-29219][SQL] Introduce SupportsCatalogOptions for TableProvider,[],open,False,,[],,18,2019-12-16T22:46:58Z,2019-12-24T05:08:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This PR introduces `SupportsCatalogOptions` as an interface for `TableProvider`. Through `SupportsCatalogOptions`, V2 DataSources can implement the two methods `extractIdentifier` and `extractCatalog` to support the creation, and existence check of tables without requiring a formal TableCatalog implementation. 

We currently don't support all SaveModes for DataSourceV2 in DataFrameWriter.save. The idea here is that eventually File based tables can be written with `DataFrameWriter.save(path)` will create a PathIdentifier where the name is `path`, and the V2SessionCatalog will be able to perform FileSystem checks at `path` to support ErrorIfExists and Ignore SaveModes.

### Why are the changes needed?

To support all Save modes for V2 data sources with DataFrameWriter. Since we can now support table creation, we will be able to provide partitioning information when first creating the table as well.

### Does this PR introduce any user-facing change?

Introduces a new interface

### How was this patch tested?

Will add tests once interface is vetted.",spark,apache,brkyvz,5243515,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26913,https://github.com/apache/spark/pull/26913,https://github.com/apache/spark/pull/26913.diff,https://github.com/apache/spark/pull/26913.patch
56,https://api.github.com/repos/apache/spark/issues/26912,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26912/labels{/name},https://api.github.com/repos/apache/spark/issues/26912/comments,https://api.github.com/repos/apache/spark/issues/26912/events,https://github.com/apache/spark/pull/26912,538627020,MDExOlB1bGxSZXF1ZXN0MzUzNzQwMzc1,26912,[SPARK-30273] [PySpark] Add melt() function,[],open,False,,[],,1,2019-12-16T20:10:23Z,2019-12-17T01:18:46Z,,NONE,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

- Adds `melt()` functionality common to Pandas


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

- It's common for users to copy and paste code from [this](https://issues.apache.org/jira/browse/SPARK-30273) answer into their own systems.  This method belongs in the PySpark API

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
- Exposes a new `melt()` method in the DataFrame class


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
- As from [SO response](https://stackoverflow.com/a/41673644/12474509)

https://issues.apache.org/jira/browse/SPARK-30273
",spark,apache,vanhooser,6209145,MDQ6VXNlcjYyMDkxNDU=,https://avatars0.githubusercontent.com/u/6209145?v=4,,https://api.github.com/users/vanhooser,https://github.com/vanhooser,https://api.github.com/users/vanhooser/followers,https://api.github.com/users/vanhooser/following{/other_user},https://api.github.com/users/vanhooser/gists{/gist_id},https://api.github.com/users/vanhooser/starred{/owner}{/repo},https://api.github.com/users/vanhooser/subscriptions,https://api.github.com/users/vanhooser/orgs,https://api.github.com/users/vanhooser/repos,https://api.github.com/users/vanhooser/events{/privacy},https://api.github.com/users/vanhooser/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26912,https://github.com/apache/spark/pull/26912,https://github.com/apache/spark/pull/26912.diff,https://github.com/apache/spark/pull/26912.patch
57,https://api.github.com/repos/apache/spark/issues/26910,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26910/labels{/name},https://api.github.com/repos/apache/spark/issues/26910/comments,https://api.github.com/repos/apache/spark/issues/26910/events,https://github.com/apache/spark/pull/26910,538449738,MDExOlB1bGxSZXF1ZXN0MzUzNTk0MDE3,26910,[SPARK-30154][ML] PySpark UDF to convert MLlib vectors to dense arrays,[],open,False,,[],,15,2019-12-16T14:33:18Z,2019-12-21T06:33:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

PySpark UDF to convert MLlib vectors to dense arrays.
Example:
```
from pyspark.ml.functions import vector_to_array
df.select(vector_to_array(col(""features""))
```

### Why are the changes needed?
If a PySpark user wants to convert MLlib sparse/dense vectors in a DataFrame into dense arrays, an efficient approach is to do that in JVM. However, it requires PySpark user to write Scala code and register it as a UDF. Often this is infeasible for a pure python project.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
UT.
",spark,apache,WeichenXu123,19235986,MDQ6VXNlcjE5MjM1OTg2,https://avatars0.githubusercontent.com/u/19235986?v=4,,https://api.github.com/users/WeichenXu123,https://github.com/WeichenXu123,https://api.github.com/users/WeichenXu123/followers,https://api.github.com/users/WeichenXu123/following{/other_user},https://api.github.com/users/WeichenXu123/gists{/gist_id},https://api.github.com/users/WeichenXu123/starred{/owner}{/repo},https://api.github.com/users/WeichenXu123/subscriptions,https://api.github.com/users/WeichenXu123/orgs,https://api.github.com/users/WeichenXu123/repos,https://api.github.com/users/WeichenXu123/events{/privacy},https://api.github.com/users/WeichenXu123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26910,https://github.com/apache/spark/pull/26910,https://github.com/apache/spark/pull/26910.diff,https://github.com/apache/spark/pull/26910.patch
58,https://api.github.com/repos/apache/spark/issues/26907,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26907/labels{/name},https://api.github.com/repos/apache/spark/issues/26907/comments,https://api.github.com/repos/apache/spark/issues/26907/events,https://github.com/apache/spark/pull/26907,538408883,MDExOlB1bGxSZXF1ZXN0MzUzNTU5NzYw,26907,[SPARK-30267][SQL] Avro arrays can be of any List,[],open,False,,[],,8,2019-12-16T13:22:21Z,2019-12-19T09:57:07Z,,NONE,"The Deserializer assumed that avro arrays are always of type `GenericData$Array` which is not the case.
Assuming they are from java.util.List is safer and fixes a ClassCastException in some avro code.

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Java.util.List has all the necessary methods and is the base class of GenericData$Array.


### Why are the changes needed?
To prevent the following exception in more complex avro objects:

```
java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.avro.generic.GenericData$Array
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$newWriter$19(AvroDeserializer.scala:170)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$newWriter$19$adapted(AvroDeserializer.scala:169)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:314)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1$adapted(AvroDeserializer.scala:310)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$2(AvroDeserializer.scala:332)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$2$adapted(AvroDeserializer.scala:329)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$converter$3(AvroDeserializer.scala:56)
	at org.apache.spark.sql.avro.AvroDeserializer.deserialize(AvroDeserializer.scala:70)
```


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
The current tests already test this behavior.  In essesence this patch just changes a type case to a more basic type.  So I expect no functional impact.
",spark,apache,steven-aerts,1381633,MDQ6VXNlcjEzODE2MzM=,https://avatars2.githubusercontent.com/u/1381633?v=4,,https://api.github.com/users/steven-aerts,https://github.com/steven-aerts,https://api.github.com/users/steven-aerts/followers,https://api.github.com/users/steven-aerts/following{/other_user},https://api.github.com/users/steven-aerts/gists{/gist_id},https://api.github.com/users/steven-aerts/starred{/owner}{/repo},https://api.github.com/users/steven-aerts/subscriptions,https://api.github.com/users/steven-aerts/orgs,https://api.github.com/users/steven-aerts/repos,https://api.github.com/users/steven-aerts/events{/privacy},https://api.github.com/users/steven-aerts/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26907,https://github.com/apache/spark/pull/26907,https://github.com/apache/spark/pull/26907.diff,https://github.com/apache/spark/pull/26907.patch
59,https://api.github.com/repos/apache/spark/issues/26905,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26905/labels{/name},https://api.github.com/repos/apache/spark/issues/26905/comments,https://api.github.com/repos/apache/spark/issues/26905/events,https://github.com/apache/spark/pull/26905,538315550,MDExOlB1bGxSZXF1ZXN0MzUzNDgxNTIx,26905,[SPARK-30266][SQL] Avoid  match error and int overflow in ApproximatePercentile and Percentile,[],open,False,,[],,27,2019-12-16T10:18:22Z,2019-12-24T09:22:35Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
accuracyExpression can accept Long which may cause overflow error.
accuracyExpression can accept fractions which are implicitly floored.
accuracyExpression can accept null which is implicitly changed to 0.
percentageExpression can accept null but cause MatchError.
percentageExpression can accept ArrayType(_, nullable=true) in which the nulls are implicitly changed to zeros.

##### cases
```sql
select percentile_approx(10.0, 0.5, 2147483648); -- overflow and fail
select percentile_approx(10.0, 0.5, 4294967297); -- overflow but success
select percentile_approx(10.0, 0.5, null); -- null cast to 0
select percentile_approx(10.0, 0.5, 1.2); -- 1.2 cast to 1
select percentile_approx(10.0, null, 1); -- scala.MatchError
select percentile_approx(10.0, array(0.2, 0.4, null), 1); -- null cast to zero.
```

##### behavior before

```sql
+select percentile_approx(10.0, 0.5, 2147483648)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, CAST(0.5BD AS DOUBLE), CAST(2147483648L AS INT))' due to data type mismatch: The accuracy provided must be a positive integer literal (current value = -2147483648); line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 4294967297)
+10.0
+

+select percentile_approx(10.0, 0.5, null)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, CAST(0.5BD AS DOUBLE), CAST(NULL AS INT))' due to data type mismatch: The accuracy provided must be a positive integer literal (current value = 0); line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 1.2)
+10.0
+
+select percentile_approx(10.0, null, 1)
+scala.MatchError
+null
+
+
+select percentile_approx(10.0, array(0.2, 0.4, null), 1)
+[10.0,10.0,10.0]
```

##### behavior after

```sql

+select percentile_approx(10.0, 0.5, 2147483648)
+10.0
+
+select percentile_approx(10.0, 0.5, 4294967297)
+10.0
+
+select percentile_approx(10.0, 0.5, null)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, 0.5BD, NULL)' due to data type mismatch: argument 3 requires integral type, however, 'NULL' is of null type.; line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 1.2)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, 0.5BD, 1.2BD)' due to data type mismatch: argument 3 requires integral type, however, '1.2BD' is of decimal(2,1) type.; line 1 pos 7
+

+select percentile_approx(10.0, null, 1)
+java.lang.IllegalArgumentException
+The value of percentage must be be between 0.0 and 1.0, but got null
+
+select percentile_approx(10.0, array(0.2, 0.4, null), 1)
+java.lang.IllegalArgumentException
+Each value of the percentage array must be be between 0.0 and 1.0, but got [0.2,0.4,null]
```



### Why are the changes needed?

bug fix

### Does this PR introduce any user-facing change?

yes, fix some improper usages of percentile_approx as cases list above

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add ut",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26905,https://github.com/apache/spark/pull/26905,https://github.com/apache/spark/pull/26905.diff,https://github.com/apache/spark/pull/26905.patch
60,https://api.github.com/repos/apache/spark/issues/26901,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26901/labels{/name},https://api.github.com/repos/apache/spark/issues/26901/comments,https://api.github.com/repos/apache/spark/issues/26901/events,https://github.com/apache/spark/pull/26901,538154267,MDExOlB1bGxSZXF1ZXN0MzUzMzUwMjgz,26901,[SPARK-29152][2.4][test-maven]Executor Plugin shutdown when dynamic allocation is enabled,[],open,False,,[],,14,2019-12-16T02:41:31Z,2019-12-19T19:24:59Z,,CONTRIBUTOR,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
This PR is for checking the issues with jenkins maven.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
#26900 reverted the changes made in #26841 due to OOM issue. This PR is for checking the issue.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
NO.
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?

Manually. Need to pass jenkins tests.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,iRakson,15366835,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26901,https://github.com/apache/spark/pull/26901,https://github.com/apache/spark/pull/26901.diff,https://github.com/apache/spark/pull/26901.patch
61,https://api.github.com/repos/apache/spark/issues/26891,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26891/labels{/name},https://api.github.com/repos/apache/spark/issues/26891/comments,https://api.github.com/repos/apache/spark/issues/26891/events,https://github.com/apache/spark/pull/26891,537883322,MDExOlB1bGxSZXF1ZXN0MzUzMTU0MTUx,26891,Fix issue where `newFilesOnly` does nothing,[],open,False,,[],,2,2019-12-14T08:12:40Z,2019-12-15T13:58:01Z,,NONE,"this 
```scala
// Calculate ignore threshold
val modTimeIgnoreThreshold = math.max(
    initialModTimeIgnoreThreshold,   // initial threshold based on newFilesOnly setting
    currentTime - durationToRemember.milliseconds  // trailing end of the remember window
)
```
is equivalent to simply

```scala
// Calculate ignore threshold
val modTimeIgnoreThreshold = currentTime - durationToRemember.milliseconds
```
whenever `initialModTimeIgnoreThreshold == 0`.",spark,apache,hejfelix,1153154,MDQ6VXNlcjExNTMxNTQ=,https://avatars2.githubusercontent.com/u/1153154?v=4,,https://api.github.com/users/hejfelix,https://github.com/hejfelix,https://api.github.com/users/hejfelix/followers,https://api.github.com/users/hejfelix/following{/other_user},https://api.github.com/users/hejfelix/gists{/gist_id},https://api.github.com/users/hejfelix/starred{/owner}{/repo},https://api.github.com/users/hejfelix/subscriptions,https://api.github.com/users/hejfelix/orgs,https://api.github.com/users/hejfelix/repos,https://api.github.com/users/hejfelix/events{/privacy},https://api.github.com/users/hejfelix/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26891,https://github.com/apache/spark/pull/26891,https://github.com/apache/spark/pull/26891.diff,https://github.com/apache/spark/pull/26891.patch
62,https://api.github.com/repos/apache/spark/issues/26890,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26890/labels{/name},https://api.github.com/repos/apache/spark/issues/26890/comments,https://api.github.com/repos/apache/spark/issues/26890/events,https://github.com/apache/spark/pull/26890,537882254,MDExOlB1bGxSZXF1ZXN0MzUzMTUzMzg4,26890,[SPARK-30039][SQL] CREATE FUNCTION should do multi-catalog resolution,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-14T08:01:44Z,2019-12-17T05:56:41Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add CreateFunctionStatement and make CREATE FUNCTION go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
It's important to make all the commands have the same table resolution behavior, to avoid confusing
CREATE FUNCTION namespace.function

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes. When running CREATE FUNCTION namespace.function Spark fails the command if the current catalog is set to a v2 catalog.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit tests.",spark,apache,planga82,12819544,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26890,https://github.com/apache/spark/pull/26890,https://github.com/apache/spark/pull/26890.diff,https://github.com/apache/spark/pull/26890.patch
63,https://api.github.com/repos/apache/spark/issues/26889,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26889/labels{/name},https://api.github.com/repos/apache/spark/issues/26889/comments,https://api.github.com/repos/apache/spark/issues/26889/events,https://github.com/apache/spark/pull/26889,537876892,MDExOlB1bGxSZXF1ZXN0MzUzMTQ5MzYx,26889,[SPARK-30261][SQL] Should not change owner of hive table for some commands like 'alter' operation,[],open,False,,[],,1,2019-12-14T06:55:12Z,2019-12-14T07:33:22Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
For SparkSQL,When we do some alter operations on hive table, the owner of hive table would be changed to someone who invoked the operation, it's unresonable. And in fact, the owner should not changed for the real prodcution environment, otherwise the  authority check is out of order.

The problem can be reproduced as described in the below:
1. First I create a table with username='xie' and then `desc formatted table `,the owner is 'xiepengjie'
```
spark-sql> desc formatted bigdata_test.tt1;
col_name data_type comment
c int NULL
# Detailed Table Information
Database bigdata_test
Table tt1
Owner xie
Created Time Wed Sep 11 11:30:49 CST 2019
Last Access Thu Jan 01 08:00:00 CST 1970
Created By Spark 2.2 or prior
Type MANAGED
Provider hive
Table Properties [PART_LIMIT=10000, transient_lastDdlTime=1568172649, LEVEL=1, TTL=60]
Location hdfs://NS1/user/hive_admin/warehouse/bigdata_test.db/tt1
Serde Library org.apache.hadoop.hive.ql.io.orc.OrcSerde
InputFormat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
OutputFormat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
Storage Properties [serialization.format=1]
Partition Provider Catalog
Time taken: 0.371 seconds, Fetched 18 row(s)

```
2. Then I use another username='johnchen' and execute `alter table bigdata_test.tt1 set location 'hdfs://NS1/user/hive_admin/warehouse/bigdata_test.db/tt1'`, check the owner of hive table  is 'johnchen', it's unresonable
```
spark-sql> desc formatted bigdata_test.tt1;
col_name        data_type       comment
c       int     NULL
 
# Detailed Table Information
Database        bigdata_test
Table   tt1
Owner   johnchen
Created Time    Wed Sep 11 11:30:49 CST 2019
Last Access     Thu Jan 01 08:00:00 CST 1970
Created By      Spark 2.2 or prior
Type    MANAGED
Provider        hive
Table Properties        [transient_lastDdlTime=1568871017, PART_LIMIT=10000, LEVEL=1, TTL=60]
Location        hdfs://NS1/user/hive_admin/warehouse/bigdata_test.db/tt1
Serde Library   org.apache.hadoop.hive.ql.io.orc.OrcSerde
InputFormat     org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
OutputFormat    org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
Storage Properties      [serialization.format=1]
Partition Provider      Catalog
Time taken: 0.041 seconds, Fetched 18 row(s)

```





### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
In fact, the owner should not changed for the real prodcution environment, otherwise the  authority check is out of order.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Manual",spark,apache,southernriver,20614350,MDQ6VXNlcjIwNjE0MzUw,https://avatars3.githubusercontent.com/u/20614350?v=4,,https://api.github.com/users/southernriver,https://github.com/southernriver,https://api.github.com/users/southernriver/followers,https://api.github.com/users/southernriver/following{/other_user},https://api.github.com/users/southernriver/gists{/gist_id},https://api.github.com/users/southernriver/starred{/owner}{/repo},https://api.github.com/users/southernriver/subscriptions,https://api.github.com/users/southernriver/orgs,https://api.github.com/users/southernriver/repos,https://api.github.com/users/southernriver/events{/privacy},https://api.github.com/users/southernriver/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26889,https://github.com/apache/spark/pull/26889,https://github.com/apache/spark/pull/26889.diff,https://github.com/apache/spark/pull/26889.patch
64,https://api.github.com/repos/apache/spark/issues/26888,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26888/labels{/name},https://api.github.com/repos/apache/spark/issues/26888/comments,https://api.github.com/repos/apache/spark/issues/26888/events,https://github.com/apache/spark/pull/26888,537873165,MDExOlB1bGxSZXF1ZXN0MzUzMTQ2NjE4,26888,[SPARK-30260][SQL] Spark-Shell throw ClassNotFoundException exception for more than one statement to use UDF jar,[],open,False,,[],,5,2019-12-14T06:08:41Z,2019-12-15T07:58:06Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When we start spark-shell and use the udf for the first statement ,it's ok. But for the other statements it failed to load jar to current classpath and would throw ClassNotFoundException.It seems like that  the first  ClassLoader is different from the other's. For Spark-shell, the maintained class loader is always IMainsTranslatingClassLoader ,and for addJar Operation, the current classLoader is NonClosableMutuableclassLoader. For the first statement, there jar was loaded to right classLoader,and for other statements, the jar has been registered to functionRegistry and would not reload to NonClosableMutuableclassLoader, we need to  reset classloader to active sparkSession's. 

Here, I will show difference between  the first statement and the second.
First statement is NonClosableMutuableclassLoader:
<img width=""1073"" alt=""image"" src=""https://user-images.githubusercontent.com/20614350/70844299-05c71d00-1e7a-11ea-8457-9d9510b97ecc.png"">

Second statement is IMainsTranslatingClassLoader:
<img width=""883"" alt=""image"" src=""https://user-images.githubusercontent.com/20614350/70844323-648c9680-1e7a-11ea-8411-38899661373a.png"">



### Why are the changes needed?
<!--

-->
The problem can be reproduced as described in the below.

```
scala> val res = spark.sql(""select  bigdata_test.Add(1,2)"").show()
 ----------------------
 |bigdata_test.Add(1, 2)|
 ----------------------
 |                     3|
 ----------------------
 scala> val res = spark.sql(""select  bigdata_test.Add(1,2)"").show()
 org.apache.spark.sql.AnalysisException: No handler for UDF/UDAF/UDTF 'scala.didi.udf.Add': java.lang.ClassNotFoundException: scala.didi.udf.Add; line 1 pos 8
   at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
   at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
   at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
   at org.apache.spark.sql.hive.HiveShim$HiveFunctionWrapper.createFunction(HiveShim.scala:251)
   at org.apache.spark.sql.hive.HiveSimpleUDF.function$lzycompute(hiveUDFs.scala:56)
   at org.apache.spark.sql.hive.HiveSimpleUDF.function(hiveUDFs.scala:56)
   at org.apache.spark.sql.hive.HiveSimpleUDF.method$lzycompute(hiveUDFs.scala:60)
   at org.apache.spark.sql.hive.HiveSimpleUDF.method(hiveUDFs.scala:59)
   at org.apache.spark.sql.hive.HiveSimpleUDF.dataType$lzycompute(hiveUDFs.scala:77)
   at org.apache.spark.sql.hive.HiveSimpleUDF.dataType(hiveUDFs.scala:77)
   at org.apache.spark.sql.hive.HiveSessionCatalog$$anonfun$makeFunctionExpression$3.apply(HiveSessionCatalog.scala:79)
   at org.apache.spark.sql.hive.HiveSessionCatalog$$anonfun$makeFunctionExpression$3.apply(HiveSessionCatalog.scala:71)
   at scala.util.Try.getOrElse(Try.scala:79)
   at org.apache.spark.sql.hive.HiveSessionCatalog.makeFunctionExpression(HiveSessionCatalog.scala:71)
   at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$org$apache$spark$sql$catalyst$catalog$SessionCatalog$$makeFunctionBuilder$1.apply(SessionCatalog.scala:1133)

```

After fix:
```
scala> val res = spark.sql(""select  bigdata_test.Add(1,2)"").show()
+----------------------+                                                       
|bigdata_test.Add(1, 2)|
+----------------------+
|                     3|
+----------------------+
 
scala> val res = spark.sql(""select  bigdata_test.Add(1,2)"").show()
+----------------------+
|bigdata_test.Add(1, 2)|
+----------------------+
|                     3|
+----------------------+
```
we should resolve this bug!

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
manual. ",spark,apache,southernriver,20614350,MDQ6VXNlcjIwNjE0MzUw,https://avatars3.githubusercontent.com/u/20614350?v=4,,https://api.github.com/users/southernriver,https://github.com/southernriver,https://api.github.com/users/southernriver/followers,https://api.github.com/users/southernriver/following{/other_user},https://api.github.com/users/southernriver/gists{/gist_id},https://api.github.com/users/southernriver/starred{/owner}{/repo},https://api.github.com/users/southernriver/subscriptions,https://api.github.com/users/southernriver/orgs,https://api.github.com/users/southernriver/repos,https://api.github.com/users/southernriver/events{/privacy},https://api.github.com/users/southernriver/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26888,https://github.com/apache/spark/pull/26888,https://github.com/apache/spark/pull/26888.diff,https://github.com/apache/spark/pull/26888.patch
65,https://api.github.com/repos/apache/spark/issues/26884,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26884/labels{/name},https://api.github.com/repos/apache/spark/issues/26884/comments,https://api.github.com/repos/apache/spark/issues/26884/events,https://github.com/apache/spark/pull/26884,537720785,MDExOlB1bGxSZXF1ZXN0MzUzMDI1NjA3,26884,[SPARK-30257] [PySpark] Add simpleString map,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-13T19:06:16Z,2019-12-16T20:20:35Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?

- Adds a simple map function from simpleString to the equivalent Spark SQL type
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
- Mapping results of dtype() to equivalent Spark SQL types is annoying


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

- Exposes new method to PySpark API


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

https://issues.apache.org/jira/browse/SPARK-30257",spark,apache,vanhooser,6209145,MDQ6VXNlcjYyMDkxNDU=,https://avatars0.githubusercontent.com/u/6209145?v=4,,https://api.github.com/users/vanhooser,https://github.com/vanhooser,https://api.github.com/users/vanhooser/followers,https://api.github.com/users/vanhooser/following{/other_user},https://api.github.com/users/vanhooser/gists{/gist_id},https://api.github.com/users/vanhooser/starred{/owner}{/repo},https://api.github.com/users/vanhooser/subscriptions,https://api.github.com/users/vanhooser/orgs,https://api.github.com/users/vanhooser/repos,https://api.github.com/users/vanhooser/events{/privacy},https://api.github.com/users/vanhooser/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26884,https://github.com/apache/spark/pull/26884,https://github.com/apache/spark/pull/26884.diff,https://github.com/apache/spark/pull/26884.patch
66,https://api.github.com/repos/apache/spark/issues/26881,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26881/labels{/name},https://api.github.com/repos/apache/spark/issues/26881/comments,https://api.github.com/repos/apache/spark/issues/26881/events,https://github.com/apache/spark/pull/26881,537449678,MDExOlB1bGxSZXF1ZXN0MzUyODAyODQ2,26881,[SPARK-30252][SQL] Disallow negative scale of Decimal under ansi mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-12-13T09:48:45Z,2019-12-14T14:07:43Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR propose to disallow negative `scale` of Decimal under ansi mode in Spark. And this PR brings two behavior changes:

1) for literals like `1.23E4BD` or `1.23E4`(with `spark.sql.legacy.exponentLiteralAsDecimal.enabled`=true, see [SPARK-29956](https://issues.apache.org/jira/browse/SPARK-29956)), we set its `precision` and `scale` to 5 and 0 rather than to 3 and -2;
2) add negative `scale` check inside the decimal method if it exposes to set `scale` explicitly. If check fails, `AnalysisException` throws. 


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

According to SQL standard,
> 4.4.2 Characteristics of numbers
An exact numeric type has a precision P and a scale S. P is a positive integer that determines the number of significant digits in a particular radix R, where R is either 2 or 10. S is a non-negative integer.

scale of Decimal should always be non-negative. And other mainstream databases, like Presto, PostgreSQL, also don't allow negative scale.

Presto:
```
presto:default> create table t (i decimal(2, -1));
Query 20191213_081238_00017_i448h failed: line 1:30: mismatched input '-'. Expecting: <integer>, <type>
create table t (i decimal(2, -1))
```

PostgrelSQL:
```
postgres=# create table t(i decimal(2, -1));
ERROR:  NUMERIC scale -1 must be between 0 and precision 2
LINE 1: create table t(i decimal(2, -1));
                         ^
```

And, actually, Spark itself already doesn't allow to create table with negative decimal types using SQL:
```
scala> spark.sql(""create table t(i decimal(2, -1))"");
org.apache.spark.sql.catalyst.parser.ParseException:
no viable alternative at input 'create table t(i decimal(2, -'(line 1, pos 28)

== SQL ==
create table t(i decimal(2, -1))
----------------------------^^^

  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:263)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:130)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:76)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)
  ... 35 elided
```

However, it is still possible to create such table or `DatFrame` using Spark SQL programming API:
```
scala> val tb =
 CatalogTable(
  TableIdentifier(""test"", None),
  CatalogTableType.MANAGED, 
  CatalogStorageFormat.empty, 
  StructType(StructField(""i"", DecimalType(2, -1) ) :: Nil))
```
```
scala> spark.sql(""SELECT 1.23E4BD"")
res2: org.apache.spark.sql.DataFrame = [1.23E+4: decimal(3,-2)]
```
while, these two different behavior could make user confused.


On the other side, even if user creates such table or `DataFrame` with negative scale decimal type, it can't write data out if using format, like `parquet` or `orc`. Because these formats have their own check for negative scale and fail on it.
```
scala> spark.sql(""SELECT 1.23E4BD"").write.saveAsTable(""parquet"")
19/12/13 17:37:04 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.IllegalArgumentException: Invalid DECIMAL scale: -2
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:53)
	at org.apache.parquet.schema.Types$BasePrimitiveBuilder.decimalMetadata(Types.java:495)
	at org.apache.parquet.schema.Types$BasePrimitiveBuilder.build(Types.java:403)
	at org.apache.parquet.schema.Types$BasePrimitiveBuilder.build(Types.java:309)
	at org.apache.parquet.schema.Types$Builder.named(Types.java:290)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:428)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:334)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.$anonfun$convert$2(ParquetSchemaConverter.scala:326)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at org.apache.spark.sql.types.StructType.map(StructType.scala:99)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convert(ParquetSchemaConverter.scala:326)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.init(ParquetWriteSupport.scala:97)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:388)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:124)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:109)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

So, I think it would be better to disallow negative scale totally under ansi mode and make behaviors above be consistent.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

Yes, under ansi mode, user couldn't create Decimal value with negative scale anymore.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Added new tests in `ExpressionParserSuite` and `DecimalSuite`;
Updated existed test in `ansi/decimalArithmeticOperations.sql`.
",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26881,https://github.com/apache/spark/pull/26881,https://github.com/apache/spark/pull/26881.diff,https://github.com/apache/spark/pull/26881.patch
67,https://api.github.com/repos/apache/spark/issues/26875,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26875/labels{/name},https://api.github.com/repos/apache/spark/issues/26875/comments,https://api.github.com/repos/apache/spark/issues/26875/events,https://github.com/apache/spark/pull/26875,537347119,MDExOlB1bGxSZXF1ZXN0MzUyNzIwMTM3,26875,[SPARK-30245][SQL] Add cache for Like and RLike when pattern is not static,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-12-13T04:49:52Z,2019-12-24T10:22:13Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add cache for Like and RLike when pattern is not static


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
When pattern is not static, we should avoid compile pattern every time even if some pattern is same.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars0.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26875,https://github.com/apache/spark/pull/26875,https://github.com/apache/spark/pull/26875.diff,https://github.com/apache/spark/pull/26875.patch
68,https://api.github.com/repos/apache/spark/issues/26870,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26870/labels{/name},https://api.github.com/repos/apache/spark/issues/26870/comments,https://api.github.com/repos/apache/spark/issues/26870/events,https://github.com/apache/spark/pull/26870,537145603,MDExOlB1bGxSZXF1ZXN0MzUyNTU0MDE3,26870,[SPARK-30137][CORE] Support delete file feature in spark,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-12-12T18:34:54Z,2019-12-17T00:03:12Z,,CONTRIBUTOR,"
### What changes were proposed in this pull request?

Support Delete File semantics which will delete the given path from the `addedFiles` of sparkContext.


### Why are the changes needed?
User can delete the file if it is not required, so that for the next TaskSet this file won't be available. 


### Does this PR introduce any user-facing change?
Yes, new feature will be introduced to the user. Same will be updated in documentation as per the jira SPARK-30135


### How was this patch tested?
Added UT and also tested manually in the local cluster
",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26870,https://github.com/apache/spark/pull/26870,https://github.com/apache/spark/pull/26870.diff,https://github.com/apache/spark/pull/26870.patch
69,https://api.github.com/repos/apache/spark/issues/26868,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26868/labels{/name},https://api.github.com/repos/apache/spark/issues/26868/comments,https://api.github.com/repos/apache/spark/issues/26868/events,https://github.com/apache/spark/pull/26868,537060554,MDExOlB1bGxSZXF1ZXN0MzUyNDgzMjAw,26868,[SPARK-29665][SQL] refine the TableProvider interface,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-12-12T15:49:01Z,2019-12-23T18:25:22Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR separates `TableProvider` into 2 interfaces, to reach 2 goals:
1. for the common case, i.e. the data source has metastore and can report schema/partitioning easily, make the API super simple to implement.
2. for sources that also accept user-specified schema/partitioning (e.g. file source), make the API clear with explicit methods to infer schema/partition.

Now we have a simple `TableProvider`
```
interface TableProvider {
  Table getTable(properties);
}
```
This is very easy to implement, and also simple to reason about: lookup the table using the given properties. The returned table reports its actual schema/partitioning.

And we have a new mixin trait
```
public interface SupportsExternalMetadata extends TableProvider {
  StructType inferSchema(options);
  Transform[] inferPartitioning(options);
  Table getTable(schema, partitioning, properties);
}
```
This forces implementations to separate the logic of schema/partition inference from table loading, which is clearer. It also makes inference explicit.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
API improvement.

In the future, when we support v2 provider for tables in Spark's built-in generic catalog, `SupportsExternalMetadata` can be used as a flag to indicate that Spark should store the schema/partitioning in the builtin catalog to help to avoid expensive schema/partitioning inference at each table scan.

If a source only implements `TableProvider`, then Spark only stores table properties in the builtin catalog, and call `getTable(properties)` at each table scan. If a source also mix in `SupportsExternalMetadata`, then Spark stores schema, partitioning and propertiese in the builtin catalog, and call `getTable(schema, partitioning, properties)` at each table scan.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26868,https://github.com/apache/spark/pull/26868,https://github.com/apache/spark/pull/26868.diff,https://github.com/apache/spark/pull/26868.patch
70,https://api.github.com/repos/apache/spark/issues/26866,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26866/labels{/name},https://api.github.com/repos/apache/spark/issues/26866/comments,https://api.github.com/repos/apache/spark/issues/26866/events,https://github.com/apache/spark/pull/26866,537032189,MDExOlB1bGxSZXF1ZXN0MzUyNDU5NjA2,26866,[WIP][SPARK-30237][SQL] Move `sql()` method from DataType to AbstractTypemv sql method,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-12T15:02:21Z,2019-12-14T01:30:20Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Move `sql()` method from DataType to AbstractTypemv sql method

### Why are the changes needed?
Make all Spark Data type (such as `StructType`) can call method of `sql`


### Does this PR introduce any user-facing change?
NO


### How was this patch tested?
NO",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26866,https://github.com/apache/spark/pull/26866,https://github.com/apache/spark/pull/26866.diff,https://github.com/apache/spark/pull/26866.patch
71,https://api.github.com/repos/apache/spark/issues/26863,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26863/labels{/name},https://api.github.com/repos/apache/spark/issues/26863/comments,https://api.github.com/repos/apache/spark/issues/26863/events,https://github.com/apache/spark/pull/26863,536886606,MDExOlB1bGxSZXF1ZXN0MzUyMzM4MjAz,26863,[SPARK-30234]ADD FILE cannot add directories from sql CLI,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-12-12T10:24:31Z,2019-12-24T05:46:59Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Now users can add directories from sql CLI as well using ADD FILE command and setting spark.sql.addDirectory.recursive to true.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
In SPARK-4687, support was added for adding directories as resources. But sql users cannot use that feature from CLI. 

`ADD FILE /path/to/folder` gives the following error:
`org.apache.spark.SparkException: Added file /path/to/folder is a directory and recursive is not turned on.`

Users need to turn on `recursive` for adding directories. Thus a configuration was required which will allow users to turn on `recursive`. 
Also Hive allow users to add directories from their shell.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
Yes. Users can set recursive using `spark.sql.addDirectory.recursive`.
<!--
```
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Manually.
Will add test cases soon.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
 SPARK SCREENSHOTS
When `spark.sql.addDirectory.recursive` is not turned on.
![Screenshot from 2019-12-13 08-02-13](https://user-images.githubusercontent.com/15366835/70765124-c6b4a100-1d7f-11ea-9352-9c010af5b38b.png)

After setting `spark.sql.addDirectory.recursive` to true.

![Screenshot from 2019-12-13 08-02-59](https://user-images.githubusercontent.com/15366835/70765118-be5c6600-1d7f-11ea-9faf-0b1c46ee299b.png)

HIVE SCREENSHOT

![Screenshot from 2019-12-13 14-44-41](https://user-images.githubusercontent.com/15366835/70788979-17e08700-1db8-11ea-9c0c-b6d6f6e80a35.png)

`RELEASE_NOTES.txt` is text file while `dummy` is a directory.
",spark,apache,iRakson,15366835,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26863,https://github.com/apache/spark/pull/26863,https://github.com/apache/spark/pull/26863.diff,https://github.com/apache/spark/pull/26863.patch
72,https://api.github.com/repos/apache/spark/issues/26858,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26858/labels{/name},https://api.github.com/repos/apache/spark/issues/26858/comments,https://api.github.com/repos/apache/spark/issues/26858/events,https://github.com/apache/spark/pull/26858,536777317,MDExOlB1bGxSZXF1ZXN0MzUyMjQ4ODAw,26858,[SPARK-30120][ML] Use BoundedPriorityQueue for small dataset in LSH approxNearestNeighbors,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-12-12T06:10:29Z,2019-12-19T06:50:31Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Use BoundedPriorityQueue for small datasets in ```LSH.approxNearestNeighbors```


### Why are the changes needed?
For small datasets, we can get exact result instead of using ```approxQuantile```


### Does this PR introduce any user-facing change?
no


### How was this patch tested?
Use existing unit tests",spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26858,https://github.com/apache/spark/pull/26858,https://github.com/apache/spark/pull/26858.diff,https://github.com/apache/spark/pull/26858.patch
73,https://api.github.com/repos/apache/spark/issues/26852,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26852/labels{/name},https://api.github.com/repos/apache/spark/issues/26852/comments,https://api.github.com/repos/apache/spark/issues/26852/events,https://github.com/apache/spark/pull/26852,536512536,MDExOlB1bGxSZXF1ZXN0MzUyMDI5OTM0,26852,[SPARK-30221] Enhanced implementation of PrometheusPushGateWaySink,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-12-11T17:25:56Z,2019-12-12T05:23:42Z,,NONE,"### What changes were proposed in this pull request?
Enhanced implementation of PrometheusPushGateWaySink.

### Why are the changes needed?
PrometheusPushGateWaySink supports metrics push to  PrometheusPushGateWay.

### Does this PR introduce any user-facing change?
Added PrometheusPushGateWay class.

### How was this patch tested?
None",spark,apache,XuQianJin-Stars,10494131,MDQ6VXNlcjEwNDk0MTMx,https://avatars2.githubusercontent.com/u/10494131?v=4,,https://api.github.com/users/XuQianJin-Stars,https://github.com/XuQianJin-Stars,https://api.github.com/users/XuQianJin-Stars/followers,https://api.github.com/users/XuQianJin-Stars/following{/other_user},https://api.github.com/users/XuQianJin-Stars/gists{/gist_id},https://api.github.com/users/XuQianJin-Stars/starred{/owner}{/repo},https://api.github.com/users/XuQianJin-Stars/subscriptions,https://api.github.com/users/XuQianJin-Stars/orgs,https://api.github.com/users/XuQianJin-Stars/repos,https://api.github.com/users/XuQianJin-Stars/events{/privacy},https://api.github.com/users/XuQianJin-Stars/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26852,https://github.com/apache/spark/pull/26852,https://github.com/apache/spark/pull/26852.diff,https://github.com/apache/spark/pull/26852.patch
74,https://api.github.com/repos/apache/spark/issues/26850,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26850/labels{/name},https://api.github.com/repos/apache/spark/issues/26850/comments,https://api.github.com/repos/apache/spark/issues/26850/events,https://github.com/apache/spark/pull/26850,536244202,MDExOlB1bGxSZXF1ZXN0MzUxODA4ODY4,26850,[SPARK-30215][SQL] Remove PrunedInMemoryFileIndex and merge its functionality into InMemoryFileIndex,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-11T09:27:16Z,2019-12-13T07:46:24Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Remove PrunedInMemoryFileIndex and merge its functionality into InMemoryFileIndex.


### Why are the changes needed?
PrunedInMemoryFileIndex is only used in CatalogFileIndex.filterPartitions, and its name is kind of confusing, we can completely merge its functionality into InMemoryFileIndex and remove the class.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Existing unit tests.
",spark,apache,fuwhu,12389745,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26850,https://github.com/apache/spark/pull/26850,https://github.com/apache/spark/pull/26850.diff,https://github.com/apache/spark/pull/26850.patch
75,https://api.github.com/repos/apache/spark/issues/26847,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26847/labels{/name},https://api.github.com/repos/apache/spark/issues/26847/comments,https://api.github.com/repos/apache/spark/issues/26847/events,https://github.com/apache/spark/pull/26847,536198305,MDExOlB1bGxSZXF1ZXN0MzUxNzcyMDQ0,26847,[SPARK-30214][SQL] Support COMMENT ON syntax,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2019-12-11T07:46:20Z,2019-12-23T04:30:34Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

As the new design of catalog v2, some properties become reserved, e.g. `location`, `comment`. We are going to disable setting reserved properties by dbproperties or tblproperites directly to avoid confliction with their related subClause or specific commands. 

For `comment`, there is no existing syntax to alter yet, so this pull request is to add those.
```sql
COMMENT ON (DATABASE|SCHEMA|NAMESPACE) ... IS ...
COMMENT ON TABLE ... IS ...
```
They are the best practices from PostgreSQL and presto.

https://www.postgresql.org/docs/12/sql-comment.html
https://prestosql.io/docs/current/sql/comment.html



### Why are the changes needed?
Comming feature support.

### Does this PR introduce any user-facing change?
yes, add new syntax


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add uts.",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26847,https://github.com/apache/spark/pull/26847,https://github.com/apache/spark/pull/26847.diff,https://github.com/apache/spark/pull/26847.patch
76,https://api.github.com/repos/apache/spark/issues/26838,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26838/labels{/name},https://api.github.com/repos/apache/spark/issues/26838/comments,https://api.github.com/repos/apache/spark/issues/26838/events,https://github.com/apache/spark/pull/26838,535889196,MDExOlB1bGxSZXF1ZXN0MzUxNTE4MDcy,26838,[SPARK-30144][ML][PySpark] Make MultilayerPerceptronClassificationModel extend MultilayerPerceptronParams,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2019-12-10T17:52:23Z,2019-12-24T01:52:55Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Make ```MultilayerPerceptronClassificationModel``` extend ```MultilayerPerceptronParams```


### Why are the changes needed?
Make ```MultilayerPerceptronClassificationModel``` extend ```MultilayerPerceptronParams``` to expose the training params, so user can see these params when calling ```extractParamMap```


### Does this PR introduce any user-facing change?
Yes. The ```MultilayerPerceptronParams``` such as ```seed```, ```maxIter``` ... are available in ```MultilayerPerceptronClassificationModel``` now


### How was this patch tested?
Manually tested ```MultilayerPerceptronClassificationModel.extractParamMap()``` to verify all the new params are there. 
",spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26838,https://github.com/apache/spark/pull/26838,https://github.com/apache/spark/pull/26838.diff,https://github.com/apache/spark/pull/26838.patch
77,https://api.github.com/repos/apache/spark/issues/26832,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26832/labels{/name},https://api.github.com/repos/apache/spark/issues/26832/comments,https://api.github.com/repos/apache/spark/issues/26832/events,https://github.com/apache/spark/pull/26832,535662969,MDExOlB1bGxSZXF1ZXN0MzUxMzM1NjI1,26832,[SPARK-30202][WIP][ML][PYSPARK] impl QuantileTransform,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-10T11:18:40Z,2019-12-23T11:59:53Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Impl QuantileTransform, a non-parametric transformation to map the data to another
distribution
The impl followed scikit-learn' s impl, however there still are sereral differences:
1, use `QuantileSummaries` for approximation, no matter the size of dataset;
2, use linear interpolate, the logic is similar to existing `IsotonicRegression`, while scikit-learn use a bi-directional interpolate (the two methods only differ on a special case that  values are repeated in the features);
3, treat sparse vectors just like dense ones, while scikit-learn have two different logics for sparse and dense datasets.

### Why are the changes needed?
1, it is common to map the data to another desired distribution, and was already impled in scikit-learn
2, it is easy for parallelism

### Does this PR introduce any user-facing change?
Yes, a new model


### How was this patch tested?
added testsuites
",spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26832,https://github.com/apache/spark/pull/26832,https://github.com/apache/spark/pull/26832.diff,https://github.com/apache/spark/pull/26832.patch
78,https://api.github.com/repos/apache/spark/issues/26821,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26821/labels{/name},https://api.github.com/repos/apache/spark/issues/26821/comments,https://api.github.com/repos/apache/spark/issues/26821/events,https://github.com/apache/spark/pull/26821,535159240,MDExOlB1bGxSZXF1ZXN0MzUwOTA4OTYw,26821,[SPARK-20656][CORE]Support Incremental parsing of event logs in SHS,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-12-09T19:03:13Z,2019-12-12T14:58:00Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Currently loading application page from history server is time consuming, if the eventlog size is high. (Eg: ~47 minutes for a 18GB eventlog file). Currently, when there is any changes in event log, history server parses the entire eventLog even for smaller changes in eventLog. In this PR, we are supporting incremental parsing of event log by storing the in memory store to a hash map and will not close the store until it is valid.


### Why are the changes needed?
To speed up loading history server page
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
No
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Added UT, Manual tests and existing tests.

Created an eventLog of size ~2GB. Added a few more events. Loading time without the PR is ~1 minute and after the PR, it is around 2 secs.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->",spark,apache,shahidki31,23054875,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26821,https://github.com/apache/spark/pull/26821,https://github.com/apache/spark/pull/26821.diff,https://github.com/apache/spark/pull/26821.patch
79,https://api.github.com/repos/apache/spark/issues/26816,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26816/labels{/name},https://api.github.com/repos/apache/spark/issues/26816/comments,https://api.github.com/repos/apache/spark/issues/26816/events,https://github.com/apache/spark/pull/26816,534955816,MDExOlB1bGxSZXF1ZXN0MzUwNzM0OTYy,26816,[SPARK-30191][YARN] optimize yarn allocator,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-12-09T14:09:30Z,2019-12-12T14:32:29Z,,NONE,"### What changes were proposed in this pull request?
add the factor `numExecutorsExiting` in YarnAllocator's method `updateResourceRequests` 

### Why are the changes needed?
when driver lost its executors because of machine hardware problem and all of service includes nodemanager, executor on the node has been killed,  it means that Resourcemanager can't update the containers info on the node until Resourcemanager try to remove the node,   but it always takes 10 mins or longger, and in the meantime, AM doesn't add the new resource request and driver missing executors. 
so maybe AM should add the factor `numExecutorsExiting` in YarnAllocator's method `updateResourceRequests`  to optimize it.


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No


### How was this patch tested?
add test(""lost executor removed from driver"") 
",spark,apache,Neilxzn,10757009,MDQ6VXNlcjEwNzU3MDA5,https://avatars1.githubusercontent.com/u/10757009?v=4,,https://api.github.com/users/Neilxzn,https://github.com/Neilxzn,https://api.github.com/users/Neilxzn/followers,https://api.github.com/users/Neilxzn/following{/other_user},https://api.github.com/users/Neilxzn/gists{/gist_id},https://api.github.com/users/Neilxzn/starred{/owner}{/repo},https://api.github.com/users/Neilxzn/subscriptions,https://api.github.com/users/Neilxzn/orgs,https://api.github.com/users/Neilxzn/repos,https://api.github.com/users/Neilxzn/events{/privacy},https://api.github.com/users/Neilxzn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26816,https://github.com/apache/spark/pull/26816,https://github.com/apache/spark/pull/26816.diff,https://github.com/apache/spark/pull/26816.patch
80,https://api.github.com/repos/apache/spark/issues/26815,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26815/labels{/name},https://api.github.com/repos/apache/spark/issues/26815/comments,https://api.github.com/repos/apache/spark/issues/26815/events,https://github.com/apache/spark/pull/26815,534909878,MDExOlB1bGxSZXF1ZXN0MzUwNjk3MTc0,26815,[SPARK-30189][SQL] Interval from year-month/date-time string should handle whitespaces,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-12-09T12:51:10Z,2019-12-09T22:43:06Z,,CONTRIBUTOR,"

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?

Currently, we parse interval from multi units strings or from date-time/year-month pattern strings, the former handles all whitespace, the latter not or even spaces.

### Why are the changes needed?

behavior consistency

### Does this PR introduce any user-facing change?
yes, interval in date-time/year-month like
```
select interval '\n-\t10\t 12:34:46.789\t' day to second
-- !query 126 schema
struct<INTERVAL '-10 days -12 hours -34 minutes -46.789 seconds':interval>
-- !query 126 output
-10 days -12 hours -34 minutes -46.789 seconds
``` 
is valid now.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

add ut.",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26815,https://github.com/apache/spark/pull/26815,https://github.com/apache/spark/pull/26815.diff,https://github.com/apache/spark/pull/26815.patch
81,https://api.github.com/repos/apache/spark/issues/26814,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26814/labels{/name},https://api.github.com/repos/apache/spark/issues/26814/comments,https://api.github.com/repos/apache/spark/issues/26814/events,https://github.com/apache/spark/pull/26814,534904694,MDExOlB1bGxSZXF1ZXN0MzUwNjkyOTcx,26814,[SPARK-30186][SQL] support Dynamic Partition Pruning in Adaptive Execution,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-12-09T12:41:43Z,2019-12-13T10:52:02Z,,NONE,"### What changes were proposed in this pull request?
To support queries with dynamic partition pruning subqueries work in adaptive query execution.


### Why are the changes needed?
Queries' performance can benefit from AE and DPP at the same time.


### Does this PR introduce any user-facing change?
NO


### How was this patch tested?
Test cases are added.
",spark,apache,chrysan,5151781,MDQ6VXNlcjUxNTE3ODE=,https://avatars1.githubusercontent.com/u/5151781?v=4,,https://api.github.com/users/chrysan,https://github.com/chrysan,https://api.github.com/users/chrysan/followers,https://api.github.com/users/chrysan/following{/other_user},https://api.github.com/users/chrysan/gists{/gist_id},https://api.github.com/users/chrysan/starred{/owner}{/repo},https://api.github.com/users/chrysan/subscriptions,https://api.github.com/users/chrysan/orgs,https://api.github.com/users/chrysan/repos,https://api.github.com/users/chrysan/events{/privacy},https://api.github.com/users/chrysan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26814,https://github.com/apache/spark/pull/26814,https://github.com/apache/spark/pull/26814.diff,https://github.com/apache/spark/pull/26814.patch
82,https://api.github.com/repos/apache/spark/issues/26813,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26813/labels{/name},https://api.github.com/repos/apache/spark/issues/26813/comments,https://api.github.com/repos/apache/spark/issues/26813/events,https://github.com/apache/spark/pull/26813,534901837,MDExOlB1bGxSZXF1ZXN0MzUwNjkwNjc1,26813,[SPARK-30188][SQL][WIP] Enable adaptive query execution by default,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2019-12-09T12:36:02Z,2019-12-16T08:07:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Enable adaptive query execution default
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
To expand the usage of AQE.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
No
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Existing unit tests
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,JkSelf,11972570,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26813,https://github.com/apache/spark/pull/26813,https://github.com/apache/spark/pull/26813.diff,https://github.com/apache/spark/pull/26813.patch
83,https://api.github.com/repos/apache/spark/issues/26809,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26809/labels{/name},https://api.github.com/repos/apache/spark/issues/26809/comments,https://api.github.com/repos/apache/spark/issues/26809/events,https://github.com/apache/spark/pull/26809,534832765,MDExOlB1bGxSZXF1ZXN0MzUwNjM2MzI1,26809,[SPARK-30185][SQL] Implement Dataset.tail API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-12-09T10:28:28Z,2019-12-24T08:31:20Z,,MEMBER,"### What changes were proposed in this pull request?

This PR proposes a `tail` API. 

Namely, as below:

```scala
scala> spark.range(10).head(5)
res1: Array[Long] = Array(0, 1, 2, 3, 4)
scala> spark.range(10).tail(5)
res2: Array[Long] = Array(5, 6, 7, 8, 9)
```

Implementation details will be similar with `head` but it will be reversed:

1. Run the job against the last partition and collect rows. If this is enough, return as is.
2. If this is not enough, calculate the number of partitions to select more based upon
 `spark.sql.limit.scaleUpFactor`
3. Run more jobs against more partitions (in a reversed order compared to head) as many as the number calculated from 2.
4. Go to 2.

**Note that**, we don't guarantee the natural order in DataFrame in general - there are cases when it's deterministic and when it's not. We probably should write down this as a caveat separately.

### Why are the changes needed?

Many other systems support the way to take data from the end, for instance, pandas[1] and
 Python[2][3]. Scala collections APIs also have head and tail

On the other hand, in Spark, we only provide a way to take data from the start
 (e.g., DataFrame.head).

This has been requested multiple times here and there in Spark user mailing list[4], StackOverFlow[5][6], JIRA[7] and other third party projects such as
 Koalas[8]. In addition, this missing API seems explicitly mentioned in comparison to another system[9] time to time.

It seems we're missing non-trivial use case in Spark and this motivated me to propose this API.

[1] https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html?highlight=tail#pandas.DataFrame.tail
[2] https://stackoverflow.com/questions/10532473/head-and-tail-in-one-line
[3] https://stackoverflow.com/questions/646644/how-to-get-last-items-of-a-list-in-python
[4] http://apache-spark-user-list.1001560.n3.nabble.com/RDD-tail-td4217.html
[5] https://stackoverflow.com/questions/39544796/how-to-select-last-row-and-also-how-to-access-pyspark-dataframe-by-index
[6] https://stackoverflow.com/questions/45406762/how-to-get-the-last-row-from-dataframe
[7] https://issues.apache.org/jira/browse/SPARK-26433
[8] https://github.com/databricks/koalas/issues/343
[9] https://medium.com/@chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2

### Does this PR introduce any user-facing change?

No, (new API)

### How was this patch tested?

Unit tests were added and manually tested.
",spark,apache,HyukjinKwon,6477701,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26809,https://github.com/apache/spark/pull/26809,https://github.com/apache/spark/pull/26809.diff,https://github.com/apache/spark/pull/26809.patch
84,https://api.github.com/repos/apache/spark/issues/26806,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26806/labels{/name},https://api.github.com/repos/apache/spark/issues/26806/comments,https://api.github.com/repos/apache/spark/issues/26806/events,https://github.com/apache/spark/pull/26806,534781781,MDExOlB1bGxSZXF1ZXN0MzUwNTk0NzI5,26806,[SPARK-30183][SQL] Disallow to specify reserved properties in CREATE ‚Ä¶,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-12-09T08:56:46Z,2019-12-10T14:26:15Z,,CONTRIBUTOR,"‚Ä¶NAMESPACE syntax

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Currently, COMMENT and LOCATION are reserved properties for Datasource v2 namespaces. They can be set via specific clauses and via properties. And the ones specified in clauses take precede of properties. Since they are reserved, which means they are not able to visit directly. They should be used in COMMENT/LOCATION clauses ONLY.

### Why are the changes needed?
make reserved properties be reserved.


### Does this PR introduce any user-facing change?
yes, 'location', 'comment' are not allowed use in db properties


### How was this patch tested?
UNIT tests.
",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26806,https://github.com/apache/spark/pull/26806,https://github.com/apache/spark/pull/26806.diff,https://github.com/apache/spark/pull/26806.patch
85,https://api.github.com/repos/apache/spark/issues/26805,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26805/labels{/name},https://api.github.com/repos/apache/spark/issues/26805/comments,https://api.github.com/repos/apache/spark/issues/26805/events,https://github.com/apache/spark/pull/26805,534768868,MDExOlB1bGxSZXF1ZXN0MzUwNTg0MjYx,26805,[SPARK-15616][SQL] Add optimizer rule PruneHiveTablePartitions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-12-09T08:27:55Z,2019-12-13T08:27:03Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Add optimizer rule PruneHiveTablePartitions pruning hive table partitions based on filters on partition columns.
Doing so, the total size of pruned partitions may be small enough for broadcast join in JoinSelection strategy.

### Why are the changes needed?
In JoinSelection strategy, spark use the ""plan.stats.sizeInBytes"" to decide whether the plan is suitable for broadcast join.
Currently, ""plan.stats.sizeInBytes"" does not take ""pruned partitions"" into account, so it may miss some broadcast join and take sort-merge join instead, which will definitely impact join performance.
This PR aim at taking ""pruned partitions"" into account for hive table in ""plan.stats.sizeInBytes"" and then improve performance by using broadcast join if possible.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Added unit tests.

This is based on #25919, credits should go to @lianhuiwang and @advancedxy.
",spark,apache,fuwhu,12389745,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26805,https://github.com/apache/spark/pull/26805,https://github.com/apache/spark/pull/26805.diff,https://github.com/apache/spark/pull/26805.patch
86,https://api.github.com/repos/apache/spark/issues/26804,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26804/labels{/name},https://api.github.com/repos/apache/spark/issues/26804/comments,https://api.github.com/repos/apache/spark/issues/26804/events,https://github.com/apache/spark/pull/26804,534726133,MDExOlB1bGxSZXF1ZXN0MzUwNTQ5MDkw,26804,[WIP][SPARK-26346][BUILD][SQL] Upgrade parquet to 1.11.0,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2019-12-09T06:49:13Z,2019-12-19T04:22:05Z,,MEMBER,"### What changes were proposed in this pull request?

This PR upgrade parquet to 1.11.0.

Note that:
I just verify that all tests passed now. I will do a benchmark later.


### Why are the changes needed?



### Does this PR introduce any user-facing change?
Unknown


### How was this patch tested?

",spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26804,https://github.com/apache/spark/pull/26804,https://github.com/apache/spark/pull/26804.diff,https://github.com/apache/spark/pull/26804.patch
87,https://api.github.com/repos/apache/spark/issues/26803,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26803/labels{/name},https://api.github.com/repos/apache/spark/issues/26803/comments,https://api.github.com/repos/apache/spark/issues/26803/events,https://github.com/apache/spark/pull/26803,534701477,MDExOlB1bGxSZXF1ZXN0MzUwNTI4ODU3,26803,[SPARK-30178][ML] RobustScaler support large numFeatures,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-12-09T05:33:06Z,2019-12-24T09:46:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
compute the medians/ranges more distributedly

### Why are the changes needed?
It is a bottleneck to collect the whole Array[QuantileSummaries] from executors,
since a QuantileSummaries is a large object, which maintains arrays of large sizes 10k(`defaultCompressThreshold`)/50k(`defaultHeadSize`).

In Spark-Shell with default params, I processed a dataset with numFeatures=69,200, and existing impl fail due to OOM.
After this PR, it will sucessfuly fit the model.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
existing testsuites
",spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26803,https://github.com/apache/spark/pull/26803,https://github.com/apache/spark/pull/26803.diff,https://github.com/apache/spark/pull/26803.patch
88,https://api.github.com/repos/apache/spark/issues/26801,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26801/labels{/name},https://api.github.com/repos/apache/spark/issues/26801/comments,https://api.github.com/repos/apache/spark/issues/26801/events,https://github.com/apache/spark/pull/26801,534662767,MDExOlB1bGxSZXF1ZXN0MzUwNDk3NTU4,26801,[SPARK-29553][ML][MLLIB]Add options to disable multi-threading of native BLAS on the executors,[],open,False,,[],,10,2019-12-09T03:05:49Z,2019-12-10T12:59:54Z,,NONE,"**What changes were proposed in this pull request?**
I use native BLAS to improvement ML/MLLIB performance on Yarn.
The file spark-env.sh which is modified by SPARK-21305 said that I should set OPENBLAS_NUM_THREADS=1 to disable multi-threading of OpenBLAS, but it does not take effect on the executor.
I modify spark.conf to set  spark.executorEnv.OPENBLAS_NUM_THREADS=1Ôºåand the performance improve.
see https://issues.apache.org/jira/browse/SPARK-29553

**How was this patch tested?**
The existing UT.
",spark,apache,Zeyiii,40429486,MDQ6VXNlcjQwNDI5NDg2,https://avatars2.githubusercontent.com/u/40429486?v=4,,https://api.github.com/users/Zeyiii,https://github.com/Zeyiii,https://api.github.com/users/Zeyiii/followers,https://api.github.com/users/Zeyiii/following{/other_user},https://api.github.com/users/Zeyiii/gists{/gist_id},https://api.github.com/users/Zeyiii/starred{/owner}{/repo},https://api.github.com/users/Zeyiii/subscriptions,https://api.github.com/users/Zeyiii/orgs,https://api.github.com/users/Zeyiii/repos,https://api.github.com/users/Zeyiii/events{/privacy},https://api.github.com/users/Zeyiii/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26801,https://github.com/apache/spark/pull/26801,https://github.com/apache/spark/pull/26801.diff,https://github.com/apache/spark/pull/26801.patch
89,https://api.github.com/repos/apache/spark/issues/26789,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26789/labels{/name},https://api.github.com/repos/apache/spark/issues/26789/comments,https://api.github.com/repos/apache/spark/issues/26789/events,https://github.com/apache/spark/pull/26789,534321745,MDExOlB1bGxSZXF1ZXN0MzUwMjUwNzEy,26789,[SPARK-30160][K8S] Introduce the ExecutorPodController API ,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-07T00:34:32Z,2019-12-10T00:37:08Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
I am putting existing functionality behind an API that allows for developers to customize how executors are brought up and down. 

### Why are the changes needed?
 Companies may require that executor creation be done by privileged resources (like admin controllers / operators who would receive events or updates) that would create the Kubernetes Executor pods on-behalf of the user. In essence, the assumption that the driver has the appropriate service-account to create pods is not a guarantee in various organizations.



### Does this PR introduce any user-facing change?
No


### How was this patch tested?
- [x] Unit tests
- [x] integration tests
- [x] Ran an implementation of the API against dev and prod, extending a custom Kubernetes CRD
",spark,apache,ifilonenko,4926714,MDQ6VXNlcjQ5MjY3MTQ=,https://avatars1.githubusercontent.com/u/4926714?v=4,,https://api.github.com/users/ifilonenko,https://github.com/ifilonenko,https://api.github.com/users/ifilonenko/followers,https://api.github.com/users/ifilonenko/following{/other_user},https://api.github.com/users/ifilonenko/gists{/gist_id},https://api.github.com/users/ifilonenko/starred{/owner}{/repo},https://api.github.com/users/ifilonenko/subscriptions,https://api.github.com/users/ifilonenko/orgs,https://api.github.com/users/ifilonenko/repos,https://api.github.com/users/ifilonenko/events{/privacy},https://api.github.com/users/ifilonenko/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26789,https://github.com/apache/spark/pull/26789,https://github.com/apache/spark/pull/26789.diff,https://github.com/apache/spark/pull/26789.patch
90,https://api.github.com/repos/apache/spark/issues/26783,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26783/labels{/name},https://api.github.com/repos/apache/spark/issues/26783/comments,https://api.github.com/repos/apache/spark/issues/26783/events,https://github.com/apache/spark/pull/26783,534127514,MDExOlB1bGxSZXF1ZXN0MzUwMDg5MDY2,26783,[SPARK-30153][PYTHON][WIP] Extend data exchange options for vectorized UDF functions with vanilla Arrow serialization,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-12-06T16:36:59Z,2019-12-11T10:32:37Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This proposes to extend data exchange options for vectorized UDF functions, by introducing the possibility to have plain Arrow serialization without conversions to Pandas. 
This PR proposes to introduce SCALAR_ARROW pandas_udf type as a step in that direction. As detailed in the test case below, the performance results are quite promising, in particular when processing data sets with numerical arrays, showing a speedup of 3x in the test case.

### Why are the changes needed?
Having the possibility to deploy vectorized Python UDFs that bypass the conversion to_pandas, can provide performance improvements (see test below) and extended flexibility (as the Arrow type system is broader than Pandas's) for operating data manipulation, in particular this is relevant for data sets including numerical arrays, possibly of variable length. While conversion to Pandas covers many cases already for vectorized Python UDF in a satisfying manner, the proposed improvement applies to certain special cases, notably of interested for scientific computing (such as in high energy physics), where performant processing of arrays of variable length is of paramount importance.

A test case shows a measured increase of performance of about 3x using the proposed approach, compared to the equivalent processing with pandas_udf (details below in the test section).  Processing of Arrow data in the test case was done via the ""awkward arrays"" library (https://github.com/scikit-hep/awkward-array).
In general, we can imagine that more specialized libraries using Arrow for data exchange can appear in the future to address computational needs in specific domains, possibly covering also cases of ML and DL libraries. It can be advantageous for Apache Spark to add the possibility to hook to such libraries, as proposed in the PR or with similar methods, for gains in performance and extra flexibility.

### Does this PR introduce any user-facing change?
This PR introduces an extension of pandas_udf types with a new SCALAR_ARROW type.

### How was this patch tested?
1. Manually tested:  a proposed ""SCALAR_ARROW"" pandas_udf test, detailed below, runs in 21 seconds vs. 62 seconds of the corresponding version with standard ""pandas_udf SCALAR"" code. This is close to a 3x speedup. The test is implemented using the awkward array library https://pypi.org/project/awkward/ for data processing, in alternative to Pandas:
```
bin/pyspark --master local[2]

df = sql(""select cast(1.0 as double) col1, rand(42) col2, Array(rand(),rand(),rand()) col3 from range(1e8)"")
df.printSchema()

root
 |-- col1: double (nullable = false)
 |-- col2: double (nullable = false)
 |-- col3: array (nullable = false)
 |    |-- element: double (containsNull = false)


from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql.types import *
import awkward

@pandas_udf(ArrayType(DoubleType()), PandasUDFType.SCALAR_ARROW)
def test_arrow(col1):
    b = awkward.fromarrow(col1.chunk(0))
    c = awkward.toarrow(b * b)
    return c 


import time
start = time.time()
df.withColumn('test', test_arrow(df.col3)).write.format(""noop"").mode(""overwrite"").save()
end = time.time()
print(end - start)
```

2. This is the reference code with pandas_udf SCALAR (this runs in 62 seconds vs. 21 seconds of the code above with SCALAR_ARROW implementation):
```
@pandas_udf(ArrayType(DoubleType()), PandasUDFType.SCALAR)
def test_pandas(col1):
  return col1*col1

import time
start = time.time()
df.withColumn('test', test_pandas(df.col3)).write.format(""noop"").mode(""overwrite"").save()
end = time.time()
print(end - start)
```

3. This is how the test workload can be implemented using Spark SQL (with higher order SQL functions). This test rund in 11 seconds
```
import time
start = time.time()
df.selectExpr(""col1"", ""col2"", ""col3"", ""transform(col3, x -> x * x) as test"").write.format(""noop"").mode(""overwrite"").save()
end = time.time()
print(end - start)
```

In conclusion the test workload runs 6x slower than Spark SQL with pandas_udf of type SCALAR, and 2x slower than Spark SQL with the proposed arrow-only serialization with SCALAR_PYARROW pandas_udf.

----
In addition, we collected and compared Flamegraph profiling of the Python code execution on the Python daemons, in the 2 Python UDF cases tested (SCALAR UDF and nthe proposed SCALAR_ARROW type). Results show that in the SCALAR case **an important amount of time is spent in the to_pandas conversion** operations and in array processing, the SCALAR_ARROW case optimizes processing of array data and avoid conversion to Pandas for array data, thus saving considerable time. 

This is a Flamegraph of workload on the Python daemon when running the pandas_udf SCALAR_ARROW:
![](https://issues.apache.org/jira/secure/attachment/12987761/Flamegraph_test_pandas_udf_SCALAR_ARROW.png)

This is a Flamegraph of workload on the Python daemon when running the pandas_udf SCALAR:
![](https://issues.apache.org/jira/secure/attachment/12987760/Flamegraph_test_pandas_udf_SCALAR.png)

Co-authors of this work:
Luca Canali @LucaCanali 
Lindsey Gray @lgray
Jim Pivarski @jpivarski
",spark,apache,LucaCanali,5243162,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26783,https://github.com/apache/spark/pull/26783,https://github.com/apache/spark/pull/26783.diff,https://github.com/apache/spark/pull/26783.patch
91,https://api.github.com/repos/apache/spark/issues/26777,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26777/labels{/name},https://api.github.com/repos/apache/spark/issues/26777/comments,https://api.github.com/repos/apache/spark/issues/26777/events,https://github.com/apache/spark/pull/26777,533824803,MDExOlB1bGxSZXF1ZXN0MzQ5ODI5MTYy,26777,[SPARK-30134][SQL]  Support DELETE JAR feature in SPARK,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-12-06T08:46:06Z,2019-12-16T10:38:10Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
 
Support DELETE JAR functionality in spark. On deletion the jar will be removed from the `IsolatedClientLoader` classpath and from `SharedState` classpath. It also removes the jar from `addedJars` map, so that next set of taskSet won't get these jars

**Sequence Diagram**

![DeleteJarFlow](https://user-images.githubusercontent.com/35216143/70412690-61a93480-1a7b-11ea-9e1d-81d7fc60b8f4.png)

`IsolatedClientLoader.deleteJar` Deletes the jar from `hiveClassLoader` and recreates the classLaoder
`SparkContext.deleteJar` removes the jar from `addedJars` list
`SharedState.deleteJar` Removes the jar from the sessionState class Loader


### Why are the changes needed?
If the jar definition is changed, use can delete the jar and add new one. This process does not require service to be restarted. Even Hive supports the DELETE jar feature.

### Does this PR introduce any user-facing change?
Yes, new feature will be introduced to the user. Same will be updated in documentation as per the jira SPARK-30135
### How was this patch tested?

Added UT and also tested maually with the following testcases
1. Tested in spark-shell,spark-sql and beeline
2. With no schema like /opt/somepath/somejar.jar, hdfs
3.  With fullpath or only with jarname
4.  dropping invalid jar
5. Tested with Hive-2.3.6 

",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26777,https://github.com/apache/spark/pull/26777,https://github.com/apache/spark/pull/26777.diff,https://github.com/apache/spark/pull/26777.patch
92,https://api.github.com/repos/apache/spark/issues/26775,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26775/labels{/name},https://api.github.com/repos/apache/spark/issues/26775/comments,https://api.github.com/repos/apache/spark/issues/26775/events,https://github.com/apache/spark/pull/26775,533777961,MDExOlB1bGxSZXF1ZXN0MzQ5NzkwOTAz,26775,[SPARK-30018][SQL] Support ALTER DATABASE SET OWNER syntax,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-12-06T06:43:21Z,2019-12-11T14:26:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
In this pull request, we are going to support `SET OWNER` syntax for databases and namespaces,

```sql
ALTER (DATABASE|SCHEME|NAMESPACE) database_name SET OWNER [USER|ROLE|GROUP] user_or_role_group; 
```
Before this commit https://github.com/apache/spark/commit/332e252a1448a27cfcfc1d1d794f7979e6cd331a, we didn't care much about ownerships for the catalog objects. In https://github.com/apache/spark/commit/332e252a1448a27cfcfc1d1d794f7979e6cd331a, we determined to use properties to store ownership staff, and temporarily used `alter database ... set dbproperties ...` to support switch ownership of a database. This PR aims to use the formal syntax to replace it.

In hive, `ownerName/Type` are fields of the database objects, also they can be normal properties.
```
create schema test1 with dbproperties('ownerName'='yaooqinn')
```
The create/alter database syntax will not change the owner to `yaooqinn` but store it in parameters. e.g.
```
+----------+----------+---------------------------------------------------------------+-------------+-------------+-----------------------+--+
| db_name  | comment  |                           location                            | owner_name  | owner_type  |      parameters       |
+----------+----------+---------------------------------------------------------------+-------------+-------------+-----------------------+--+
| test1    |          | hdfs://quickstart.cloudera:8020/user/hive/warehouse/test1.db  | anonymous   | USER        | {ownerName=yaooqinn}  |
+----------+----------+---------------------------------------------------------------+-------------+-------------+-----------------------+--+
```
In this pull request, because we let the `ownerName` become reversed, so it will neither change the owner nor store in dbproperties, just be omitted silently.



## Why are the changes needed?

Formal syntax support for changing database ownership


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
yes, add a new syntax

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

add unit tests",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26775,https://github.com/apache/spark/pull/26775,https://github.com/apache/spark/pull/26775.diff,https://github.com/apache/spark/pull/26775.patch
93,https://api.github.com/repos/apache/spark/issues/26772,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26772/labels{/name},https://api.github.com/repos/apache/spark/issues/26772/comments,https://api.github.com/repos/apache/spark/issues/26772/events,https://github.com/apache/spark/pull/26772,533695939,MDExOlB1bGxSZXF1ZXN0MzQ5NzI0NzMx,26772,[SPARK-28210][CORE] Introducing Shuffle Reader API,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-12-06T01:43:16Z,2019-12-17T16:58:12Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Extending the ExecutorComponents to include reader code


### Why are the changes needed?
TODO


### Does this PR introduce any user-facing change?
TODO


### How was this patch tested?
- [x] Unit tests

- [ ] External implementation
",spark,apache,ifilonenko,4926714,MDQ6VXNlcjQ5MjY3MTQ=,https://avatars1.githubusercontent.com/u/4926714?v=4,,https://api.github.com/users/ifilonenko,https://github.com/ifilonenko,https://api.github.com/users/ifilonenko/followers,https://api.github.com/users/ifilonenko/following{/other_user},https://api.github.com/users/ifilonenko/gists{/gist_id},https://api.github.com/users/ifilonenko/starred{/owner}{/repo},https://api.github.com/users/ifilonenko/subscriptions,https://api.github.com/users/ifilonenko/orgs,https://api.github.com/users/ifilonenko/repos,https://api.github.com/users/ifilonenko/events{/privacy},https://api.github.com/users/ifilonenko/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26772,https://github.com/apache/spark/pull/26772,https://github.com/apache/spark/pull/26772.diff,https://github.com/apache/spark/pull/26772.patch
94,https://api.github.com/repos/apache/spark/issues/26762,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26762/labels{/name},https://api.github.com/repos/apache/spark/issues/26762/comments,https://api.github.com/repos/apache/spark/issues/26762/events,https://github.com/apache/spark/pull/26762,532999858,MDExOlB1bGxSZXF1ZXN0MzQ5MTQ4MTMw,26762,[SPARK-30131] add array_median function ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-12-04T23:20:43Z,2019-12-05T17:48:25Z,,NONE,"### What changes were proposed in this pull request?
In this PR I have added a dataframe function `array_median` that takes in one argument i.e. the array, checks that it only contains numeric values and returns the median value as a double.


### Why are the changes needed?
It is known that there isn't any exact median function in Spark SQL, and this might be a difficult problem to solve efficiently. However, to find the median for an array should be a simple task, and something that users can utilize when collecting numeric values to a list or set.

There is a ticket requesting exact median: https://issues.apache.org/jira/browse/SPARK-26589

And some online questions regarding medians and exact medians:
https://stackoverflow.com/questions/41431270/how-to-find-exact-median-for-grouped-data-in-spark?rq=1
https://www.edureka.co/community/24516/how-can-i-calculate-exact-median-with-apache-spark
https://stackoverflow.com/questions/47988133/to-find-median-value-of-a-data-frame-in-apache-spark
https://stackoverflow.com/questions/31432843/how-to-find-median-and-quantiles-using-spark)
https://stackoverflow.com/questions/28158729/how-can-i-calculate-exact-median-with-apache-spark

This is not an aggregate function per se, but in other DBs you can find exact median functions:
https://www.ibm.com/support/knowledgecenter/en/SSCJDQ/com.ibm.swg.im.dashdb.sql.ref.doc/doc/r0061839.html
https://docs.aws.amazon.com/redshift/latest/dg/r_MEDIAN.html
https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions086.htm
http://www.h2database.com/html/functions-aggregate.html#median

or have extensions implementing it:
https://pgxn.org/dist/quantile/ 

### Does this PR introduce any user-facing change?
Yes, a new function will be available for users, `array_median`.


### How was this patch tested?
unit tests
",spark,apache,hagerf,5435221,MDQ6VXNlcjU0MzUyMjE=,https://avatars0.githubusercontent.com/u/5435221?v=4,,https://api.github.com/users/hagerf,https://github.com/hagerf,https://api.github.com/users/hagerf/followers,https://api.github.com/users/hagerf/following{/other_user},https://api.github.com/users/hagerf/gists{/gist_id},https://api.github.com/users/hagerf/starred{/owner}{/repo},https://api.github.com/users/hagerf/subscriptions,https://api.github.com/users/hagerf/orgs,https://api.github.com/users/hagerf/repos,https://api.github.com/users/hagerf/events{/privacy},https://api.github.com/users/hagerf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26762,https://github.com/apache/spark/pull/26762,https://github.com/apache/spark/pull/26762.diff,https://github.com/apache/spark/pull/26762.patch
95,https://api.github.com/repos/apache/spark/issues/26759,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26759/labels{/name},https://api.github.com/repos/apache/spark/issues/26759/comments,https://api.github.com/repos/apache/spark/issues/26759/events,https://github.com/apache/spark/pull/26759,532707686,MDExOlB1bGxSZXF1ZXN0MzQ4OTA3NjY5,26759,[SPARK-28794][SQL][DOC] Documentation for Create table Command,"[{'id': 1406627011, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MDEx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCUMENTATION', 'name': 'DOCUMENTATION', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-12-04T14:07:37Z,2019-12-20T06:59:50Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Document CREATE TABLE statement in SQL Reference Guide.

### Why are the changes needed?
Adding documentation for SQL reference.

### Does this PR introduce any user-facing change?
yes

Before:
There was no documentation for this.


### How was this patch tested?
Used jekyll build and serve to verify.",spark,apache,PavithraRamachandran,51401130,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26759,https://github.com/apache/spark/pull/26759,https://github.com/apache/spark/pull/26759.diff,https://github.com/apache/spark/pull/26759.patch
96,https://api.github.com/repos/apache/spark/issues/26756,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26756/labels{/name},https://api.github.com/repos/apache/spark/issues/26756/comments,https://api.github.com/repos/apache/spark/issues/26756/events,https://github.com/apache/spark/pull/26756,532490484,MDExOlB1bGxSZXF1ZXN0MzQ4NzI3MDUx,26756,[SPARK-30119][WebUI]Support Pagination for Batch Tables in Streaming Tab,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-12-04T07:02:54Z,2019-12-16T08:55:35Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Adding support for pagination in streaming tab for completed batch table using existing framework for pagination. Refer PR #26215 
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
If our streaming job is running for long time and number of batches are huge then out of memory error may come while loading the streaming page. Introducing pagination will solve this problem and also improve the loading time of page. Besides jobs,stages,sql and thrift-server page contains pagination. So it also brings consistency.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
Yes.
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Manually Tested.

Before
![Screenshot from 2019-12-04 15-44-02](https://user-images.githubusercontent.com/15366835/70145574-6e6a0900-16c6-11ea-8a40-7708638d0137.png)


After
![Screenshot from 2019-12-15 12-54-48](https://user-images.githubusercontent.com/15366835/70859481-42f2e400-1f3a-11ea-8d93-a1f04355aaa5.png)


<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,iRakson,15366835,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26756,https://github.com/apache/spark/pull/26756,https://github.com/apache/spark/pull/26756.diff,https://github.com/apache/spark/pull/26756.patch
97,https://api.github.com/repos/apache/spark/issues/26754,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26754/labels{/name},https://api.github.com/repos/apache/spark/issues/26754/comments,https://api.github.com/repos/apache/spark/issues/26754/events,https://github.com/apache/spark/pull/26754,532398718,MDExOlB1bGxSZXF1ZXN0MzQ4NjU2NTE4,26754,[SPARK-30115][SQL] Improve limit only query on datasource table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-12-04T02:04:25Z,2019-12-05T17:48:22Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

We use Spark as ad-hoc query engine. Most of users' SELECT queries with LIMIT operation like
1) SELECT * FROM TABLE_A LIMIT N
2) SELECT colA FROM TABLE_A LIMIT N
3) CREATE TAB_B as SELECT * FROM TABLE_A LIMIT N
If the TABLE_A is a large table (a RDD with thousands and thousands of partitions), the execution time would be very big since it has to list all files to build a RDD before execution. But almost time, the N is just like 10, 100, 1000, not very big. We don't need to scan all files. This optimization will create a **SinglePartitionReadRDD** to address it.

In our production result, this optimization benefits a lot. The duration time of simple query with LIMIT could reduce 5~10 times. For example, before this optimization, a query on a table which has about one hundred thousands files would run over 30 seconds, after applying this optimization, the time decreased to 5 seconds.

Should support both Spark DataSource Table and Hive Table which can be converted to DataSource table.
Should support bucket table, partition table, normal table.
Should support different file formats like parquet, orc.
This PR only addresses Spark datasource table.
Hive table and view will be filed after this merged.

### How to implement?
1. Add two configurations, `PARTIAL_LISTING_ENABLED` and `PARTIAL_LISTING_MAX_FILES`
2. In `FindDataSourceTable.apply()`, we resolve `GlobalLimit` to add a flag `partialListing` to `FileIndex`
3. In `DataSourceScanExec.inputRDD`, by checking the flag `partialListing` in `relation.location`, we create a `SinglePartitionReadRDD`. This RDD will assign less than `PARTIAL_LISTING_MAX_FILES` files to a single partition.

### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Add LimitOnlyQuerySuite",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26754,https://github.com/apache/spark/pull/26754,https://github.com/apache/spark/pull/26754.diff,https://github.com/apache/spark/pull/26754.patch
98,https://api.github.com/repos/apache/spark/issues/26750,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26750/labels{/name},https://api.github.com/repos/apache/spark/issues/26750/comments,https://api.github.com/repos/apache/spark/issues/26750/events,https://github.com/apache/spark/pull/26750,532166608,MDExOlB1bGxSZXF1ZXN0MzQ4NDczNDY3,26750,[SPARK-28948][SQL] Support passing all Table metadata in TableProvider,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-12-03T18:06:24Z,2019-12-10T18:37:01Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
The `TableProvider` only accepts table schema and properties. It should accept table partitioning as well.

This is extracted from https://github.com/apache/spark/pull/25651, to only keep the API changes and make the diff smaller.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Although `DataFrameReader`/`DataStreamReader` don't support user-specified partitioning, we need to pass the table partitioning when getting tables from `TableProvider` if we store tables in Hive metastore with v2 provider.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
not yet.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26750,https://github.com/apache/spark/pull/26750,https://github.com/apache/spark/pull/26750.diff,https://github.com/apache/spark/pull/26750.patch
99,https://api.github.com/repos/apache/spark/issues/26740,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26740/labels{/name},https://api.github.com/repos/apache/spark/issues/26740/comments,https://api.github.com/repos/apache/spark/issues/26740/events,https://github.com/apache/spark/pull/26740,531498677,MDExOlB1bGxSZXF1ZXN0MzQ3OTQ2MDYw,26740,[SPARK-30053][SQL] Add the ability for v2 datasource so specify a vacuum action on the table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-12-02T19:48:19Z,2019-12-10T03:04:32Z,,NONE,"https://issues.apache.org/jira/browse/SPARK-30053?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=16985760


### What changes were proposed in this pull request?

This cr includes a new api so that users of the V2 Datasource api can specify an action to take when a user types in the 'Vacuum {table}' command.  This is commonly used to remove deleted records and optimize the table.


### Why are the changes needed?

There is currently no way for users to create this type of behavior in SQL

> vacuum is a very common command. for example .
>
> - https://www.sqlite.org/lang_vacuum.html
> - https://www.postgresql.org/docs/9.1/sql-vacuum.html
> - https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html
> - https://docs.databricks.com/spark/latest/spark-sql/language-manual/vacuum.html
> 
> Another option would the optimize key word which databricks uses for delta.
> - https://docs.databricks.com/spark/latest/spark-sql/language-manual/optimize.html

### Does this PR introduce any user-facing change?

Yes this includes a new public but experimental API

### How was this patch tested?
 Unit tests.
",spark,apache,AndrewKL,2659018,MDQ6VXNlcjI2NTkwMTg=,https://avatars1.githubusercontent.com/u/2659018?v=4,,https://api.github.com/users/AndrewKL,https://github.com/AndrewKL,https://api.github.com/users/AndrewKL/followers,https://api.github.com/users/AndrewKL/following{/other_user},https://api.github.com/users/AndrewKL/gists{/gist_id},https://api.github.com/users/AndrewKL/starred{/owner}{/repo},https://api.github.com/users/AndrewKL/subscriptions,https://api.github.com/users/AndrewKL/orgs,https://api.github.com/users/AndrewKL/repos,https://api.github.com/users/AndrewKL/events{/privacy},https://api.github.com/users/AndrewKL/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26740,https://github.com/apache/spark/pull/26740,https://github.com/apache/spark/pull/26740.diff,https://github.com/apache/spark/pull/26740.patch
100,https://api.github.com/repos/apache/spark/issues/26735,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26735/labels{/name},https://api.github.com/repos/apache/spark/issues/26735/comments,https://api.github.com/repos/apache/spark/issues/26735/events,https://github.com/apache/spark/pull/26735,531069666,MDExOlB1bGxSZXF1ZXN0MzQ3NTc4MjMw,26735,[SPARK-30102][ML][PYSPARK] GMM supports instance weighting,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-12-02T11:33:29Z,2019-12-24T09:46:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
supports instance weighting in GMM


### Why are the changes needed?
ML should support instance weighting


### Does this PR introduce any user-facing change?
yes, a new param `weightCol` is exposed


### How was this patch tested?
added testsuits
",spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26735,https://github.com/apache/spark/pull/26735,https://github.com/apache/spark/pull/26735.diff,https://github.com/apache/spark/pull/26735.patch
101,https://api.github.com/repos/apache/spark/issues/26727,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26727/labels{/name},https://api.github.com/repos/apache/spark/issues/26727/comments,https://api.github.com/repos/apache/spark/issues/26727/events,https://github.com/apache/spark/pull/26727,530754460,MDExOlB1bGxSZXF1ZXN0MzQ3MzQ0Mzg5,26727,[SPARK-30087][CORE] Enhanced implementation of JmxSink on RMI remote calls,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-12-01T16:08:40Z,2019-12-16T07:58:33Z,,NONE,"### What changes were proposed in this pull request?
Enhanced implementation of JmxSink on RMI remote calls.
E.g:
service: jmx:rmi://127.0.0.1:1986/jndi/rmi://127.0.0.1:1986/jmxrmi to connect


### Why are the changes needed?
JMX supports RMI remote which can be more suitable for production use.

### Does this PR introduce any user-facing change?
Added JMX RMI remote port connection method.

### How was this patch tested?
`JmxSinkSuite.scala`
",spark,apache,XuQianJin-Stars,10494131,MDQ6VXNlcjEwNDk0MTMx,https://avatars2.githubusercontent.com/u/10494131?v=4,,https://api.github.com/users/XuQianJin-Stars,https://github.com/XuQianJin-Stars,https://api.github.com/users/XuQianJin-Stars/followers,https://api.github.com/users/XuQianJin-Stars/following{/other_user},https://api.github.com/users/XuQianJin-Stars/gists{/gist_id},https://api.github.com/users/XuQianJin-Stars/starred{/owner}{/repo},https://api.github.com/users/XuQianJin-Stars/subscriptions,https://api.github.com/users/XuQianJin-Stars/orgs,https://api.github.com/users/XuQianJin-Stars/repos,https://api.github.com/users/XuQianJin-Stars/events{/privacy},https://api.github.com/users/XuQianJin-Stars/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26727,https://github.com/apache/spark/pull/26727,https://github.com/apache/spark/pull/26727.diff,https://github.com/apache/spark/pull/26727.patch
102,https://api.github.com/repos/apache/spark/issues/26724,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26724/labels{/name},https://api.github.com/repos/apache/spark/issues/26724/comments,https://api.github.com/repos/apache/spark/issues/26724/events,https://github.com/apache/spark/pull/26724,530643365,MDExOlB1bGxSZXF1ZXN0MzQ3MjY5MzIx,26724,return tightest common type when inferring schema,[],open,False,,[],,2,2019-11-30T23:32:34Z,2019-12-01T09:45:42Z,,NONE,Allows inferring schema when writing rdd despite having multiple types present. This roughly mirrors what occurs in the Scala api.,spark,apache,potatochip,10922120,MDQ6VXNlcjEwOTIyMTIw,https://avatars1.githubusercontent.com/u/10922120?v=4,,https://api.github.com/users/potatochip,https://github.com/potatochip,https://api.github.com/users/potatochip/followers,https://api.github.com/users/potatochip/following{/other_user},https://api.github.com/users/potatochip/gists{/gist_id},https://api.github.com/users/potatochip/starred{/owner}{/repo},https://api.github.com/users/potatochip/subscriptions,https://api.github.com/users/potatochip/orgs,https://api.github.com/users/potatochip/repos,https://api.github.com/users/potatochip/events{/privacy},https://api.github.com/users/potatochip/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26724,https://github.com/apache/spark/pull/26724,https://github.com/apache/spark/pull/26724.diff,https://github.com/apache/spark/pull/26724.patch
103,https://api.github.com/repos/apache/spark/issues/26723,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26723/labels{/name},https://api.github.com/repos/apache/spark/issues/26723/comments,https://api.github.com/repos/apache/spark/issues/26723/events,https://github.com/apache/spark/pull/26723,530570795,MDExOlB1bGxSZXF1ZXN0MzQ3MjE4ODkw,26723,[SPARK-27523][CORE] - Resolve scheme-less event log directory relative to default filesystem,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-11-30T12:04:09Z,2019-12-17T22:23:32Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
`evenlog.dir` is resolved with respect to the default filesystem.


### Why are the changes needed?
These changes needed for resolved `evelog.dir` when `defaultFS` is specified. 


### Does this PR introduce any user-facing change?
no


### How was this patch tested?
this patch was tested manually and covered by the unit test. 
",spark,apache,ayudovin,34862741,MDQ6VXNlcjM0ODYyNzQx,https://avatars1.githubusercontent.com/u/34862741?v=4,,https://api.github.com/users/ayudovin,https://github.com/ayudovin,https://api.github.com/users/ayudovin/followers,https://api.github.com/users/ayudovin/following{/other_user},https://api.github.com/users/ayudovin/gists{/gist_id},https://api.github.com/users/ayudovin/starred{/owner}{/repo},https://api.github.com/users/ayudovin/subscriptions,https://api.github.com/users/ayudovin/orgs,https://api.github.com/users/ayudovin/repos,https://api.github.com/users/ayudovin/events{/privacy},https://api.github.com/users/ayudovin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26723,https://github.com/apache/spark/pull/26723,https://github.com/apache/spark/pull/26723.diff,https://github.com/apache/spark/pull/26723.patch
104,https://api.github.com/repos/apache/spark/issues/26711,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26711/labels{/name},https://api.github.com/repos/apache/spark/issues/26711/comments,https://api.github.com/repos/apache/spark/issues/26711/events,https://github.com/apache/spark/pull/26711,530228467,MDExOlB1bGxSZXF1ZXN0MzQ2OTUxOTkx,26711,[SPARK-30069][CORE][YARN] Clean up non-shuffle disk block manager files following executor exists on YARN,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-11-29T08:58:20Z,2019-12-05T17:49:55Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Currently we only clean up the local directories on application removed. However, when executors die and restart repeatedly, many temp files are left untouched in the local directories, which is undesired behavior and could cause disk space used up gradually. Especially, in long running service like Spark thrift-server with dynamic resource allocation disabled, it's very easy causes local disk full.
#21390 fixed the same problem on Standalone mode. On YARN, this issue still exists.

From https://github.com/apache/spark/pull/21390#issuecomment-391695376, YARN only cleans container local dirs when container (executor) is exited. But these files are not in container local dirs.
<img width=""1527"" alt=""Screen Shot 2019-11-29 at 4 52 56 PM"" src=""https://user-images.githubusercontent.com/1853780/69856506-c66cce00-12c8-11ea-9e62-058aa2d3c12e.png"">

So this patch is very straightforward:
We create these ""temp_xxx "" files under the container dirs when the executor is running in YARN container.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Add an UT and manually test.
",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26711,https://github.com/apache/spark/pull/26711,https://github.com/apache/spark/pull/26711.diff,https://github.com/apache/spark/pull/26711.patch
105,https://api.github.com/repos/apache/spark/issues/26702,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26702/labels{/name},https://api.github.com/repos/apache/spark/issues/26702/comments,https://api.github.com/repos/apache/spark/issues/26702/events,https://github.com/apache/spark/pull/26702,529889347,MDExOlB1bGxSZXF1ZXN0MzQ2NjgxNDc1,26702,[SPARK-30070][SQL] Support ANSI datetimes predicate - overlaps,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-11-28T12:38:47Z,2019-12-09T09:37:01Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This pull request introduces ANSI `OVERLAPS` predicate to SparkSQL. It yields true when two time periods (defined by their endpoints) overlap, false when they do not overlap.
Syntaxes as bellow.
```sql
(start1, end1) OVERLAPS (start2, end2)
(start1, length1) OVERLAPS (start2, length2)
```

```scala
 /**
 * The operator `OVERLAPS` determines whether or not two chronological periods overlap in time. A
 * chronological period is specified by a pair of datetimes (starting and ending) or as a
 * starting datetime and an interval(resolved by analyzer to datetimes pairs early).
 *
 * If the length of the period is greater than 0, then the period consists of all points of time
 * greater than or equal to the lower endpoint, and less than the upper endpoint,
 * a.k.a [lower, upper).
 *
 * If the length of the period is equal to 0, then the period consists of a single point in time,
 * the lower endpoint, a.k.a [lower, lower].
 *
 * Two periods overlap if they have at least one point in common.
 *
 */
```

```sql
postgres=# select (cast(a as timestamp), cast(b as timestamp)) overlaps (cast(c as timestamp), cast(d as timestamp)) from (values
 ('2011-11-11', '2011-11-11', '2011-11-11', '2011-11-11'),
 ('2011-11-10', '2011-11-11', '2011-11-11', '2011-11-12'),
 ('2011-11-11', '2011-11-10', '2011-11-11', '2011-11-12'),
 ('2011-11-11', '2011-11-10', '2011-11-12', '2011-11-11'),
 ('2011-11-10', '2011-11-11', '2011-11-12', '2011-11-13'),
 ('2011-11-10', '2011-11-20', '2011-11-11', '2011-11-19'),
 ('2011-11-11', '2011-11-19', '2011-11-10', '2011-11-20')) t(a,b,c,d);
 overlaps
----------
 t
 f
 f
 f
 f
 t
 t
(7 rows)
```

### Why are the changes needed?

ANSI support FeatureID: F053, and better PostgreSQL feature parity too.

### Does this PR introduce any user-facing change?

Yes, add a datetime predicate.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add ut",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26702,https://github.com/apache/spark/pull/26702,https://github.com/apache/spark/pull/26702.diff,https://github.com/apache/spark/pull/26702.patch
106,https://api.github.com/repos/apache/spark/issues/26698,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26698/labels{/name},https://api.github.com/repos/apache/spark/issues/26698/comments,https://api.github.com/repos/apache/spark/issues/26698/events,https://github.com/apache/spark/pull/26698,529651898,MDExOlB1bGxSZXF1ZXN0MzQ2NDg5MTk2,26698,[SPARK-29685][SQL] Adds format to the spark-sql output like its implemented by Dataset showString,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-11-28T02:00:02Z,2019-12-04T01:08:17Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR adds a formatted print method like it is provided in the showString implementation from Dataset while using the spark-sql client.

Actual Spark-sql client
```
spark-sql> select * from table1;
5       name3   add3
5       name1   add1
5       name2   add2
```

Spark Dataset show
```
scala> sql(""select * from table1"").show()
+---+-----+-------+
| id| name|address|
+---+-----+-------+
|  5|name3|   add3|
|  5|name1|   add1|
|  5|name2|   add2|
+---+-----+-------+
```
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To have a consistent output between Dataset show and spark-sql interfaces.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
org.apache.spark.sql.hive.thriftserver.CliSuite.scala was adapted for the new fortmat. Code compiles and pass tests.
",spark,apache,javierivanov,7876890,MDQ6VXNlcjc4NzY4OTA=,https://avatars1.githubusercontent.com/u/7876890?v=4,,https://api.github.com/users/javierivanov,https://github.com/javierivanov,https://api.github.com/users/javierivanov/followers,https://api.github.com/users/javierivanov/following{/other_user},https://api.github.com/users/javierivanov/gists{/gist_id},https://api.github.com/users/javierivanov/starred{/owner}{/repo},https://api.github.com/users/javierivanov/subscriptions,https://api.github.com/users/javierivanov/orgs,https://api.github.com/users/javierivanov/repos,https://api.github.com/users/javierivanov/events{/privacy},https://api.github.com/users/javierivanov/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26698,https://github.com/apache/spark/pull/26698,https://github.com/apache/spark/pull/26698.diff,https://github.com/apache/spark/pull/26698.patch
107,https://api.github.com/repos/apache/spark/issues/26696,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26696/labels{/name},https://api.github.com/repos/apache/spark/issues/26696/comments,https://api.github.com/repos/apache/spark/issues/26696/events,https://github.com/apache/spark/pull/26696,529614693,MDExOlB1bGxSZXF1ZXN0MzQ2NDU5Nzg1,26696,[WIP][SPARK-18886][CORE] Make locality wait time be the time since a TSM's available slots were fully utilized,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2019-11-27T23:22:14Z,2019-12-24T01:02:32Z,,NONE,"### What changes were proposed in this pull request?
Currently the time window that locality wait times are measuring is the time since the last task launched for a TSM. The proposed change is to instead measure the time since this TSM's available slots were fully utilized.

The number of available slots for a TSM is determined by dividing all slots among the TSMs according to the scheduling policy (FIFO vs FAIR). 

### Why are the changes needed?

- cluster can become heavily underutilized as described in [SPARK-18886](https://issues.apache.org/jira/browse/SPARK-18886?jql=project%20%3D%20SPARK%20AND%20text%20~%20delay)


### How was this patch tested?
PoolSuite
TaskSchedulerImplSuite
TaskSetManagerSuite

Thoughts on this approach?
@viirya 
@squito 
@kayousterhout ",spark,apache,bmarcott,481161,MDQ6VXNlcjQ4MTE2MQ==,https://avatars2.githubusercontent.com/u/481161?v=4,,https://api.github.com/users/bmarcott,https://github.com/bmarcott,https://api.github.com/users/bmarcott/followers,https://api.github.com/users/bmarcott/following{/other_user},https://api.github.com/users/bmarcott/gists{/gist_id},https://api.github.com/users/bmarcott/starred{/owner}{/repo},https://api.github.com/users/bmarcott/subscriptions,https://api.github.com/users/bmarcott/orgs,https://api.github.com/users/bmarcott/repos,https://api.github.com/users/bmarcott/events{/privacy},https://api.github.com/users/bmarcott/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26696,https://github.com/apache/spark/pull/26696,https://github.com/apache/spark/pull/26696.diff,https://github.com/apache/spark/pull/26696.patch
108,https://api.github.com/repos/apache/spark/issues/26687,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26687/labels{/name},https://api.github.com/repos/apache/spark/issues/26687/comments,https://api.github.com/repos/apache/spark/issues/26687/events,https://github.com/apache/spark/pull/26687,529044850,MDExOlB1bGxSZXF1ZXN0MzQ1OTk1MjYx,26687,[SPARK-30055][k8s] Allow configuration of restart policy for Kubernetes pods,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2019-11-27T00:28:59Z,2019-12-20T20:03:50Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This allows the configuration of the restart policy for drivers and executors running on Kubernetes. It also changes the default executor restart policy to ""Always"" instead of ""Never"".

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The current restart policies are hard-coded to ""Never"". Restarting a failed application requires deleting and rescheduling all of its pods, which is very slow, requires external management, and drops any cache that exists in memory or container filesystem. The default executor restart policy is changed to ""Always"", which should be an objective improvement over the existing driver-managed reschedule-on-failure behavior. The driver will still reschedule evicted executors.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
New configuration properties `spark.kubernetes.driver.restartPolicy` and `spark.kubernetes.executor.restartPolicy` were added.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
I manually verified the changes worked and ran the test script. I didn't add unit tests because it didn't seem like the tests would prove anything useful. Should I? ü§∑‚Äç‚ôÇ ",spark,apache,khogeland,1495099,MDQ6VXNlcjE0OTUwOTk=,https://avatars3.githubusercontent.com/u/1495099?v=4,,https://api.github.com/users/khogeland,https://github.com/khogeland,https://api.github.com/users/khogeland/followers,https://api.github.com/users/khogeland/following{/other_user},https://api.github.com/users/khogeland/gists{/gist_id},https://api.github.com/users/khogeland/starred{/owner}{/repo},https://api.github.com/users/khogeland/subscriptions,https://api.github.com/users/khogeland/orgs,https://api.github.com/users/khogeland/repos,https://api.github.com/users/khogeland/events{/privacy},https://api.github.com/users/khogeland/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26687,https://github.com/apache/spark/pull/26687,https://github.com/apache/spark/pull/26687.diff,https://github.com/apache/spark/pull/26687.patch
109,https://api.github.com/repos/apache/spark/issues/26682,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26682/labels{/name},https://api.github.com/repos/apache/spark/issues/26682/comments,https://api.github.com/repos/apache/spark/issues/26682/events,https://github.com/apache/spark/pull/26682,528894015,MDExOlB1bGxSZXF1ZXN0MzQ1ODcxNzY1,26682,[SPARK-29306][CORE] Stage Level Sched: Executors need to track what ResourceProfile they are created with ,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-11-26T18:15:47Z,2019-12-13T17:12:54Z,,CONTRIBUTOR,"
### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This is the second PR for the Stage Level Scheduling. This is adding in the necessary executor side changes:
1) executors to know what ResourceProfile they should be using
2) handle parsing the resource profile settings - these are not in the global configs
3) then reporting back to the driver what resource profile it was started with.

This PR adds all the piping for YARN to pass the information all the way to executors, but it just uses the default ResourceProfile (which is the global applicatino level configs).

At a high level these changes include:
1) adding a new --resourceProfileId option to the CoarseGrainedExecutorBackend
2) Add the ResourceProfile settings to new internal confs that gets passed into the Executor
3) Executor changes that use the resource profile id passed in to read the corresponding ResourceProfile confs and then parse those requests and discover resources as necessary
4) Executor registers to Driver with the Resource profile id so that the ExecutorMonitor can track how many executor with each profile are running
5) YARN side changes to show that passing the resource profile id and confs actually works. Just uses the DefaultResourceProfile for now.

I also removed a check from the CoarseGrainedExecutorBackend that used to check to make sure there were task requirements before parsing any custom resource executor requests.  With the resource profiles this becomes much more expensive because we would then have to pass the task requests to each executor and the check was just a short cut and not really needed. It was much cleaner just to remove it.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

These changes are needed for the executor to report which ResourceProfile they are using so that ultimately the dynamic allocation manager can use that information to know how many with a profile are running and how many more it needs to request.  Its also needed to get the resource profile confs to the executor so that it can run the appropriate discovery script if needed.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit tests and manually on YARN.",spark,apache,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26682,https://github.com/apache/spark/pull/26682,https://github.com/apache/spark/pull/26682.diff,https://github.com/apache/spark/pull/26682.patch
110,https://api.github.com/repos/apache/spark/issues/26678,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26678/labels{/name},https://api.github.com/repos/apache/spark/issues/26678/comments,https://api.github.com/repos/apache/spark/issues/26678/events,https://github.com/apache/spark/pull/26678,528627387,MDExOlB1bGxSZXF1ZXN0MzQ1NjUyNTM1,26678,[SPARK-30226][SQL] Remove withXXX functions in WriteBuilder,[],open,False,,[],,19,2019-11-26T10:23:54Z,2019-12-24T07:24:17Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Adding a `LogicalWriteInfo` interface as suggested by @cloud-fan in https://github.com/apache/spark/pull/25990#issuecomment-555132991 

### Why are the changes needed?
It provides compile-time guarantees where we previously had none, which will make it harder to introduce bugs in the future.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Compiles and passes tests",spark,apache,edrevo,1845771,MDQ6VXNlcjE4NDU3NzE=,https://avatars1.githubusercontent.com/u/1845771?v=4,,https://api.github.com/users/edrevo,https://github.com/edrevo,https://api.github.com/users/edrevo/followers,https://api.github.com/users/edrevo/following{/other_user},https://api.github.com/users/edrevo/gists{/gist_id},https://api.github.com/users/edrevo/starred{/owner}{/repo},https://api.github.com/users/edrevo/subscriptions,https://api.github.com/users/edrevo/orgs,https://api.github.com/users/edrevo/repos,https://api.github.com/users/edrevo/events{/privacy},https://api.github.com/users/edrevo/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26678,https://github.com/apache/spark/pull/26678,https://github.com/apache/spark/pull/26678.diff,https://github.com/apache/spark/pull/26678.patch
111,https://api.github.com/repos/apache/spark/issues/26675,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26675/labels{/name},https://api.github.com/repos/apache/spark/issues/26675/comments,https://api.github.com/repos/apache/spark/issues/26675/events,https://github.com/apache/spark/pull/26675,528575870,MDExOlB1bGxSZXF1ZXN0MzQ1NjEwNTE3,26675,[SPARK-30041][SQL][WEBUI] Add Codegen Stage Id to Stage DAG visualization in Web UI,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-11-26T08:50:51Z,2019-12-20T01:38:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
SPARK-29894 provides information on the Codegen Stage Id in WEBUI for SQL Plan graphs. Similarly, this proposes to add Codegen Stage Id in the DAG visualization for Stage execution. DAGs for Stage execution are available in the WEBUI under the Jobs and Stages tabs.

### Why are the changes needed?
This is proposed as an aid for drill-down analysis of complex SQL statement execution, as it is not always easy to match parts of the SQL Plan graph with the corresponding Stage DAG execution graph. Adding Codegen Stage Id for WholeStageCodegen operations makes this task easier.

### Does this PR introduce any user-facing change?
Stage DAG visualization in the WEBUI will show codegen stage id for WholeStageCodegen operations, as in the example snippet from the WEBUI, Jobs tab  (the query used in the example is TPCDS 2.4 q14a):
![](https://issues.apache.org/jira/secure/attachment/12987461/Snippet_StagesDags_with_CodegenId%20_annotated.png)

### How was this patch tested?
Manually tested, see also example snippet.",spark,apache,LucaCanali,5243162,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26675,https://github.com/apache/spark/pull/26675,https://github.com/apache/spark/pull/26675.diff,https://github.com/apache/spark/pull/26675.patch
112,https://api.github.com/repos/apache/spark/issues/26674,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26674/labels{/name},https://api.github.com/repos/apache/spark/issues/26674/comments,https://api.github.com/repos/apache/spark/issues/26674/events,https://github.com/apache/spark/pull/26674,528538166,MDExOlB1bGxSZXF1ZXN0MzQ1NTgwNzA0,26674,[SPARK-30059][CORE]Stop AsyncEventQueue when interrupted in dispatch,"[{'id': 1406606875, 'node_id': 'MDU6TGFiZWwxNDA2NjA2ODc1', 'url': 'https://api.github.com/repos/apache/spark/labels/SCHEDULER', 'name': 'SCHEDULER', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-11-26T07:23:16Z,2019-11-29T14:30:16Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
PR #21356  stop `AsyncEventQueue` when interrupted in `postToAll`. 
However, if it's interrupted in `AsyncEventQueue#dispatch`,  SparkContext would be stopped.
This PR proposes to stop `AsyncEventQueue` when interrupted in dispatch, rather than stop the SparkContext.

### Why are the changes needed?
Avoid stopping the SparkContext when interrupted in `AsyncEventQueue#dispatch`.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
New UT.
",spark,apache,wangshuo128,4003322,MDQ6VXNlcjQwMDMzMjI=,https://avatars0.githubusercontent.com/u/4003322?v=4,,https://api.github.com/users/wangshuo128,https://github.com/wangshuo128,https://api.github.com/users/wangshuo128/followers,https://api.github.com/users/wangshuo128/following{/other_user},https://api.github.com/users/wangshuo128/gists{/gist_id},https://api.github.com/users/wangshuo128/starred{/owner}{/repo},https://api.github.com/users/wangshuo128/subscriptions,https://api.github.com/users/wangshuo128/orgs,https://api.github.com/users/wangshuo128/repos,https://api.github.com/users/wangshuo128/events{/privacy},https://api.github.com/users/wangshuo128/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26674,https://github.com/apache/spark/pull/26674,https://github.com/apache/spark/pull/26674.diff,https://github.com/apache/spark/pull/26674.patch
113,https://api.github.com/repos/apache/spark/issues/26670,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26670/labels{/name},https://api.github.com/repos/apache/spark/issues/26670/comments,https://api.github.com/repos/apache/spark/issues/26670/events,https://github.com/apache/spark/pull/26670,528416138,MDExOlB1bGxSZXF1ZXN0MzQ1NDgyOTgx,26670,[SPARK-30033][core] Manage shuffle IO plugins using Spark's plugin system.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-11-26T00:32:18Z,2019-11-26T19:41:07Z,,CONTRIBUTOR,"SPARK-25299 is introducing a new plugin interface for shuffle IO; currently,
parts of that API provide lifecycle methods that are already covered by the
plugin API that was added in SPARK-29396.

This change makes some modifications so that:

- The driver and executor components of the shuffle plugin extend their
  respective counterparts in the generic plugin API.
- The shuffle IO plugin is managed by the same code that manages other
  generic plugins.

This simplifies and reuses similar code that exists in both implementations,
and also provides more functionality to shuffle plugins: not only do they have
more contextual information (without having to query APIs like SparkEnv) but
they also have access to other functionality in the plugin API that would
otherwise require touching internal Spark APIs.

There is a small change to the generic plugin API to avoid registering an
RPC endpoint and starting threads when not needed; plugins now must explicitly
say they want to handle RPC messages for the endpoint to be created. This is
done because the default shuffle plugin is now loaded by the plugin system,
and does not need the RPC functionality. (This API hasn't been released yet
so it's ok to make the change.)

The only downside is that initialization of the SortShuffleManager in executors
is a bit weird, because of the order in which things are initialized: the
shuffle manager is initialized by SparkEnv, and plugin initialization happens
after that. In any case, all initialization is done before any tasks are
allowed to run..

Currently, the shuffle plugin is always loaded, regardless of whether the sort
shuffle manager is being used; this was already the case in the driver, but
now is also the case in the executors. It shouldn't be hard to fix that if
needed.

Tested with existing and updated unit tests.
",spark,apache,vanzin,1694083,MDQ6VXNlcjE2OTQwODM=,https://avatars0.githubusercontent.com/u/1694083?v=4,,https://api.github.com/users/vanzin,https://github.com/vanzin,https://api.github.com/users/vanzin/followers,https://api.github.com/users/vanzin/following{/other_user},https://api.github.com/users/vanzin/gists{/gist_id},https://api.github.com/users/vanzin/starred{/owner}{/repo},https://api.github.com/users/vanzin/subscriptions,https://api.github.com/users/vanzin/orgs,https://api.github.com/users/vanzin/repos,https://api.github.com/users/vanzin/events{/privacy},https://api.github.com/users/vanzin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26670,https://github.com/apache/spark/pull/26670,https://github.com/apache/spark/pull/26670.diff,https://github.com/apache/spark/pull/26670.patch
114,https://api.github.com/repos/apache/spark/issues/26656,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26656/labels{/name},https://api.github.com/repos/apache/spark/issues/26656/comments,https://api.github.com/repos/apache/spark/issues/26656/events,https://github.com/apache/spark/pull/26656,527799954,MDExOlB1bGxSZXF1ZXN0MzQ0OTgwNjU5,26656,[SPARK-27986][SQL] Support ANSI SQL filter clause for aggregate expression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,74,2019-11-25T01:53:21Z,2019-12-24T08:16:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
The filter predicate for aggregate expression is an `ANSI SQL`.
```
<aggregate function> ::=
COUNT <left paren> <asterisk> <right paren> [ <filter clause> ]
| <general set function> [ <filter clause> ]
| <binary set function> [ <filter clause> ]
| <ordered set function> [ <filter clause> ]
| <array aggregate function> [ <filter clause> ]
| <row pattern count function> [ <filter clause> ]
```
There are some mainstream database support this syntax.
**PostgreSQL:**
https://www.postgresql.org/docs/current/sql-expressions.html#SYNTAX-AGGREGATES
For example:
```
SELECT
  year,
  count(*) FILTER (WHERE gdp_per_capita >= 40000)
FROM
  countries
GROUP BY
  year
```
```
SELECT
  year,
  code,
  gdp_per_capita,
  count(*) 
    FILTER (WHERE gdp_per_capita >= 40000) 
    OVER   (PARTITION BY year)
FROM
  countries
```
**jOOQ:**
https://blog.jooq.org/2014/12/30/the-awesome-postgresql-9-4-sql2003-filter-clause-for-aggregate-functions/

**Notice:**
1.This PR only supports FILTER predicate without codegen. maropu will create another PR is related to SPARK-30027 to support codegen.
2.This PR only supports FILTER predicate without DISTINCT. I will create another PR is related to SPARK-30276 to support this.
3.This PR only supports FILTER predicate that can't reference the outer query. I created ticket SPARK-30219 to support it.
4.This PR only supports FILTER predicate that can't use IN/EXISTS predicate sub-queries. I created ticket SPARK-30220 to support it.
5.Spark SQL cannot supports a SQL with nested aggregate. I created ticket SPARK-30182 to support it.

There are some show of the PR on my production environment.
```
spark-sql> desc gja_test_partition;
key     string  NULL
value   string  NULL
other   string  NULL
col2    int     NULL
# Partition Information
# col_name      data_type       comment
col2    int     NULL
Time taken: 0.79 s
```
```
spark-sql> select * from gja_test_partition;
a       A       ao      1
b       B       bo      1
c       C       co      1
d       D       do      1
e       E       eo      2
g       G       go      2
h       H       ho      2
j       J       jo      2
f       F       fo      3
k       K       ko      3
l       L       lo      4
i       I       io      4
Time taken: 1.75 s
```
```
spark-sql> select count(key), sum(col2) from gja_test_partition;
12      26
Time taken: 1.848 s
```
```
spark-sql> select count(key) filter (where col2 > 1) from gja_test_partition;
8
Time taken: 2.926 s
```
```
spark-sql> select sum(col2) filter (where col2 > 2) from gja_test_partition;
14
Time taken: 2.087 s
```
```
spark-sql> select count(key) filter (where col2 > 1), sum(col2) filter (where col2 > 2) from gja_test_partition;
8       14
Time taken: 2.847 s
```
```
spark-sql> select count(key), count(key) filter (where col2 > 1), sum(col2), sum(col2) filter (where col2 > 2) from gja_test_partition;
12      8       26      14
Time taken: 1.787 s
```
```
spark-sql> desc student;
id      int     NULL
name    string  NULL
sex     string  NULL
class_id        int     NULL
Time taken: 0.206 s
```
```
spark-sql> select * from student;
1       Âº†‰∏â    man     1
2       ÊùéÂõõ    man     1
3       Áéã‰∫î    man     2
4       ËµµÂÖ≠    man     2
5       Èí±Â∞èËä±  woman   1
6       Ëµµ‰πùÁ∫¢  woman   2
7       ÈÉ≠‰∏Ω‰∏Ω  woman   2
Time taken: 0.786 s
```
```
spark-sql> select class_id, count(id), sum(id) from student group by class_id;
1       3       8
2       4       20
Time taken: 18.783 s
```
```
spark-sql> select class_id, count(id) filter (where sex = 'man'), sum(id) filter (where sex = 'woman') from student group by class_id;
1       2       5
2       2       13
Time taken: 3.887 s
```

### Why are the changes needed?
Add new SQL feature.


### Does this PR introduce any user-facing change?
'No'.


### How was this patch tested?
Exists UT and new UT.
",spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26656,https://github.com/apache/spark/pull/26656,https://github.com/apache/spark/pull/26656.diff,https://github.com/apache/spark/pull/26656.patch
115,https://api.github.com/repos/apache/spark/issues/26644,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26644/labels{/name},https://api.github.com/repos/apache/spark/issues/26644/comments,https://api.github.com/repos/apache/spark/issues/26644/events,https://github.com/apache/spark/pull/26644,527574325,MDExOlB1bGxSZXF1ZXN0MzQ0ODIwNDQ0,26644,[SPARK-30004][SQL] Allow merge UserDefinedType into a native DataType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2019-11-23T15:59:45Z,2019-12-11T13:00:07Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?

In case you write a UDT, you always need to read it with the UDT registered. In many cases, you want to write it, and then convert it into a native DataType.

In the case of Delta or when appending a partition, you can write to the same table and then it needs to be able to convert merge the UDT into the native type again.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?

When appending data to the table, I get the exception:

```
Failed to merge fields 'START_DATE_MAINTENANCE_FLPL' and 'START_DATE_MAINTENANCE_FLPL'.
Failed to merge incompatible data types TimestampType and org.apache.spark.sql.types.CustomXMLGregorianCalendarType@5ff12345;;
```
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?

* Add a unit test to the DataTypeSuite.scala
* Add an integration test to UserDefinedTypeSuite.scala
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

https://jira.apache.org/jira/browse/SPARK-30004",spark,apache,Fokko,1134248,MDQ6VXNlcjExMzQyNDg=,https://avatars0.githubusercontent.com/u/1134248?v=4,,https://api.github.com/users/Fokko,https://github.com/Fokko,https://api.github.com/users/Fokko/followers,https://api.github.com/users/Fokko/following{/other_user},https://api.github.com/users/Fokko/gists{/gist_id},https://api.github.com/users/Fokko/starred{/owner}{/repo},https://api.github.com/users/Fokko/subscriptions,https://api.github.com/users/Fokko/orgs,https://api.github.com/users/Fokko/repos,https://api.github.com/users/Fokko/events{/privacy},https://api.github.com/users/Fokko/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26644,https://github.com/apache/spark/pull/26644,https://github.com/apache/spark/pull/26644.diff,https://github.com/apache/spark/pull/26644.patch
116,https://api.github.com/repos/apache/spark/issues/26634,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26634/labels{/name},https://api.github.com/repos/apache/spark/issues/26634/comments,https://api.github.com/repos/apache/spark/issues/26634/events,https://github.com/apache/spark/pull/26634,527006969,MDExOlB1bGxSZXF1ZXN0MzQ0MzY1MjUw,26634,[SPARK-29996][SQL] Add v2 command exec check in adaptive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-11-22T06:07:24Z,2019-11-22T17:08:51Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
AdaptivePlan should also ignore v2 command exec plan like v1.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Avoid logging.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars0.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26634,https://github.com/apache/spark/pull/26634,https://github.com/apache/spark/pull/26634.diff,https://github.com/apache/spark/pull/26634.patch
117,https://api.github.com/repos/apache/spark/issues/26633,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26633/labels{/name},https://api.github.com/repos/apache/spark/issues/26633/comments,https://api.github.com/repos/apache/spark/issues/26633/events,https://github.com/apache/spark/pull/26633,526974741,MDExOlB1bGxSZXF1ZXN0MzQ0MzQwMDE2,26633,[SPARK-29994][CORE] Add WILDCARD task location,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,43,2019-11-22T04:03:27Z,2019-12-10T20:26:52Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This PR adds a new WILDCARD task location that can match any host. This WILDCARD location can be used together with other regular locations in the list of preferred locations to indicate that the task can be assigned to any host/executor if none of the preferred locations is available.

### Why are the changes needed?
This is motivated by the requirement from LocalShuffledRowRDD. When the number of initial mappers of LocalShuffledRowRDD is smaller than the number of worker nodes, it can cause serious regressions if short-running tasks all wait on their preferred locations while they could have otherwise finished quickly on non-preferred locations too.

We have a ""locality wait time"" configuration that allows a task set to downgrade locality requirement after a certain time has passed. Yet, this configuration affects all task sets in the scheduler, and tasks all differ in penalty of locality miss. Thus, we need this finer-grained option for individual tasks to opt out of locality.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Added UT.",spark,apache,maryannxue,4171904,MDQ6VXNlcjQxNzE5MDQ=,https://avatars3.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26633,https://github.com/apache/spark/pull/26633,https://github.com/apache/spark/pull/26633.diff,https://github.com/apache/spark/pull/26633.patch
118,https://api.github.com/repos/apache/spark/issues/26630,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26630/labels{/name},https://api.github.com/repos/apache/spark/issues/26630/comments,https://api.github.com/repos/apache/spark/issues/26630/events,https://github.com/apache/spark/pull/26630,526846913,MDExOlB1bGxSZXF1ZXN0MzQ0MjM4MTg3,26630,[SPARK-29965][core] Ensure that killed executors don't re-register with driver.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-11-21T21:18:34Z,2019-12-15T06:35:10Z,,CONTRIBUTOR,"There are 3 different issues that cause the same underlying problem: an executor
that the driver has killed during downscaling registers back with the block
manager in the driver, and the block manager from that point on keeps trying
to contact the dead executor.

The first one is that the heartbeat receiver was asking unknown executors to
re-register when receiving a heartbeat. That code path only really happens
when the executor dies because of a driver killing it, so there's no reason
to re-register.

The second one is a race between the heartbeat receiver and the DAG scheduler.
Both received notifications of an executor's addition and removal
asynchronously (the first one via the listener bus *and* an async local RPC,
the second via its own separate internal message queue). This led to
situations where they disagreed about which executors were really alive; the
change makes it so the heartbeat receiver is updated first, and once that's
done, then the DAG scheduler can update itself. This ensures the hearbeat
receiver knows which executors not to ask to re-register.

The third one is because the block manager couldn't differentiate between
an unknown executor (like one that's been removed) and an executor that needs
to re-register (like one the scheduler decided to unregister because of
too many fetch failures). The change adds code in the block manager master to
track which executors have been removed, so that instead of asking them to
re-register, it just ignores them.

While there I simplified the executor shutdown a bit since it was doing
some stuff unnecessarily.

Tested with existing unit tests, and by repeatedly runnins worklogs on k8s
with dynamic allocation; previously I'd hit these different issues somewhat
often, with the fixes I'm not able to reproduce them.",spark,apache,vanzin,1694083,MDQ6VXNlcjE2OTQwODM=,https://avatars0.githubusercontent.com/u/1694083?v=4,,https://api.github.com/users/vanzin,https://github.com/vanzin,https://api.github.com/users/vanzin/followers,https://api.github.com/users/vanzin/following{/other_user},https://api.github.com/users/vanzin/gists{/gist_id},https://api.github.com/users/vanzin/starred{/owner}{/repo},https://api.github.com/users/vanzin/subscriptions,https://api.github.com/users/vanzin/orgs,https://api.github.com/users/vanzin/repos,https://api.github.com/users/vanzin/events{/privacy},https://api.github.com/users/vanzin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26630,https://github.com/apache/spark/pull/26630,https://github.com/apache/spark/pull/26630.diff,https://github.com/apache/spark/pull/26630.patch
119,https://api.github.com/repos/apache/spark/issues/26624,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26624/labels{/name},https://api.github.com/repos/apache/spark/issues/26624/comments,https://api.github.com/repos/apache/spark/issues/26624/events,https://github.com/apache/spark/pull/26624,526488283,MDExOlB1bGxSZXF1ZXN0MzQzOTQzMDQ2,26624,[SPARK-8981][core] Add MDC support in Executor ,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-11-21T10:04:23Z,2019-12-18T10:27:42Z,,NONE,"### What changes were proposed in this pull request?
Added MDC support in all thread pulls
TheardUtils create new pools that pass over MDC.

### Why are the changes needed?
In many cases, it is very hard to understand from which actions the logs in the executor come from.
when you are doing multi-thread work in the driver and send actions in parallel.

### Does this PR introduce any user-facing change?
No


### How was this patch tested?
No test added because no new functionality added it is thread pull change and all current tests pass. 
",spark,apache,igreenfield,5586356,MDQ6VXNlcjU1ODYzNTY=,https://avatars0.githubusercontent.com/u/5586356?v=4,,https://api.github.com/users/igreenfield,https://github.com/igreenfield,https://api.github.com/users/igreenfield/followers,https://api.github.com/users/igreenfield/following{/other_user},https://api.github.com/users/igreenfield/gists{/gist_id},https://api.github.com/users/igreenfield/starred{/owner}{/repo},https://api.github.com/users/igreenfield/subscriptions,https://api.github.com/users/igreenfield/orgs,https://api.github.com/users/igreenfield/repos,https://api.github.com/users/igreenfield/events{/privacy},https://api.github.com/users/igreenfield/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26624,https://github.com/apache/spark/pull/26624,https://github.com/apache/spark/pull/26624.diff,https://github.com/apache/spark/pull/26624.patch
120,https://api.github.com/repos/apache/spark/issues/26598,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26598/labels{/name},https://api.github.com/repos/apache/spark/issues/26598/comments,https://api.github.com/repos/apache/spark/issues/26598/events,https://github.com/apache/spark/pull/26598,524964083,MDExOlB1bGxSZXF1ZXN0MzQyNjQyMTA1,26598,[SPARK-29861][CORE] Reduce downtime in Spark standalone HA master switch,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-11-19T11:58:45Z,2019-12-17T10:25:33Z,,NONE,"### What changes were proposed in this pull request?
1. Close open zookeeper connections during spark shutdown (SIGTERM)
2. Bump the curator version to 4.2.0
3. Implement a custom error policy that is tolerant to a zookeeper connection suspension.

### Why are the changes needed?
As described in [SPARK-29861](https://issues.apache.org/jira/browse/SPARK-29861), the recovery process of Spark (standalone) master in HA with zookeeper took about 1-2 minutes. During this time no spark master is active, which makes interaction with spark essentially impossible.

Justification for the changes:
1. because the spark master election does not properly restart if there are still open zookeeper connections
2. as a precondition for 3
3. to not restart spark when the connection is suspended temporarily, but only if it is really closed.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
* Set up spark with two masters in standalone mode and zookeeper
* Kill active master
* Observe that the second master takes over immediately (within a few milliseconds)
",spark,apache,Nibooor,15635917,MDQ6VXNlcjE1NjM1OTE3,https://avatars1.githubusercontent.com/u/15635917?v=4,,https://api.github.com/users/Nibooor,https://github.com/Nibooor,https://api.github.com/users/Nibooor/followers,https://api.github.com/users/Nibooor/following{/other_user},https://api.github.com/users/Nibooor/gists{/gist_id},https://api.github.com/users/Nibooor/starred{/owner}{/repo},https://api.github.com/users/Nibooor/subscriptions,https://api.github.com/users/Nibooor/orgs,https://api.github.com/users/Nibooor/repos,https://api.github.com/users/Nibooor/events{/privacy},https://api.github.com/users/Nibooor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26598,https://github.com/apache/spark/pull/26598,https://github.com/apache/spark/pull/26598.diff,https://github.com/apache/spark/pull/26598.patch
121,https://api.github.com/repos/apache/spark/issues/26592,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26592/labels{/name},https://api.github.com/repos/apache/spark/issues/26592/comments,https://api.github.com/repos/apache/spark/issues/26592/events,https://github.com/apache/spark/pull/26592,524792521,MDExOlB1bGxSZXF1ZXN0MzQyNTAyNjYx,26592,[SPARK-29371][SQL] Fractional representation for interval string,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2019-11-19T05:58:33Z,2019-11-27T12:27:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Add fractional representation for interval values
```
postgres=# select interval '1.41 years 2.51 months 2.21 weeks 15.24 days 3.31 hours 5.38 minutes 12.3456789 seconds';
               interval
---------------------------------------
 1 year 6 mons 45 days 27:38:35.145679
(1 row)postgres=# select interval '1.42 years 2.51 months 2.21 weeks 15.24 days 3.31 hours 5.38 minutes 12.3456789 seconds';
               interval
---------------------------------------
 1 year 7 mons 45 days 27:38:35.145679
```
### RULES
#### 1. The table shows how each unit converts to `CalendarInterval`'s `months`, `days` and `microseconds`.
| Unit | TO | Rounding mode| Example|Notes|
|--|--|--|--|--|
|year| months only| ROUND_DOWN | `1.41 year = 1 years 4 months`,  `1.42 year = 1 years 5 months`| only the integral part of the total years * 30 will be used in months, the rest will be omitted|
| month | months| NO | |the month's integral part will be added exactly to months
| month | days |NO||the factional of months * 30, and the integral of this result will be added exactly to days
|month| microseconds|  java.lang.Math.round| `1.1333333333301 months = 1 months 3 days 23 hours 59 minutes 59.999992 seconds`|the rest part will be added to microseconds |
|week| days |NO||| total weeks * 7, and the integral of the result will be added exactly to days
|week|microseconds|  java.lang.Math.round|`2.13333333 week = 14 days 22 hours 23 minutes 59.997984 seconds` |the rest part will be added to microseconds
|day| days|NO|||the day's integral part will be added exactly to months
|day|microseconds|java.lang.Math.round| | the rest of day will be added to microseconds
|hour, minute, second, millisecond, microsecond|microseconds|java.lang.Math.round| | all these unit value go to microseconds|

#### 2. Additional RULES
2.1. ROUNDING for microseconds happens in each value-unit, not at the end, `0.0004 millisecond 0.4 microseconds` should be 0 not 1 microsecond, for example,
```sql
postgres=# select interval '0.0004 millisecond 0.4 microseconds';
 interval
----------
 00:00:00
(1 row)

postgres=# select interval '0.001 millisecond 0.4 microseconds';
    interval
-----------------
 00:00:00.000001
(1 row)
```
2.2 When `seconds` value is decimal, the millisecond and microsecond cannot be decimal, I have no reason to support this at the cost for performance and making the parsing logic more complex.
```
select interval '0.1111111 seconds 2 microseconds';
ERROR:  invalid input syntax for type interval: ""0.1111111 seconds 2 microseconds""
LINE 1: select interval '0.1111111 seconds 2 microseconds';

postgres=# select interval '1 seconds 2.1 microseconds';
    interval
-----------------
 00:00:01.000002
(1 row)
```
2.3 0.5 microsecond round to 0 in PostgreSQL, but 1 in Spark due to different round policy supported by the host language I guess.

This PR closes #26314, which has performance regression.


### Why are the changes needed?

In PostgreSQL, interval field values can have fractional parts. See https://www.postgresql.org/docs/current/datatype-datetime.html#DATATYPE-INTERVAL-INPUT


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

Yes
1. add fraction input support for interval values
2. NO nanosecond out of range exception, round silently.

### How was this patch tested?

##### 1. add unit tests
##### 2. benchmark test, with a modified benchmark test which adds several fraction second value.
https://github.com/apache/spark/pull/26592/files#diff-c02ae27cf4adc93f30d4a13839aa6bbaR85-R89
It is supported by master and this fix, then we can equally see the difference. only 2~3% performace loss here.
###### master(before)
```java
[info] Java HotSpot(TM) 64-Bit Server VM 1.8.0_231-b11 on Mac OS X 10.15.1
[info] Intel(R) Core(TM) i5-5287U CPU @ 2.90GHz
[info] cast strings to intervals:                Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] prepare string w/ interval                          789            838          45          1.3         789.1       1.0X
[info] prepare string w/o interval                         761           1248         728          1.3         760.9       1.0X
[info] 1 units w/ interval                                 576            624          42          1.7         576.0       1.4X
[info] 1 units w/o interval                                589            623          32          1.7         588.9       1.3X
[info] 2 units w/ interval                                 814            913          99          1.2         814.3       1.0X
[info] 2 units w/o interval                                698            713          18          1.4         698.0       1.1X
[info] 4 units w/ interval                                1564           1626          88          0.6        1564.2       0.5X
[info] 4 units w/o interval                               1459           1596         210          0.7        1459.2       0.5X
[info] 6 units w/ interval                                1965           2026          67          0.5        1964.6       0.4X
[info] 6 units w/o interval                               1747           1771          21          0.6        1747.4       0.5X
[info] 8 units w/ interval                                2193           2348         163          0.5        2193.0       0.4X
[info] 8 units w/o interval                               2247           2274          24          0.4        2247.3       0.4X
[info] 10 units w/ interval                               2321           2437         185          0.4        2320.8       0.3X
[info] 10 units w/o interval                              2306           2339          39          0.4        2305.8       0.3X
[info] 12 units w/ interval                               2774           2793          18          0.4        2774.1       0.3X
[info] 12 units w/o interval                              2765           2791          22          0.4        2764.8       0.3X
[info] 14 units w/ interval                               3265           3290          36          0.3        3264.8       0.2X
[info] 14 units w/o interval                              3264           3276          16          0.3        3263.7       0.2X
```
###### this fix
```java
[info] Java HotSpot(TM) 64-Bit Server VM 1.8.0_231-b11 on Mac OS X 10.15.1
[info] Intel(R) Core(TM) i5-5287U CPU @ 2.90GHz
[info] cast strings to intervals:                Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] prepare string w/ interval                          675            735          68          1.5         675.2       1.0X
[info] prepare string w/o interval                         569            630          60          1.8         569.2       1.2X
[info] 1 units w/ interval                                 519            565          68          1.9         519.1       1.3X
[info] 1 units w/o interval                                450            468          21          2.2         450.1       1.5X
[info] 2 units w/ interval                                 637            645          11          1.6         636.6       1.1X
[info] 2 units w/o interval                                601            616          14          1.7         601.5       1.1X
[info] 4 units w/ interval                                1403           1418          26          0.7        1403.2       0.5X
[info] 4 units w/o interval                               1404           1408           5          0.7        1403.6       0.5X
[info] 6 units w/ interval                                1720           1728          12          0.6        1720.2       0.4X
[info] 6 units w/o interval                               1679           1686           6          0.6        1678.7       0.4X
[info] 8 units w/ interval                                1998           2022          21          0.5        1997.6       0.3X
[info] 8 units w/o interval                               2012           2021          13          0.5        2011.6       0.3X
[info] 10 units w/ interval                               2362           2385          22          0.4        2362.0       0.3X
[info] 10 units w/o interval                              2378           2401          22          0.4        2377.6       0.3X
[info] 12 units w/ interval                               2835           2851          17          0.4        2835.0       0.2X
[info] 12 units w/o interval                              2829           2832           4          0.4        2829.4       0.2X
[info] 14 units w/ interval                               3325           3376          47          0.3        3325.2       0.2X
[info] 14 units w/o interval                              3323           3336          13          0.3        3323.2       0.2X
[info]
[success] Total time: 231 s, completed 2019-11-19 13:40:18
```

##### 3. precision check with PostgreSQL

```

postgres=# select interval '1.41666666666666 year';
   interval
---------------
 1 year 4 mons
(1 row)

postgres=# select interval '1.41666666666667 year';
   interval
---------------
 1 year 5 mons
(1 row)

postgres=# select interval '2.13333333 week';
        interval
-------------------------
 14 days 22:23:59.997984
(1 row)

postgres=# select interval '2.13333334 week';
        interval
-------------------------
 14 days 22:24:00.004032
(1 row)


postgres=# select interval '1.133333333330 months';
           interval
------------------------------
 1 mon 3 days 23:59:59.999991
(1 row)

postgres=# select interval '1.1333333333301 months';
           interval
------------------------------
 1 mon 3 days 23:59:59.999992
(1 row)

postgres=#
postgres=# select interval '0.50 microseconds';
 interval
----------
 00:00:00
(1 row)

postgres=# select interval '0.60 microseconds';
    interval
-----------------
 00:00:00.000001
(1 row)
```",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26592,https://github.com/apache/spark/pull/26592,https://github.com/apache/spark/pull/26592.diff,https://github.com/apache/spark/pull/26592.patch
122,https://api.github.com/repos/apache/spark/issues/26589,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26589/labels{/name},https://api.github.com/repos/apache/spark/issues/26589/comments,https://api.github.com/repos/apache/spark/issues/26589/events,https://github.com/apache/spark/pull/26589,524745618,MDExOlB1bGxSZXF1ZXN0MzQyNDY0NTYz,26589,[SPARK-29947][SQL] Improve ResolveRelations performance,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-11-19T03:11:24Z,2019-12-15T19:20:07Z,,MEMBER,"### What changes were proposed in this pull request?

It is very common for a SQL query to query a table more than twice. For example:
```
== Physical Plan ==
*(12) HashAggregate(keys=[cmn_mtrc_summ_dt#21, rev_rollup#1279, CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END#1366, CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END#1367], functions=[sum(coalesce(bid_count#34, 0)), sum(coalesce(ck_trans_count#35, 0)), sum(coalesce(ended_bid_count#36, 0)), sum(coalesce(ended_lstg_count#37, 0)), sum(coalesce(ended_success_lstg_count#38, 0)), sum(coalesce(item_sold_count#39, 0)), sum(coalesce(new_lstg_count#40, 0)), sum(coalesce(gmv_us_amt#41, 0.00)), sum(coalesce(gmv_slr_lc_amt#42, 0.00)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_insrtn_fee_us_amt#46, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_insrtn_crd_us_amt#50, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fetr_fee_us_amt#54, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fetr_crd_us_amt#58, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fv_fee_us_amt#62, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fv_crd_us_amt#67, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_l_fee_us_amt#72, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_l_crd_us_amt#76, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_nl_fee_us_amt#80, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_nl_crd_us_amt#84, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_slr_tools_fee_us_amt#88, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_slr_tools_crd_us_amt#92, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(coalesce(rvnu_unasgnd_us_amt#96, 0.000000)), sum((coalesce(rvnu_transaction_us_amt#112, 0.0) + coalesce(rvnu_transaction_crd_us_amt#115, 0.0))), sum((coalesce(rvnu_total_us_amt#118, 0.0) + coalesce(rvnu_total_crd_us_amt#121, 0.0)))])
+- Exchange hashpartitioning(cmn_mtrc_summ_dt#21, rev_rollup#1279, CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END#1366, CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END#1367, 200), true, [id=#403]
   +- *(11) HashAggregate(keys=[cmn_mtrc_summ_dt#21, rev_rollup#1279, CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END AS CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END#1366, CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END AS CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END#1367], functions=[partial_sum(coalesce(bid_count#34, 0)), partial_sum(coalesce(ck_trans_count#35, 0)), partial_sum(coalesce(ended_bid_count#36, 0)), partial_sum(coalesce(ended_lstg_count#37, 0)), partial_sum(coalesce(ended_success_lstg_count#38, 0)), partial_sum(coalesce(item_sold_count#39, 0)), partial_sum(coalesce(new_lstg_count#40, 0)), partial_sum(coalesce(gmv_us_amt#41, 0.00)), partial_sum(coalesce(gmv_slr_lc_amt#42, 0.00)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_insrtn_fee_us_amt#46, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_insrtn_crd_us_amt#50, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fetr_fee_us_amt#54, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fetr_crd_us_amt#58, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fv_fee_us_amt#62, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fv_crd_us_amt#67, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_l_fee_us_amt#72, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_l_crd_us_amt#76, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_nl_fee_us_amt#80, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_nl_crd_us_amt#84, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_slr_tools_fee_us_amt#88, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_slr_tools_crd_us_amt#92, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(coalesce(rvnu_unasgnd_us_amt#96, 0.000000)), partial_sum((coalesce(rvnu_transaction_us_amt#112, 0.0) + coalesce(rvnu_transaction_crd_us_amt#115, 0.0))), partial_sum((coalesce(rvnu_total_us_amt#118, 0.0) + coalesce(rvnu_total_crd_us_amt#121, 0.0)))])
      +- *(11) Project [cmn_mtrc_summ_dt#21, sap_category_id#24, bid_count#34, ck_trans_count#35, ended_bid_count#36, ended_lstg_count#37, ended_success_lstg_count#38, item_sold_count#39, new_lstg_count#40, gmv_us_amt#41, gmv_slr_lc_amt#42, rvnu_insrtn_fee_us_amt#46, rvnu_insrtn_crd_us_amt#50, rvnu_fetr_fee_us_amt#54, rvnu_fetr_crd_us_amt#58, rvnu_fv_fee_us_amt#62, rvnu_fv_crd_us_amt#67, rvnu_othr_l_fee_us_amt#72, rvnu_othr_l_crd_us_amt#76, rvnu_othr_nl_fee_us_amt#80, rvnu_othr_nl_crd_us_amt#84, rvnu_slr_tools_fee_us_amt#88, rvnu_slr_tools_crd_us_amt#92, rvnu_unasgnd_us_amt#96, ... 6 more fields]
         +- *(11) BroadcastHashJoin [byr_cntry_id#23], [cntry_id#1309], LeftOuter, BuildRight
            :- *(11) Project [cmn_mtrc_summ_dt#21, byr_cntry_id#23, sap_category_id#24, bid_count#34, ck_trans_count#35, ended_bid_count#36, ended_lstg_count#37, ended_success_lstg_count#38, item_sold_count#39, new_lstg_count#40, gmv_us_amt#41, gmv_slr_lc_amt#42, rvnu_insrtn_fee_us_amt#46, rvnu_insrtn_crd_us_amt#50, rvnu_fetr_fee_us_amt#54, rvnu_fetr_crd_us_amt#58, rvnu_fv_fee_us_amt#62, rvnu_fv_crd_us_amt#67, rvnu_othr_l_fee_us_amt#72, rvnu_othr_l_crd_us_amt#76, rvnu_othr_nl_fee_us_amt#80, rvnu_othr_nl_crd_us_amt#84, rvnu_slr_tools_fee_us_amt#88, rvnu_slr_tools_crd_us_amt#92, ... 6 more fields]
            :  +- *(11) BroadcastHashJoin [slr_cntry_id#28], [cntry_id#1269], LeftOuter, BuildRight
            :     :- *(11) Project [gen_attr_1#360 AS cmn_mtrc_summ_dt#21, gen_attr_5#267 AS byr_cntry_id#23, gen_attr_7#268 AS sap_category_id#24, gen_attr_15#272 AS slr_cntry_id#28, gen_attr_27#278 AS bid_count#34, gen_attr_29#279 AS ck_trans_count#35, gen_attr_31#280 AS ended_bid_count#36, gen_attr_33#282 AS ended_lstg_count#37, gen_attr_35#283 AS ended_success_lstg_count#38, gen_attr_37#284 AS item_sold_count#39, gen_attr_39#281 AS new_lstg_count#40, gen_attr_41#285 AS gmv_us_amt#41, gen_attr_43#287 AS gmv_slr_lc_amt#42, gen_attr_51#290 AS rvnu_insrtn_fee_us_amt#46, gen_attr_59#294 AS rvnu_insrtn_crd_us_amt#50, gen_attr_67#298 AS rvnu_fetr_fee_us_amt#54, gen_attr_75#302 AS rvnu_fetr_crd_us_amt#58, gen_attr_83#306 AS rvnu_fv_fee_us_amt#62, gen_attr_93#311 AS rvnu_fv_crd_us_amt#67, gen_attr_103#316 AS rvnu_othr_l_fee_us_amt#72, gen_attr_111#320 AS rvnu_othr_l_crd_us_amt#76, gen_attr_119#324 AS rvnu_othr_nl_fee_us_amt#80, gen_attr_127#328 AS rvnu_othr_nl_crd_us_amt#84, gen_attr_135#332 AS rvnu_slr_tools_fee_us_amt#88, ... 6 more fields]
            :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_308#777 as decimal(20,0))], [cast(gen_attr_309#803 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_310#674 as int)], [cast(gen_attr_311#774 as int)], LeftOuter, BuildRight
            :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_5#267 as decimal(20,0))], [cast(gen_attr_312#665 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 5 more fields]
            :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_313#565 as decimal(20,0))], [cast(gen_attr_314#591 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_315#462 as int)], [cast(gen_attr_316#562 as int)], LeftOuter, BuildRight
            :     :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_15#272 as decimal(20,0))], [cast(gen_attr_317#453 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 5 more fields]
            :     :     :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_25#277 as decimal(20,0))], [cast(gen_attr_318#379 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_25#277, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, ... 6 more fields]
            :     :     :     :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_23#276 as decimal(20,0))], [cast(gen_attr_319#367 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :     :     :     :     :- *(11) Project [byr_cntry_id#1169 AS gen_attr_5#267, sap_category_id#1170 AS gen_attr_7#268, slr_cntry_id#1174 AS gen_attr_15#272, lstg_curncy_id#1178 AS gen_attr_23#276, blng_curncy_id#1179 AS gen_attr_25#277, bid_count#1180 AS gen_attr_27#278, ck_trans_count#1181 AS gen_attr_29#279, ended_bid_count#1182 AS gen_attr_31#280, new_lstg_count#1183 AS gen_attr_39#281, ended_lstg_count#1184 AS gen_attr_33#282, ended_success_lstg_count#1185 AS gen_attr_35#283, item_sold_count#1186 AS gen_attr_37#284, gmv_us_amt#1187 AS gen_attr_41#285, gmv_slr_lc_amt#1189 AS gen_attr_43#287, rvnu_insrtn_fee_us_amt#1192 AS gen_attr_51#290, rvnu_insrtn_crd_us_amt#1196 AS gen_attr_59#294, rvnu_fetr_fee_us_amt#1200 AS gen_attr_67#298, rvnu_fetr_crd_us_amt#1204 AS gen_attr_75#302, rvnu_fv_fee_us_amt#1208 AS gen_attr_83#306, rvnu_fv_crd_us_amt#1213 AS gen_attr_93#311, rvnu_othr_l_fee_us_amt#1218 AS gen_attr_103#316, rvnu_othr_l_crd_us_amt#1222 AS gen_attr_111#320, rvnu_othr_nl_fee_us_amt#1226 AS gen_attr_119#324, rvnu_othr_nl_crd_us_amt#1230 AS gen_attr_127#328, ... 7 more fields]
            :     :     :     :     :     :     :     :     :     :  +- *(11) ColumnarToRow
            :     :     :     :     :     :     :     :     :     :     +- FileScan parquet default.big_table1[byr_cntry_id#1169,sap_category_id#1170,slr_cntry_id#1174,lstg_curncy_id#1178,blng_curncy_id#1179,bid_count#1180,ck_trans_count#1181,ended_bid_count#1182,new_lstg_count#1183,ended_lstg_count#1184,ended_success_lstg_count#1185,item_sold_count#1186,gmv_us_amt#1187,gmv_slr_lc_amt#1189,rvnu_insrtn_fee_us_amt#1192,rvnu_insrtn_crd_us_amt#1196,rvnu_fetr_fee_us_amt#1200,rvnu_fetr_crd_us_amt#1204,rvnu_fv_fee_us_amt#1208,rvnu_fv_crd_us_amt#1213,rvnu_othr_l_fee_us_amt#1218,rvnu_othr_l_crd_us_amt#1222,rvnu_othr_nl_fee_us_amt#1226,rvnu_othr_nl_crd_us_amt#1230,... 7 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionFilters: [isnotnull(cmn_mtrc_summ_dt#1262), (cmn_mtrc_summ_dt#1262 >= 18078), (cmn_mtrc_summ_dt#1262 <= 18..., PushedFilters: [], ReadSchema: struct<byr_cntry_id:decimal(4,0),sap_category_id:decimal(9,0),slr_cntry_id:decimal(4,0),lstg_curn...
            :     :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#288]
            :     :     :     :     :     :     :     :     :        +- *(1) Project [CURNCY_ID#1263 AS gen_attr_319#367]
            :     :     :     :     :     :     :     :     :           +- *(1) Filter isnotnull(CURNCY_ID#1263)
            :     :     :     :     :     :     :     :     :              +- *(1) ColumnarToRow
            :     :     :     :     :     :     :     :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#297]
            :     :     :     :     :     :     :     :        +- *(2) Project [CURNCY_ID#1263 AS gen_attr_318#379]
            :     :     :     :     :     :     :     :           +- *(2) Filter isnotnull(CURNCY_ID#1263)
            :     :     :     :     :     :     :     :              +- *(2) ColumnarToRow
            :     :     :     :     :     :     :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(4,0), true] as decimal(20,0)))), [id=#306]
            :     :     :     :     :     :     :        +- *(3) Project [cntry_id#1269 AS gen_attr_317#453, rev_rollup_id#1278 AS gen_attr_315#462]
            :     :     :     :     :     :     :           +- *(3) Filter isnotnull(cntry_id#1269)
            :     :     :     :     :     :     :              +- *(3) ColumnarToRow
            :     :     :     :     :     :     :                 +- FileScan parquet default.small_table2[cntry_id#1269,rev_rollup_id#1278] Batched: true, DataFilters: [isnotnull(cntry_id#1269)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table2], PartitionFilters: [], PushedFilters: [IsNotNull(cntry_id)], ReadSchema: struct<cntry_id:decimal(4,0),rev_rollup_id:smallint>
            :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, smallint, true] as int) as bigint))), [id=#315]
            :     :     :     :     :     :        +- *(4) Project [rev_rollup_id#1286 AS gen_attr_316#562, curncy_id#1289 AS gen_attr_313#565]
            :     :     :     :     :     :           +- *(4) Filter isnotnull(rev_rollup_id#1286)
            :     :     :     :     :     :              +- *(4) ColumnarToRow
            :     :     :     :     :     :                 +- FileScan parquet default.small_table3[rev_rollup_id#1286,curncy_id#1289] Batched: true, DataFilters: [isnotnull(rev_rollup_id#1286)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table3], PartitionFilters: [], PushedFilters: [IsNotNull(rev_rollup_id)], ReadSchema: struct<rev_rollup_id:smallint,curncy_id:decimal(4,0)>
            :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#324]
            :     :     :     :     :        +- *(5) Project [CURNCY_ID#1263 AS gen_attr_314#591]
            :     :     :     :     :           +- *(5) Filter isnotnull(CURNCY_ID#1263)
            :     :     :     :     :              +- *(5) ColumnarToRow
            :     :     :     :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(4,0), true] as decimal(20,0)))), [id=#333]
            :     :     :     :        +- *(6) Project [cntry_id#1269 AS gen_attr_312#665, rev_rollup_id#1278 AS gen_attr_310#674]
            :     :     :     :           +- *(6) Filter isnotnull(cntry_id#1269)
            :     :     :     :              +- *(6) ColumnarToRow
            :     :     :     :                 +- FileScan parquet default.small_table2[cntry_id#1269,rev_rollup_id#1278] Batched: true, DataFilters: [isnotnull(cntry_id#1269)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table2], PartitionFilters: [], PushedFilters: [IsNotNull(cntry_id)], ReadSchema: struct<cntry_id:decimal(4,0),rev_rollup_id:smallint>
            :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, smallint, true] as int) as bigint))), [id=#342]
            :     :     :        +- *(7) Project [rev_rollup_id#1286 AS gen_attr_311#774, curncy_id#1289 AS gen_attr_308#777]
            :     :     :           +- *(7) Filter isnotnull(rev_rollup_id#1286)
            :     :     :              +- *(7) ColumnarToRow
            :     :     :                 +- FileScan parquet default.small_table3[rev_rollup_id#1286,curncy_id#1289] Batched: true, DataFilters: [isnotnull(rev_rollup_id#1286)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table3], PartitionFilters: [], PushedFilters: [IsNotNull(rev_rollup_id)], ReadSchema: struct<rev_rollup_id:smallint,curncy_id:decimal(4,0)>
            :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#351]
            :     :        +- *(8) Project [CURNCY_ID#1263 AS gen_attr_309#803]
            :     :           +- *(8) Filter isnotnull(CURNCY_ID#1263)
            :     :              +- *(8) ColumnarToRow
            :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, decimal(4,0), true])), [id=#360]
            :        +- *(9) Project [cntry_id#1269, rev_rollup#1279]
            :           +- *(9) Filter isnotnull(cntry_id#1269)
            :              +- *(9) ColumnarToRow
            :                 +- FileScan parquet default.small_table2[cntry_id#1269,rev_rollup#1279] Batched: true, DataFilters: [isnotnull(cntry_id#1269)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table2], PartitionFilters: [], PushedFilters: [IsNotNull(cntry_id)], ReadSchema: struct<cntry_id:decimal(4,0),rev_rollup:string>
            +- ReusedExchange [cntry_id#1309, rev_rollup#1319], BroadcastExchange HashedRelationBroadcastMode(List(input[0, decimal(4,0), true])), [id=#360]
```
This PR try to improve `ResolveTables` and `ResolveRelations` performance by reducing the connection times to Hive Metastore Server in such case.


### Why are the changes needed?
1. Reduce the connection times to Hive Metastore Server.
2. Improve `ResolveTables` and `ResolveRelations` performance.


### Does this PR introduce any user-facing change?
No.


### How was this patch tested?

manual test.
After [SPARK-29606](https://issues.apache.org/jira/browse/SPARK-29606) and before this PR:
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 9323
Total time: 2.687441263 seconds

Rule                                                                                               Effective Time / Total Time                     Effective Runs / Total Runs

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations                                   929173767 / 930133504                           2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables                                      0 / 383363402                                   0 / 18
org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin                                         0 / 99433540                                    0 / 4
org.apache.spark.sql.catalyst.analysis.DecimalPrecision                                            41809394 / 83727901                             2 / 18
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions                               71372977 / 71372977                             1 / 1
org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts                              0 / 59071933                                    0 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences                                  37858325 / 58471776                             5 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$PromoteStrings                                 20889892 / 53229016                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion                     23428968 / 50890815                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$InConversion                                   23230666 / 49182607                             1 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator                                   0 / 43638350                                    0 / 18
org.apache.spark.sql.catalyst.optimizer.ColumnPruning                                              17194844 / 42530885                             1 / 6
```
After [SPARK-29606](https://issues.apache.org/jira/browse/SPARK-29606) and after this PR:
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 9323
Total time: 2.163765869 seconds

Rule                                                                                               Effective Time / Total Time                     Effective Runs / Total Runs

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations                                   658905353 / 659829383                           2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables                                      0 / 220708715                                   0 / 18
org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin                                         0 / 99606816                                    0 / 4
org.apache.spark.sql.catalyst.analysis.DecimalPrecision                                            39616060 / 78215752                             2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences                                  36706549 / 54917789                             5 / 18
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions                               53561921 / 53561921                             1 / 1
org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts                              0 / 52329678                                    0 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$PromoteStrings                                 20945755 / 49695998                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion                     20872241 / 46740145                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$InConversion                                   19780298 / 44327227                             1 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator                                   0 / 42312023                                    0 / 18
org.apache.spark.sql.catalyst.optimizer.ColumnPruning                                              17197393 / 39501424                             1 / 6
```

",spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26589,https://github.com/apache/spark/pull/26589,https://github.com/apache/spark/pull/26589.diff,https://github.com/apache/spark/pull/26589.patch
123,https://api.github.com/repos/apache/spark/issues/26586,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26586/labels{/name},https://api.github.com/repos/apache/spark/issues/26586/comments,https://api.github.com/repos/apache/spark/issues/26586/events,https://github.com/apache/spark/pull/26586,524699718,MDExOlB1bGxSZXF1ZXN0MzQyNDI4MDcy,26586,[SPARK-29950][k8s] Blacklist deleted executors in K8S with dynamic allocation.,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2019-11-19T00:28:29Z,2019-12-19T22:08:07Z,,CONTRIBUTOR,"The issue here is that when Spark is downscaling the application and deletes
a few pod requests that aren't needed anymore, it may actually race with the
K8S scheduler, who may be bringing up those executors. So they may have enough
time to connect back to the driver, register, to just be deleted soon after.
This wastes resources and causes misleading entries in the driver log.

The change (ab)uses the blacklisting mechanism to consider the deleted excess
pods as blacklisted, so that if they try to connect back, the driver will deny
it.

It also changes the executor registration slightly, since even with the above
change there were misleading logs. That was because the executor registration
message was an RPC that always succeeded (bar network issues), so the executor
would always try to send an unregistration message to the driver, which would
then log several messages about not knowing anything about the executor. The
change makes the registration RPC succeed or fail directly, instead of using
the separate failure message that would lead to this issue.

Note the last change required some changes in a standalone test suite related
to dynamic allocation, since it relied on the driver not throwing exceptions
when a duplicate executor registration happened.

Tested with existing unit tests, and with live cluster with dyn alloc on.
",spark,apache,vanzin,1694083,MDQ6VXNlcjE2OTQwODM=,https://avatars0.githubusercontent.com/u/1694083?v=4,,https://api.github.com/users/vanzin,https://github.com/vanzin,https://api.github.com/users/vanzin/followers,https://api.github.com/users/vanzin/following{/other_user},https://api.github.com/users/vanzin/gists{/gist_id},https://api.github.com/users/vanzin/starred{/owner}{/repo},https://api.github.com/users/vanzin/subscriptions,https://api.github.com/users/vanzin/orgs,https://api.github.com/users/vanzin/repos,https://api.github.com/users/vanzin/events{/privacy},https://api.github.com/users/vanzin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26586,https://github.com/apache/spark/pull/26586,https://github.com/apache/spark/pull/26586.diff,https://github.com/apache/spark/pull/26586.patch
124,https://api.github.com/repos/apache/spark/issues/26585,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26585/labels{/name},https://api.github.com/repos/apache/spark/issues/26585/comments,https://api.github.com/repos/apache/spark/issues/26585/events,https://github.com/apache/spark/pull/26585,524664686,MDExOlB1bGxSZXF1ZXN0MzQyMzk5Mjky,26585,[WIP][SPARK-25351][SQL][Python] Handle Pandas category type when converting from Python with Arrow,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-11-18T22:46:07Z,2019-11-20T07:55:11Z,,NONE,"Handle Pandas category type while converting from python with Arrow vector

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- Manual Testing
```
Using Python version 2.7.16 (default, Sep  2 2019 11:59:44)
SparkSession available as 'spark'.
>>> import pandas as pd
>>> pdf = pd.DataFrame({""A"":[u""a"",u""b"",u""c"",u""a""]})
>>> pdf[""B""] = pdf[""A""].astype('category')
>>> pdf
   A  B
0  a  a
1  b  b
2  c  c
3  a  a
>>> pdf.dtypes
A      object
B    category
dtype: object
>>> spark.conf.set(""spark.sql.execution.arrow.enabled"", False)
>>> df = spark.createDataFrame(pdf)
>>> df.printSchema()
root
 |-- A: string (nullable = true)
 |-- B: string (nullable = true)

>>> df.show()
+---+---+
|  A|  B|
+---+---+
|  a|  a|
|  b|  b|
|  c|  c|
|  a|  a|
+---+---+

>>> spark.conf.set(""spark.sql.execution.arrow.enabled"", True)
>>> df = spark.createDataFrame(pdf)
>>> df.printSchema()
root
 |-- A: string (nullable = true)
 |-- B: string (nullable = true)

>>> df.show()
+---+---+
|  A|  B|
+---+---+
|  a|  a|
|  b|  b|
|  c|  c|
|  a|  a|
+---+---+

>>>
```
",spark,apache,jalpan-randeri,5408989,MDQ6VXNlcjU0MDg5ODk=,https://avatars1.githubusercontent.com/u/5408989?v=4,,https://api.github.com/users/jalpan-randeri,https://github.com/jalpan-randeri,https://api.github.com/users/jalpan-randeri/followers,https://api.github.com/users/jalpan-randeri/following{/other_user},https://api.github.com/users/jalpan-randeri/gists{/gist_id},https://api.github.com/users/jalpan-randeri/starred{/owner}{/repo},https://api.github.com/users/jalpan-randeri/subscriptions,https://api.github.com/users/jalpan-randeri/orgs,https://api.github.com/users/jalpan-randeri/repos,https://api.github.com/users/jalpan-randeri/events{/privacy},https://api.github.com/users/jalpan-randeri/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26585,https://github.com/apache/spark/pull/26585,https://github.com/apache/spark/pull/26585.diff,https://github.com/apache/spark/pull/26585.patch
125,https://api.github.com/repos/apache/spark/issues/26550,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26550/labels{/name},https://api.github.com/repos/apache/spark/issues/26550/comments,https://api.github.com/repos/apache/spark/issues/26550/events,https://github.com/apache/spark/pull/26550,523640815,MDExOlB1bGxSZXF1ZXN0MzQxNjAwNzI3,26550,[WIP][SPARK-29334][ML][MLLIB] Support for basic vector operators,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-11-15T19:13:26Z,2019-12-23T12:26:13Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?

Basic operator support for `ml.linalg.Vector`, and `mllib.linagl.Vector`:

- negation
- scalar multiplication and division
- vector addition and subtraction 

### Why are the changes needed?

To improve parity with Vector operators provided with pySpark.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Unit tests in the appropriate places.  To run quickly:

```
sbt ""mllib-local/testOnly org.apache.spark.ml.linalg.VectorsSuite""
```",spark,apache,phpisciuneri,5530169,MDQ6VXNlcjU1MzAxNjk=,https://avatars0.githubusercontent.com/u/5530169?v=4,,https://api.github.com/users/phpisciuneri,https://github.com/phpisciuneri,https://api.github.com/users/phpisciuneri/followers,https://api.github.com/users/phpisciuneri/following{/other_user},https://api.github.com/users/phpisciuneri/gists{/gist_id},https://api.github.com/users/phpisciuneri/starred{/owner}{/repo},https://api.github.com/users/phpisciuneri/subscriptions,https://api.github.com/users/phpisciuneri/orgs,https://api.github.com/users/phpisciuneri/repos,https://api.github.com/users/phpisciuneri/events{/privacy},https://api.github.com/users/phpisciuneri/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26550,https://github.com/apache/spark/pull/26550,https://github.com/apache/spark/pull/26550.diff,https://github.com/apache/spark/pull/26550.patch
126,https://api.github.com/repos/apache/spark/issues/26544,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26544/labels{/name},https://api.github.com/repos/apache/spark/issues/26544/comments,https://api.github.com/repos/apache/spark/issues/26544/events,https://github.com/apache/spark/pull/26544,523318631,MDExOlB1bGxSZXF1ZXN0MzQxMzQ3MTI2,26544,[SPARK-29912][SQL] Pruning shuffle exchange and coalesce when input and output both are one partition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-11-15T08:11:41Z,2019-11-15T17:27:21Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
It is meaningless to do `repartition(1)` or `coalesce(1)` when a child plan just output one partition.
Now, we can not get the output numPartitions during logic plan, so this issue pruning the operation in physical plan.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Improve performance.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add UT.",spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars0.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26544,https://github.com/apache/spark/pull/26544,https://github.com/apache/spark/pull/26544.diff,https://github.com/apache/spark/pull/26544.patch
127,https://api.github.com/repos/apache/spark/issues/26541,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26541/labels{/name},https://api.github.com/repos/apache/spark/issues/26541/comments,https://api.github.com/repos/apache/spark/issues/26541/events,https://github.com/apache/spark/pull/26541,523286879,MDExOlB1bGxSZXF1ZXN0MzQxMzIxNTkw,26541,[SPARK-29910][SQL] Optimize speculation performance by adding minimum runtime limit,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-11-15T06:44:36Z,2019-11-21T03:20:24Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
The minimum runtime to speculation used to be a fixed value 100ms.  It means tasks finished in seconds will also be speculated and more executors will be required.
To improve this situation, we add `spark.speculation.minRuntime` to control the minimum runtime limit of speculation.
We can reduce normal tasks to be speculated by adjusting `spark.speculation.minRuntime`.

_**Example:**_
Tasks that don't need to be speculated:
![image](https://user-images.githubusercontent.com/25916266/68921759-b62afe00-07b4-11ea-8786-d50ef0d20ea0.png)
and
![image](https://user-images.githubusercontent.com/25916266/68921795-d3f86300-07b4-11ea-8ebf-27cf2a0fa493.png)

Tasks are more likely to go wrong and need to be speculated:
(especially those shuffle tasks with large amount of data and will cost minutes even hours)
![image](https://user-images.githubusercontent.com/25916266/68921934-39e4ea80-07b5-11ea-84b2-6e115c3960b0.png)


### Why are the changes needed?
To improve speculation performance by reducing speculated tasks which don't need to be speculated actually.


### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Unit tests.
",spark,apache,Deegue,25916266,MDQ6VXNlcjI1OTE2MjY2,https://avatars3.githubusercontent.com/u/25916266?v=4,,https://api.github.com/users/Deegue,https://github.com/Deegue,https://api.github.com/users/Deegue/followers,https://api.github.com/users/Deegue/following{/other_user},https://api.github.com/users/Deegue/gists{/gist_id},https://api.github.com/users/Deegue/starred{/owner}{/repo},https://api.github.com/users/Deegue/subscriptions,https://api.github.com/users/Deegue/orgs,https://api.github.com/users/Deegue/repos,https://api.github.com/users/Deegue/events{/privacy},https://api.github.com/users/Deegue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26541,https://github.com/apache/spark/pull/26541,https://github.com/apache/spark/pull/26541.diff,https://github.com/apache/spark/pull/26541.patch
128,https://api.github.com/repos/apache/spark/issues/26535,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26535/labels{/name},https://api.github.com/repos/apache/spark/issues/26535/comments,https://api.github.com/repos/apache/spark/issues/26535/events,https://github.com/apache/spark/pull/26535,523210893,MDExOlB1bGxSZXF1ZXN0MzQxMjYwNzc1,26535,[SPARK-29905][k8s] Improve pod lifecycle manager behavior with dynamic allocation.,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-11-15T02:11:05Z,2019-11-15T19:58:13Z,,CONTRIBUTOR,"This issue mainly shows up when you enable dynamic allocation:
because there are many executor state changes (because of executors
being requested and starting to run, and later stopped), the lifecycle
manager class could end up logging information about the same executor
multiple times, since the different events would cause the same
executor update to be present in multiple pod snapshots. On top of that,
it could end up making multiple redundant calls into the API server
for the same pod.

Another issue was when the config was set to not delete executor
pods; with dynamic allocation, that means pods keep accumulating
in the API server, and every time the full sync is done by the
polling source, all executors, even the finished ones that Spark
technically does not care about anymore, would be processed.

The change modifies the lifecycle monitor so that it:

- logs executor updates a single time, even if it shows up in
  multiple snapshots, by checking whether the state change
  happened before.
- marks finished-but-not-deleted-in-k8s executors with a label
  so that they can be easily filtered out.

This reduces the amount of logging done by the lifecycle manager,
which is a minor thing in general since the logs are at debug level.
But it also reduces the amount of data that needs to be fetched
from the API server under certain configurations, and overall
reduces interaction with the API server when dynamic allocation is on.

There's also a change in the snapshot store to ensure that the
same subscriber is not called concurrently. That is kind of a bug,
since it means subscribers could be processing snapshots out of order,
or even that they could block multiple threads (e.g. the allocator
callback was synchronized). I actually ran into the ""concurrent calls""
situation in the lifecycle manager during testing, and while it did not
seem to cause problems, it did make for some head scratching while
looking at the logs. It seemed safer to fix that.

Unit tests were updated to check for the changes. Also tested in real
cluster with dynamic allocation on.",spark,apache,vanzin,1694083,MDQ6VXNlcjE2OTQwODM=,https://avatars0.githubusercontent.com/u/1694083?v=4,,https://api.github.com/users/vanzin,https://github.com/vanzin,https://api.github.com/users/vanzin/followers,https://api.github.com/users/vanzin/following{/other_user},https://api.github.com/users/vanzin/gists{/gist_id},https://api.github.com/users/vanzin/starred{/owner}{/repo},https://api.github.com/users/vanzin/subscriptions,https://api.github.com/users/vanzin/orgs,https://api.github.com/users/vanzin/repos,https://api.github.com/users/vanzin/events{/privacy},https://api.github.com/users/vanzin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26535,https://github.com/apache/spark/pull/26535,https://github.com/apache/spark/pull/26535.diff,https://github.com/apache/spark/pull/26535.patch
129,https://api.github.com/repos/apache/spark/issues/26524,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26524/labels{/name},https://api.github.com/repos/apache/spark/issues/26524/comments,https://api.github.com/repos/apache/spark/issues/26524/events,https://github.com/apache/spark/pull/26524,522881839,MDExOlB1bGxSZXF1ZXN0MzQwOTkzNTMw,26524,[SPARK-29898][SQL] Support Avro Custom Logical Types,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-11-14T14:07:02Z,2019-12-03T18:31:48Z,,NONE,"### What changes were proposed in this pull request?

Extends options for the Spark Avro formatter allowing to use custom Avro logical types.

### Why are the changes needed?

At the moment only timestamp and decimal logical types are supported at Spark but Avro support any conversion that you could need. This change keep the default mappings and allow to add news.

```scala
spark
    .read
    .format(""avro"")
    .option(""logicalTypeMapper"", ""org.example.CustomAvroLogicalCatalystMapper"")
    .load()     
```

Only you need is register your custom Avro logical type and then implement `AvroLogicalTypeCatalystMapper`

#### Example of mapper implementation: 

```scala
object ISODatetimeLogicalType extends LogicalType(""datetime"") {
  def register(): Unit = LogicalTypes.register(getName, new LogicalTypes.LogicalTypeFactory() {
    override def fromSchema(schema: Schema): LogicalType = ISODatetimeLogicalType.this
  })
  override def validate(schema: Schema): Unit = {
    if (schema.getType ne Schema.Type.STRING) {
      throw new IllegalArgumentException(
        ""Datetime (iso8601) can only be used with an underlying string type"")
    }
  }
}

class CustomAvroLogicalCatalystMapper extends AvroLogicalTypeCatalystMapper {
  override def toSqlType: PartialFunction[LogicalType, SchemaConverters.SchemaType] = {
    case ISODatetimeLogicalType => SchemaType(TimestampType, nullable = false)
  }

  override def toAvroSchema: PartialFunction[RecordInfo, Schema] = {
    case RecordInfo(TimestampType, _, _) =>
      ISODatetimeLogicalType.addToSchema(SchemaBuilder.builder().stringType())
  }

  override def deserialize
  : PartialFunction[LogicalType, DataDeserializer => Unit] = {
    case ISODatetimeLogicalType =>
      dataUpdater =>
        val datetime = dataUpdater.value match {
          case s: String => UTF8String.fromString(s)
          case s: Utf8 => val bytes = new Array[Byte](s.getByteLength)
            System.arraycopy(s.getBytes, 0, bytes, 0, s.getByteLength)
            UTF8String.fromBytes(bytes)
        }
        val timestamp = Timestamp.from(
          OffsetDateTime.parse(datetime.toString)
            .atZoneSameInstant(ZoneOffset.UTC)
            .toInstant
        )
        dataUpdater.updater
          .setLong(dataUpdater.ordinal, timestamp.toInstant.toEpochMilli * 1000L)
  }

  override def serialize: PartialFunction[LogicalType, DataSerializer => Any] = {
    case ISODatetimeLogicalType =>
      dataSerializer =>
        val datetime = OffsetDateTime.ofInstant(
          Instant.ofEpochMilli(dataSerializer
            .getter
            .getLong(dataSerializer.ordinal) / 1000),
          ZoneOffset.UTC
        ).toString

        Try(DateTimeFormatter.ISO_DATE_TIME.parse(datetime))
          .map(_ => datetime)
          .getOrElse(throw new IncompatibleSchemaException(
            s""Cannot Serialize to Avro logical type ISO8601Datetime: "" +
              s""'$datetime' is not a valid datetime.""))

  } 
}
```

### Does this PR introduce any user-facing change?

Yes, at the moment of use the Spark Avro formatter and `to_avro` & `from_avro` functions. now you can pass the class name of the logical types mapper as an optional parameter.


### How was this patch tested?

- AvroSuit: struct <=> schema types conversions test.
- AvroLogicalTypeSuit: Avro formatter test.",spark,apache,pradomota,11295347,MDQ6VXNlcjExMjk1MzQ3,https://avatars2.githubusercontent.com/u/11295347?v=4,,https://api.github.com/users/pradomota,https://github.com/pradomota,https://api.github.com/users/pradomota/followers,https://api.github.com/users/pradomota/following{/other_user},https://api.github.com/users/pradomota/gists{/gist_id},https://api.github.com/users/pradomota/starred{/owner}{/repo},https://api.github.com/users/pradomota/subscriptions,https://api.github.com/users/pradomota/orgs,https://api.github.com/users/pradomota/repos,https://api.github.com/users/pradomota/events{/privacy},https://api.github.com/users/pradomota/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26524,https://github.com/apache/spark/pull/26524,https://github.com/apache/spark/pull/26524.diff,https://github.com/apache/spark/pull/26524.patch
130,https://api.github.com/repos/apache/spark/issues/26511,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26511/labels{/name},https://api.github.com/repos/apache/spark/issues/26511/comments,https://api.github.com/repos/apache/spark/issues/26511/events,https://github.com/apache/spark/pull/26511,522623771,MDExOlB1bGxSZXF1ZXN0MzQwNzg3MTU5,26511,[SPARK-29886][SQL] Add support for spark style hash patitioning to v2 data sources,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-11-14T04:53:24Z,2019-11-18T22:22:48Z,,NONE,"https://issues.apache.org/jira/browse/SPARK-29886

WARNING: this is a preview of a potential way in which to add spark style hash partitioning to v2 datasources.

### What changes were proposed in this pull request?

Adds support for spark style hash partitioning to v2 datasources which allows for bucket joins.  Adds concrete classes that are usable by users to define this type of partitioning.

### Why are the changes needed?

Currently V2 Datasources are unable to provide support for bucket joins.

### Does this PR introduce any user-facing change?

Yes this adds a concrerte implementation of v2 datasource partitioning and cluster so that users can specify spark style hash partitioning.


### How was this patch tested?

Unit tests
",spark,apache,AndrewKL,2659018,MDQ6VXNlcjI2NTkwMTg=,https://avatars1.githubusercontent.com/u/2659018?v=4,,https://api.github.com/users/AndrewKL,https://github.com/AndrewKL,https://api.github.com/users/AndrewKL/followers,https://api.github.com/users/AndrewKL/following{/other_user},https://api.github.com/users/AndrewKL/gists{/gist_id},https://api.github.com/users/AndrewKL/starred{/owner}{/repo},https://api.github.com/users/AndrewKL/subscriptions,https://api.github.com/users/AndrewKL/orgs,https://api.github.com/users/AndrewKL/repos,https://api.github.com/users/AndrewKL/events{/privacy},https://api.github.com/users/AndrewKL/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26511,https://github.com/apache/spark/pull/26511,https://github.com/apache/spark/pull/26511.diff,https://github.com/apache/spark/pull/26511.patch
131,https://api.github.com/repos/apache/spark/issues/26503,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26503/labels{/name},https://api.github.com/repos/apache/spark/issues/26503/comments,https://api.github.com/repos/apache/spark/issues/26503/events,https://github.com/apache/spark/pull/26503,522213574,MDExOlB1bGxSZXF1ZXN0MzQwNDU0NDcy,26503,[SPARK-29880][CORE][YARN] Handle submit exception when submit to federation cluster,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-11-13T13:11:15Z,2019-11-16T15:14:55Z,,CONTRIBUTOR,"When we submit application to federation yarn cluster. Since getYarnClusterMetrics is not implemented. The submission will exit with failure.

    ResourceRequestHelper.validateResources(sparkConf)
    var appId: ApplicationId = null
    try {
      launcherBackend.connect()
      yarnClient.init(hadoopConf)
      yarnClient.start()

     logInfo(""Requesting a new application from cluster with %d NodeManagers""
          .format(yarnClient.getYarnClusterMetrics.getNumNodeManagers))`
### Why are the changes needed?
Since hadoop federation cluster was deployed, spark application will submit with failure if we do not handle the exception.
### How was this patch tested?
UT
",spark,apache,caneGuy,26762018,MDQ6VXNlcjI2NzYyMDE4,https://avatars2.githubusercontent.com/u/26762018?v=4,,https://api.github.com/users/caneGuy,https://github.com/caneGuy,https://api.github.com/users/caneGuy/followers,https://api.github.com/users/caneGuy/following{/other_user},https://api.github.com/users/caneGuy/gists{/gist_id},https://api.github.com/users/caneGuy/starred{/owner}{/repo},https://api.github.com/users/caneGuy/subscriptions,https://api.github.com/users/caneGuy/orgs,https://api.github.com/users/caneGuy/repos,https://api.github.com/users/caneGuy/events{/privacy},https://api.github.com/users/caneGuy/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26503,https://github.com/apache/spark/pull/26503,https://github.com/apache/spark/pull/26503.diff,https://github.com/apache/spark/pull/26503.patch
132,https://api.github.com/repos/apache/spark/issues/26502,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26502/labels{/name},https://api.github.com/repos/apache/spark/issues/26502/comments,https://api.github.com/repos/apache/spark/issues/26502/events,https://github.com/apache/spark/pull/26502,522121826,MDExOlB1bGxSZXF1ZXN0MzQwMzc1NTk2,26502,[SPARK-29876][SS] Delete/archive file source completed files in separate thread,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-11-13T10:49:36Z,2019-12-04T01:28:54Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
[SPARK-20568](https://issues.apache.org/jira/browse/SPARK-20568) added the possibility to clean up completed files in streaming query. Deleting/archiving uses the main thread which can slow down processing. In this PR I've created thread pool to handle file delete/archival. The number of threads can be configured with `spark.sql.streaming.fileSource.cleaner.numThreads`.

### Why are the changes needed?
Do file delete/archival in separate thread.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing unit tests.
",spark,apache,gaborgsomogyi,18561820,MDQ6VXNlcjE4NTYxODIw,https://avatars2.githubusercontent.com/u/18561820?v=4,,https://api.github.com/users/gaborgsomogyi,https://github.com/gaborgsomogyi,https://api.github.com/users/gaborgsomogyi/followers,https://api.github.com/users/gaborgsomogyi/following{/other_user},https://api.github.com/users/gaborgsomogyi/gists{/gist_id},https://api.github.com/users/gaborgsomogyi/starred{/owner}{/repo},https://api.github.com/users/gaborgsomogyi/subscriptions,https://api.github.com/users/gaborgsomogyi/orgs,https://api.github.com/users/gaborgsomogyi/repos,https://api.github.com/users/gaborgsomogyi/events{/privacy},https://api.github.com/users/gaborgsomogyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26502,https://github.com/apache/spark/pull/26502,https://github.com/apache/spark/pull/26502.diff,https://github.com/apache/spark/pull/26502.patch
133,https://api.github.com/repos/apache/spark/issues/26496,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26496/labels{/name},https://api.github.com/repos/apache/spark/issues/26496/comments,https://api.github.com/repos/apache/spark/issues/26496/events,https://github.com/apache/spark/pull/26496,522010090,MDExOlB1bGxSZXF1ZXN0MzQwMjgyMTc3,26496,[WIP][SPARK-29748][PYTHON][SQL] Remove Row field sorting in PySpark,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-11-13T07:24:23Z,2019-12-04T02:20:07Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Removing the sorting of PySpark SQL Row fields that were previously sorted by name alphabetically. Rows are now used like tuples and are applied to schemas by position.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This caused inconsistent behavior in that local Rows could be applied to a schema by matching names, but once serialized the Row could only be used by position and the fields were possibly in a different order.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes, Row fields are no longer sorted alphabetically. For Python < 3.6 `kwargs` can not guarantee the order as entered, so `Row`s can no longer be created with `kwargs`. Instead users can pass in a `collections.OrderedDict` or construct `Row`s in other ways.

An environment variable ""PYSPARK_LEGACY_ROW_ENABLED"" can be set that will override construction of `Row` to maintain compatibility with Spark 2.x. 


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Modified existing tests for new behavior and added new tests for `LegacyRow`
",spark,apache,BryanCutler,4534389,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26496,https://github.com/apache/spark/pull/26496,https://github.com/apache/spark/pull/26496.diff,https://github.com/apache/spark/pull/26496.patch
134,https://api.github.com/repos/apache/spark/issues/26477,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26477/labels{/name},https://api.github.com/repos/apache/spark/issues/26477/comments,https://api.github.com/repos/apache/spark/issues/26477/events,https://github.com/apache/spark/pull/26477,521337934,MDExOlB1bGxSZXF1ZXN0MzM5NzM1MTcy,26477,[SPARK-29776][SQL] rpad returning invalid value when parameter is empty,[],open,False,,[],,7,2019-11-12T05:31:27Z,2019-12-24T05:41:31Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
rpad returning null value when padding parameter is empty

### Why are the changes needed?
Need to add check-point if padding string is empty and require length is greater then zero.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Old unit tests correct as per this jira.
",spark,apache,07ARB,8948111,MDQ6VXNlcjg5NDgxMTE=,https://avatars0.githubusercontent.com/u/8948111?v=4,,https://api.github.com/users/07ARB,https://github.com/07ARB,https://api.github.com/users/07ARB/followers,https://api.github.com/users/07ARB/following{/other_user},https://api.github.com/users/07ARB/gists{/gist_id},https://api.github.com/users/07ARB/starred{/owner}{/repo},https://api.github.com/users/07ARB/subscriptions,https://api.github.com/users/07ARB/orgs,https://api.github.com/users/07ARB/repos,https://api.github.com/users/07ARB/events{/privacy},https://api.github.com/users/07ARB/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26477,https://github.com/apache/spark/pull/26477,https://github.com/apache/spark/pull/26477.diff,https://github.com/apache/spark/pull/26477.patch
135,https://api.github.com/repos/apache/spark/issues/26470,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26470/labels{/name},https://api.github.com/repos/apache/spark/issues/26470/comments,https://api.github.com/repos/apache/spark/issues/26470/events,https://github.com/apache/spark/pull/26470,521007787,MDExOlB1bGxSZXF1ZXN0MzM5NDcxNzYy,26470,[SPARK-27042][SS] Invalidate cached Kafka producer in case of task retry,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-11-11T14:52:34Z,2019-11-15T19:21:33Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
If a task is failing due to a corrupt cached Kafka producer and the task is retried in the same executor, then the task may get the same producer over and over again. After several retries the query may stop.

In this PR I'm invalidating the old cached producers for a specific key and re-opening a new one. This will reduce the possibility of a faulty producer re-use. It must be mentioned if a producer under the key is used by another task, then in won't be closed. The functionality is similar to the consumer side [here](https://github.com/apache/spark/blob/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaDataConsumer.scala#L628).

### Why are the changes needed?
Increase producer side availability.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing + additional unit tests.
",spark,apache,gaborgsomogyi,18561820,MDQ6VXNlcjE4NTYxODIw,https://avatars2.githubusercontent.com/u/18561820?v=4,,https://api.github.com/users/gaborgsomogyi,https://github.com/gaborgsomogyi,https://api.github.com/users/gaborgsomogyi/followers,https://api.github.com/users/gaborgsomogyi/following{/other_user},https://api.github.com/users/gaborgsomogyi/gists{/gist_id},https://api.github.com/users/gaborgsomogyi/starred{/owner}{/repo},https://api.github.com/users/gaborgsomogyi/subscriptions,https://api.github.com/users/gaborgsomogyi/orgs,https://api.github.com/users/gaborgsomogyi/repos,https://api.github.com/users/gaborgsomogyi/events{/privacy},https://api.github.com/users/gaborgsomogyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26470,https://github.com/apache/spark/pull/26470,https://github.com/apache/spark/pull/26470.diff,https://github.com/apache/spark/pull/26470.patch
136,https://api.github.com/repos/apache/spark/issues/26440,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26440/labels{/name},https://api.github.com/repos/apache/spark/issues/26440/comments,https://api.github.com/repos/apache/spark/issues/26440/events,https://github.com/apache/spark/pull/26440,520238088,MDExOlB1bGxSZXF1ZXN0MzM4ODYyMjIz,26440,[WIP][SPARK-20628][CORE][K8S] Start to improve Spark decommissioning & preemption support,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2019-11-08T21:17:41Z,2019-11-16T02:21:25Z,,CONTRIBUTOR,"This PR is based on an existing/previou PR - https://github.com/apache/spark/pull/19045

### What changes were proposed in this pull request?

This changes adds a decommissioning state that we can enter when the cloud provider/scheduler lets us know we aren't going to be removed immediately but instead will be removed soon. This concept fits nicely in K8s and also with spot-instances on AWS / preemptible instances all of which we can get a notice that our host is going away. For now we simply stop scheduling jobs, in the future we could perform some kind of migration of data during scale-down, or at least stop accepting new blocks to cache.

There is a design document at https://docs.google.com/document/d/1xVO1b6KAwdUhjEJBolVPl9C6sLj7oOveErwDSYdT-pE/edit?usp=sharing

### Why are the changes needed?

With more move to preemptible multi-tenancy, serverless environments, and spot-instances better handling of node scale down is required.

### Does this PR introduce any user-facing change?

There is no API change, however an additional configuration flag is added to enable/disable this behaviour.

### How was this patch tested?

New integration tests in the Spark K8s integration testing. Extension of the AppClientSuite to test decommissioning seperate from the K8s.",spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars1.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26440,https://github.com/apache/spark/pull/26440,https://github.com/apache/spark/pull/26440.diff,https://github.com/apache/spark/pull/26440.patch
137,https://api.github.com/repos/apache/spark/issues/26437,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26437/labels{/name},https://api.github.com/repos/apache/spark/issues/26437/comments,https://api.github.com/repos/apache/spark/issues/26437/events,https://github.com/apache/spark/pull/26437,519938209,MDExOlB1bGxSZXF1ZXN0MzM4NjEzNDMz,26437,[SPARK-29800][SQL] Plan non-correlated Exists 's subquery in PlanSubqueries,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2019-11-08T10:34:44Z,2019-11-26T07:06:10Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
 
When use `EXISTS` in Join's `ON` condition, some join type such as FULL OUTER JOIN it can't be pushdown and leave this expression un-planed,
 The change is to build a physical node of exists-subquery. 

### Why are the changes needed?
Support exist used by join on condition


### Does this PR introduce any user-facing change?
NO

### How was this patch tested?

",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26437,https://github.com/apache/spark/pull/26437,https://github.com/apache/spark/pull/26437.diff,https://github.com/apache/spark/pull/26437.patch
138,https://api.github.com/repos/apache/spark/issues/26435,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26435/labels{/name},https://api.github.com/repos/apache/spark/issues/26435/comments,https://api.github.com/repos/apache/spark/issues/26435/events,https://github.com/apache/spark/pull/26435,519922986,MDExOlB1bGxSZXF1ZXN0MzM4NjAwODQ2,26435,[SPARK-29821][SQL] Allow calling non-aggregate SQL functions with column name,[],open,False,,[],,2,2019-11-08T10:03:58Z,2019-11-11T01:27:37Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
I added functions that can be called with the column name for the functions in the non-aggregate functions section of `functions.scala`.

* `isnan(columnName: String): Column`
* `isnull(columnName: String): Column`
* `nanvl(col1Name: String, col2Name: String): Column`
* `negate(columnName: String): Column`
* `not(columnName: String): Column`
* `bitwiseNOT(columnName: String): Column `

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This pull requests makes it possible to check for nan values in the column `x` by calling `isnan(""x"")`, instead of `isnan($""x"")`. PySpark: `isnan(""x"")`, instead of `isnan(col(""x""))`. This way, users don't need to remember to transform the value to a column. This makes it consistent with other functions such as `sqrt` that can already be called with the column name.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes
See previous section.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
I couldn't find a test file, where sql functions and pyspark sql functions are tested. Please point me in the right direction.",spark,apache,MaxHaertwig,9899051,MDQ6VXNlcjk4OTkwNTE=,https://avatars1.githubusercontent.com/u/9899051?v=4,,https://api.github.com/users/MaxHaertwig,https://github.com/MaxHaertwig,https://api.github.com/users/MaxHaertwig/followers,https://api.github.com/users/MaxHaertwig/following{/other_user},https://api.github.com/users/MaxHaertwig/gists{/gist_id},https://api.github.com/users/MaxHaertwig/starred{/owner}{/repo},https://api.github.com/users/MaxHaertwig/subscriptions,https://api.github.com/users/MaxHaertwig/orgs,https://api.github.com/users/MaxHaertwig/repos,https://api.github.com/users/MaxHaertwig/events{/privacy},https://api.github.com/users/MaxHaertwig/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26435,https://github.com/apache/spark/pull/26435,https://github.com/apache/spark/pull/26435.diff,https://github.com/apache/spark/pull/26435.patch
139,https://api.github.com/repos/apache/spark/issues/26434,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26434/labels{/name},https://api.github.com/repos/apache/spark/issues/26434/comments,https://api.github.com/repos/apache/spark/issues/26434/events,https://github.com/apache/spark/pull/26434,519843048,MDExOlB1bGxSZXF1ZXN0MzM4NTM2NzA4,26434,[SPARK-29544] [SQL] optimize skewed partition based on data size,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-11-08T06:47:11Z,2019-12-23T10:16:57Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
This PR implement a rule `OptimizeSkewedPartitions` to optimize the skewed partition based on the partition data size. 
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
To optimize the skewed partition in runtime based on AQE
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
No
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
UT
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,JkSelf,11972570,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26434,https://github.com/apache/spark/pull/26434,https://github.com/apache/spark/pull/26434.diff,https://github.com/apache/spark/pull/26434.patch
140,https://api.github.com/repos/apache/spark/issues/26433,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26433/labels{/name},https://api.github.com/repos/apache/spark/issues/26433/comments,https://api.github.com/repos/apache/spark/issues/26433/events,https://github.com/apache/spark/pull/26433,519643825,MDExOlB1bGxSZXF1ZXN0MzM4MzQ3NjE0,26433,[SPARK-29771][K8S] Add configure to limit executor failures,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-11-08T03:30:55Z,2019-11-16T07:47:32Z,,CONTRIBUTOR," ### What changes were proposed in this pull request?
ExecutorPodsAllocator does not limit the number of executor errors or deletions, which may cause executor restart continuously without application failure.
A simple example for this, add `--conf spark.executor.extraJavaOptions=-Xmse` after spark-submit, which can make executor restart thousands of times without application failure.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Run suites and add suite test
",spark,apache,stczwd,10897625,MDQ6VXNlcjEwODk3NjI1,https://avatars0.githubusercontent.com/u/10897625?v=4,,https://api.github.com/users/stczwd,https://github.com/stczwd,https://api.github.com/users/stczwd/followers,https://api.github.com/users/stczwd/following{/other_user},https://api.github.com/users/stczwd/gists{/gist_id},https://api.github.com/users/stczwd/starred{/owner}{/repo},https://api.github.com/users/stczwd/subscriptions,https://api.github.com/users/stczwd/orgs,https://api.github.com/users/stczwd/repos,https://api.github.com/users/stczwd/events{/privacy},https://api.github.com/users/stczwd/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26433,https://github.com/apache/spark/pull/26433,https://github.com/apache/spark/pull/26433.diff,https://github.com/apache/spark/pull/26433.patch
141,https://api.github.com/repos/apache/spark/issues/26431,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26431/labels{/name},https://api.github.com/repos/apache/spark/issues/26431/comments,https://api.github.com/repos/apache/spark/issues/26431/events,https://github.com/apache/spark/pull/26431,519605344,MDExOlB1bGxSZXF1ZXN0MzM4MzE3MDcy,26431,"[SPARK-29769][SQL] Support non-correlate ""exists/not exists"" condition as all type Join's on condition","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2019-11-08T01:25:36Z,2019-11-11T05:56:12Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
As I show in https://issues.apache.org/jira/browse/SPARK-29769
we can't use `EXISTS/NOT EXISTS` as on condition in `LEFTE OUTER JOIN/ FULL OUTER JOIN / LEFT ANTI JOIN` 
This pr is to support this. 



### Why are the changes needed?
Support EXISTS/NOT EXISTS  as join's on condition


### Does this PR introduce any user-facing change?
People can use exits like 
```
SELECT s1.id FROM s1
LEFT OUTER  JOIN s2 ON s1.id = s2.id
 AND EXISTS (SELECT * from s3 where s3.id > 6)

SELECT s1.id FROM s1
RIGHT OUTER JOIN  s2 ON s1.id = s2.id
AND EXISTS (SELECT * from s3 where s3.id > 6)

SELECT s1.id FROM s1
LEFT SEMI  JOIN s2 ON s1.id = s2.id
AND EXISTS (SELECT * from s3 where s3.id > 6)


SELECT s1.id FROM s1
LEFT ANTI  JOIN s2 ON s1.id = s2.id
AND EXISTS (SELECT * from s3 where s3.id > 6)

SELECT s1.id FROM s1
FULL OUTER JOIN  s2 ON s1.id = s2.id
AND EXISTS (SELECT * from s3 where s3.id > 6)
```


### How was this patch tested?
Added UT
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26431,https://github.com/apache/spark/pull/26431,https://github.com/apache/spark/pull/26431.diff,https://github.com/apache/spark/pull/26431.patch
142,https://api.github.com/repos/apache/spark/issues/26422,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26422/labels{/name},https://api.github.com/repos/apache/spark/issues/26422/comments,https://api.github.com/repos/apache/spark/issues/26422/events,https://github.com/apache/spark/pull/26422,519096786,MDExOlB1bGxSZXF1ZXN0MzM3OTAwMDk3,26422,[SPARK-29786][SQL] Fix MetaException when dropping a partition not exists on HDFS,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-11-07T07:43:52Z,2019-11-19T03:40:19Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
When we drop a partition which exist on Hive meta and doesn't exist on HDFS, it should be dropped successfully instead of throwing MetaException.

Hive also deals with this case by this method.

Example:
Before this patch:
```
spark-sql > alter table test.tmp drop partition(stat_day=20190516);
Error: Error running query: MetaException(message:File does not exist: /user/hive/warehouse/test.db/tmp/stat_day=20190516
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(FSDirectory.java:2414)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:4719)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1237)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getContentSummary(AuthorizationProviderProxyClientProtocol.java:568)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:896)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2274)
) (state=,code=0)
```

After this patch:
```
spark-sql > alter table test.tmp drop partition(stat_day=20190516);
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (0.521 seconds)
```

### Why are the changes needed?
When we drop a partition which exist on Hive meta and doesn't exist on HDFS, we will receive MetaException. But actually, this partition has been dropped. It's quite confusing and in this case, no Exception should be thrown.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Unit tests.
",spark,apache,Deegue,25916266,MDQ6VXNlcjI1OTE2MjY2,https://avatars3.githubusercontent.com/u/25916266?v=4,,https://api.github.com/users/Deegue,https://github.com/Deegue,https://api.github.com/users/Deegue/followers,https://api.github.com/users/Deegue/following{/other_user},https://api.github.com/users/Deegue/gists{/gist_id},https://api.github.com/users/Deegue/starred{/owner}{/repo},https://api.github.com/users/Deegue/subscriptions,https://api.github.com/users/Deegue/orgs,https://api.github.com/users/Deegue/repos,https://api.github.com/users/Deegue/events{/privacy},https://api.github.com/users/Deegue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26422,https://github.com/apache/spark/pull/26422,https://github.com/apache/spark/pull/26422.diff,https://github.com/apache/spark/pull/26422.patch
143,https://api.github.com/repos/apache/spark/issues/26421,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26421/labels{/name},https://api.github.com/repos/apache/spark/issues/26421/comments,https://api.github.com/repos/apache/spark/issues/26421/events,https://github.com/apache/spark/pull/26421,519076267,MDExOlB1bGxSZXF1ZXN0MzM3ODgzNTA0,26421,[SPARK-29785][SQL] Optimize opening session of Spark Thrift Server,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-11-07T06:49:52Z,2019-11-09T20:03:05Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Add configuration `spark.sql.thriftServer.ignoreDefaultDatabase` to control whether it will request a free executor when we opening a new session of Spark Thrift Server.

`use:database` is supported in [SPARK-17819](https://issues.apache.org/jira/browse/SPARK-17819).

If we set `spark.sql.thriftServer.ignoreDefaultDatabase=true`, we don't need to allocate an executor and opening a session to Spark Thrift Server costs only ~100ms.

If we set `spark.sql.thriftServer.ignoreDefaultDatabase=false`, opening a session to Spark Thrift Server cost at least ~5s or up to the time util we get a free executor.

The default of `spark.sql.thriftServer.ignoreDefaultDatabase` is false.

### Why are the changes needed?
It's not reasonable that a free executor is needed when we open a session. Especially when all  the executors are busy and our cluster is under big pressure.


### Does this PR introduce any user-facing change?
If you don't need opening a session to Spark Thrift Server with configurations, it's better to set `spark.sql.thriftServer.ignoreDefaultDatabase=true`.



### How was this patch tested?
Unit tests.
",spark,apache,Deegue,25916266,MDQ6VXNlcjI1OTE2MjY2,https://avatars3.githubusercontent.com/u/25916266?v=4,,https://api.github.com/users/Deegue,https://github.com/Deegue,https://api.github.com/users/Deegue/followers,https://api.github.com/users/Deegue/following{/other_user},https://api.github.com/users/Deegue/gists{/gist_id},https://api.github.com/users/Deegue/starred{/owner}{/repo},https://api.github.com/users/Deegue/subscriptions,https://api.github.com/users/Deegue/orgs,https://api.github.com/users/Deegue/repos,https://api.github.com/users/Deegue/events{/privacy},https://api.github.com/users/Deegue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26421,https://github.com/apache/spark/pull/26421,https://github.com/apache/spark/pull/26421.diff,https://github.com/apache/spark/pull/26421.patch
144,https://api.github.com/repos/apache/spark/issues/26416,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26416/labels{/name},https://api.github.com/repos/apache/spark/issues/26416/comments,https://api.github.com/repos/apache/spark/issues/26416/events,https://github.com/apache/spark/pull/26416,518815701,MDExOlB1bGxSZXF1ZXN0MzM3NjU1NzMy,26416,[SPARK-29779][CORE] Compact old event log files and cleanup,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,66,2019-11-06T21:39:44Z,2019-12-24T08:31:21Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch proposes to compact old event log files when end users enable rolling event log, and clean up these files after compaction.

Here the ""compaction"" really mean is filtering out listener events for finished/removed things - like jobs and SQL executions which take most of space for event log file. To achieve this, compactor does two phases reading: 1) tracking the live jobs and SQL executions (and more to add) 2) filtering events via leveraging the information about live things and rewriting to the ""compacted"" file.

This approach retains the ability of compatibility on event log file and adds the possibility of reducing the overall size of event logs. There's a downside here as well: executor metrics for tasks would be inaccurate, as compactor will filter out the task events which job is finished, but I don't feel it as a blocker.

The compaction is triggered whenever FsHistoryProvider considers the app log as reload target: we only run compaction if there's new event log file be available via memorizing the last index to try compaction, so the compaction is actually triggered per new addition of event log file.

Also, first phase of two phases is done incrementally (in other words, event filter builder is constructed and updated incrementally, per log file); so even compaction is triggered with new log files, only new log files will be read for updating event filter builder. Second phase still requires reading all of event log files per compacting, though.

### Why are the changes needed?

One of major goal of SPARK-28594 is to prevent the event logs to become too huge, and SPARK-29779 achieves the goal. We've got another approach in prior, but the old approach required models in both KVStore and live entities to guarantee compatibility, while they're not designed to do so.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UTs. Also manually tested with my event log file.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26416,https://github.com/apache/spark/pull/26416,https://github.com/apache/spark/pull/26416.diff,https://github.com/apache/spark/pull/26416.patch
145,https://api.github.com/repos/apache/spark/issues/26404,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26404/labels{/name},https://api.github.com/repos/apache/spark/issues/26404/comments,https://api.github.com/repos/apache/spark/issues/26404/events,https://github.com/apache/spark/pull/26404,518004020,MDExOlB1bGxSZXF1ZXN0MzM2OTY3NTEy,26404,[SPARK-24442][SQL] Added parameters to control dataset show defaults,[],open,False,,[],,11,2019-11-05T20:17:46Z,2019-12-01T01:37:26Z,,NONE,"It's been a while.  I've finally had time to polish up this pull request.  See previous comments for issues I've addressed.

https://github.com/apache/spark/pull/22162


### What changes were proposed in this pull request?
https://issues.apache.org/jira/browse/SPARK-24442

### How was this patch tested?
Unit tests plus local testing. This change is designed to not modify default behavior unless a user set the following configs...

""spark.sql.show.defaultNumRows"" ->default 20 (as it was before)

""spark.sql.show.truncateMaxCharsPerColumn"" -> default 30 (as it was before)",spark,apache,AndrewKL,2659018,MDQ6VXNlcjI2NTkwMTg=,https://avatars1.githubusercontent.com/u/2659018?v=4,,https://api.github.com/users/AndrewKL,https://github.com/AndrewKL,https://api.github.com/users/AndrewKL/followers,https://api.github.com/users/AndrewKL/following{/other_user},https://api.github.com/users/AndrewKL/gists{/gist_id},https://api.github.com/users/AndrewKL/starred{/owner}{/repo},https://api.github.com/users/AndrewKL/subscriptions,https://api.github.com/users/AndrewKL/orgs,https://api.github.com/users/AndrewKL/repos,https://api.github.com/users/AndrewKL/events{/privacy},https://api.github.com/users/AndrewKL/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26404,https://github.com/apache/spark/pull/26404,https://github.com/apache/spark/pull/26404.diff,https://github.com/apache/spark/pull/26404.patch
146,https://api.github.com/repos/apache/spark/issues/26391,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26391/labels{/name},https://api.github.com/repos/apache/spark/issues/26391/comments,https://api.github.com/repos/apache/spark/issues/26391/events,https://github.com/apache/spark/pull/26391,517501043,MDExOlB1bGxSZXF1ZXN0MzM2NTU4ODcy,26391,[SPARK-29749][SQL] Add ParquetScan statistics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-11-05T01:43:11Z,2019-11-11T00:37:25Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
With datasource v2, spark add a `SupportsReportStatistics` interface and now it only implement in `FileScan` which looks like fallback plan. 

Parquet api provide a possible that get file statistics without scan total file, so we can use the more accurate statistics instead estimate.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Make parquet statistics more accurate.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add UT.",spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars0.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26391,https://github.com/apache/spark/pull/26391,https://github.com/apache/spark/pull/26391.diff,https://github.com/apache/spark/pull/26391.patch
147,https://api.github.com/repos/apache/spark/issues/26371,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26371/labels{/name},https://api.github.com/repos/apache/spark/issues/26371/comments,https://api.github.com/repos/apache/spark/issues/26371/events,https://github.com/apache/spark/pull/26371,516685070,MDExOlB1bGxSZXF1ZXN0MzM1OTExNTMy,26371,[SPARK-27976][SQL] Add built-in Array Functions: array_append,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-11-02T18:45:41Z,2019-12-04T03:36:52Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Function | Return Type | Description | Example | Result
-- | -- | -- | -- | --
array_append(anyarray,anyelement) | anyarray | append an element to the end of an array | array_append(ARRAY[1,2], 3) | {{ {1,2,3}}}


### Why are the changes needed?



https://www.postgresql.org/docs/current/functions-array.html

Other DBs:
https://phoenix.apache.org/language/functions.html#array_append
https://docs.teradata.com/reader/kmuOwjp1zEYg98JsB8fu_A/68fdFR3LWhx7KtHc9Iv5Qg

### Does this PR introduce any user-facing change?

yes. add a new array function array_append


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add uts",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26371,https://github.com/apache/spark/pull/26371,https://github.com/apache/spark/pull/26371.diff,https://github.com/apache/spark/pull/26371.patch
148,https://api.github.com/repos/apache/spark/issues/26357,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26357/labels{/name},https://api.github.com/repos/apache/spark/issues/26357/comments,https://api.github.com/repos/apache/spark/issues/26357/events,https://github.com/apache/spark/pull/26357,516027114,MDExOlB1bGxSZXF1ZXN0MzM1MzU3MDIx,26357,[SPARK-29711][SQL] Support dynamic adjust sql class log level,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-11-01T10:15:04Z,2019-11-06T14:10:10Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Support dynamic change the log level in sql

### Why are the changes needed?
It is very convenient for us to debug some problem about sql. We will not restart the thriftserver to see some debug logs. And this change is very small.


### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
UT
",spark,apache,deshanxiao,42019462,MDQ6VXNlcjQyMDE5NDYy,https://avatars0.githubusercontent.com/u/42019462?v=4,,https://api.github.com/users/deshanxiao,https://github.com/deshanxiao,https://api.github.com/users/deshanxiao/followers,https://api.github.com/users/deshanxiao/following{/other_user},https://api.github.com/users/deshanxiao/gists{/gist_id},https://api.github.com/users/deshanxiao/starred{/owner}{/repo},https://api.github.com/users/deshanxiao/subscriptions,https://api.github.com/users/deshanxiao/orgs,https://api.github.com/users/deshanxiao/repos,https://api.github.com/users/deshanxiao/events{/privacy},https://api.github.com/users/deshanxiao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26357,https://github.com/apache/spark/pull/26357,https://github.com/apache/spark/pull/26357.diff,https://github.com/apache/spark/pull/26357.patch
149,https://api.github.com/repos/apache/spark/issues/26348,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26348/labels{/name},https://api.github.com/repos/apache/spark/issues/26348/comments,https://api.github.com/repos/apache/spark/issues/26348/events,https://github.com/apache/spark/pull/26348,515523236,MDExOlB1bGxSZXF1ZXN0MzM0OTcyMzEy,26348,[SPARK-29689][WEB-UI] Enable show total shuffle read size even when task failed,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-10-31T15:26:28Z,2019-11-03T03:25:14Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
If task failed during reading shuffle data or because of executor loss, its shuffle read size would be shown as 0.

But this size is important for user, it can help detect data skew.

In this PR, I compute this size and set it when task failed.

### Why are the changes needed?

As described in above section.


### Does this PR introduce any user-facing change?
Yes, User can see shuffle read size even if task failed.

### How was this patch tested?

",spark,apache,turboFei,6757692,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26348,https://github.com/apache/spark/pull/26348,https://github.com/apache/spark/pull/26348.diff,https://github.com/apache/spark/pull/26348.patch
150,https://api.github.com/repos/apache/spark/issues/26343,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26343/labels{/name},https://api.github.com/repos/apache/spark/issues/26343/comments,https://api.github.com/repos/apache/spark/issues/26343/events,https://github.com/apache/spark/pull/26343,515294259,MDExOlB1bGxSZXF1ZXN0MzM0Nzc3ODQz,26343,[SPARK-29683][YARN] Job will fail due to executor failures all available nodes are blacklisted,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-10-31T09:46:16Z,2019-11-15T06:54:36Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

- Check the nodes in `spark.yarn.exclude.nodes` if exists in Yarn cluster.
- Loosen condition of checking if all node blacklisted: give a waiting time before job failed.

### Why are the changes needed?

My streaming job will fail due to executor failures all available nodes are blacklisted. This exception is thrown only when all node is blacklisted:
```
def isAllNodeBlacklisted: Boolean = currentBlacklistedYarnNodes.size >= numClusterNodes

val allBlacklistedNodes = excludeNodes ++ schedulerBlacklist ++ allocatorBlacklist.keySet
```
After diving into the code, I found some critical conditions not be handle properly:
- unchecked `excludeNodes`: it comes from user config. If not set properly, it may lead to ""currentBlacklistedYarnNodes.size >= numClusterNodes"". For example, we may set some nodes not in Yarn cluster.
```
excludeNodes = (invalid1, invalid2, invalid3)
clusterNodes = (valid1, valid2)
```
- `numClusterNodes` may equals 0: When HA Yarn failover, it will take some time for all NodeManagers to register ResourceManager again. In this case, `numClusterNode` may equals 0 or some other number, and Spark driver failed.
- too strong condition check: Spark driver will fail as long as `""currentBlacklistedYarnNodes.size >= numClusterNodes""`. This condition should not indicate a unrecovered fatal. For example, there are some NodeManagers restarting. So we can give some waiting time before job failed.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?

- Add new UT and update current UTs.
- manual test
",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26343,https://github.com/apache/spark/pull/26343,https://github.com/apache/spark/pull/26343.diff,https://github.com/apache/spark/pull/26343.patch
151,https://api.github.com/repos/apache/spark/issues/26340,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26340/labels{/name},https://api.github.com/repos/apache/spark/issues/26340/comments,https://api.github.com/repos/apache/spark/issues/26340/events,https://github.com/apache/spark/pull/26340,515257227,MDExOlB1bGxSZXF1ZXN0MzM0NzQ3MDAy,26340,[WIP][SPARK-29108][SQL] Add new module sql/thriftserver  with all code and UT,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406606402, 'node_id': 'MDU6TGFiZWwxNDA2NjA2NDAy', 'url': 'https://api.github.com/repos/apache/spark/labels/TESTS', 'name': 'TESTS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,39,2019-10-31T08:41:20Z,2019-11-29T16:17:18Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
As comment https://github.com/apache/spark/pull/26221#issuecomment-545954335
Raise a pr with entire code and pass origin `hive-thriftserver` 's UT

Changes:
1. Impelemnt `Type` in scala since spark con't support all type of hive
2. Implement `Service/AbstractService` prepare for remove hive conf in future
3. Construct RowSet with StructType and Row
4. Implement `HiveAuthFactory` since between 1.2.1/2.3.5, their delegation token managerment changed. Impelment on DelegationTokenMnagerment by scala
5. MV `tableTypeString` from `SparkMetadataOperationUtils` to `SparkMetadataOperation`
5. Since there are `tableTypeString` in `SparkMetadataOperation` remove `ClassicTypeMapping`, `HiveTableTypeMapping`, `TableTypeMapping` and `TableTypeMappingFactory`
6. Implement all oepration for spark since it execute in different way
7. Add new method `GetQueryId`, `SetClientInfo` for thrift version v11 in `ThriftCLIService`
8. Add `statementid` to `Operation` for implement `GetQueryId`
9. Remove `GlobalHivercFileProcessor` `setFetchSize` `processGlobalInitFile`etc
10. Remove unused `openSession` `openSessionWithImpersonation` in `CLIService`
11. Copy `hive-thriftserver` test to `spark-thriftserver` and remove `ThriftSeverShimUtils` about test UT, since we won't need it.
12. Implement it's own thrift client code. `CLIServiceClient` `ThrictCLIServiceClient`


### Why are the changes needed?
Solve hive version conflics and finally build a thriftserver make hive plugability. 


### Does this PR introduce any user-facing change?

start and stop
```
./sbin/start-spark-thriftserver.sh
./sbin/stop-spark-thriftserver.sh
```

use beeline connect as origin.

### How was this patch tested?
UT 
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26340,https://github.com/apache/spark/pull/26340,https://github.com/apache/spark/pull/26340.diff,https://github.com/apache/spark/pull/26340.patch
152,https://api.github.com/repos/apache/spark/issues/26339,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26339/labels{/name},https://api.github.com/repos/apache/spark/issues/26339/comments,https://api.github.com/repos/apache/spark/issues/26339/events,https://github.com/apache/spark/pull/26339,515243457,MDExOlB1bGxSZXF1ZXN0MzM0NzM1ODEz,26339,[SPARK-27194][SPARK-29302][SQL] Fix the issue that for dynamic partition overwrite a task would conflict with its speculative task,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-10-31T08:14:09Z,2019-12-19T06:19:13Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
As described in https://issues.apache.org/jira/browse/SPARK-27194 and https://issues.apache.org/jira/browse/SPARK-29302,  there is an issue for dynamic partition overwrite.

Based on the proposal by @advancedxy (see details in https://github.com/apache/spark/pull/24142#issuecomment-516723747), I implement this PR.

1. Set a working path under stagingDir named `_temporary`.
2. Set task name to `_temporary/partitionPath/taskAttemptId/filename` firstly.
3. after task completed, rename `_temporary/partitionPath/taskAttemptId/filename` to  `partitionPath/filename`.

### Why are the changes needed?

For dynamic partition overwrite, a task may conflict with its speculative task.


### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Existing UT.
",spark,apache,turboFei,6757692,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26339,https://github.com/apache/spark/pull/26339,https://github.com/apache/spark/pull/26339.diff,https://github.com/apache/spark/pull/26339.patch
153,https://api.github.com/repos/apache/spark/issues/26323,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26323/labels{/name},https://api.github.com/repos/apache/spark/issues/26323/comments,https://api.github.com/repos/apache/spark/issues/26323/events,https://github.com/apache/spark/pull/26323,514587137,MDExOlB1bGxSZXF1ZXN0MzM0MTgwMjQw,26323,[SPARK-29657][CORE] Iterator spill supporting radix sort with null prefix,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-10-30T11:21:26Z,2019-11-20T13:46:59Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Support ```ChainedIterator``` with ```SortedIterator``` spill

### Why are the changes needed?
In the case of radix sort, when the ```insertRecord``` part of the keyPrefix is null, the iterator type returned by ```getSortedIterator``` is ```ChainedIterator```.
Currently ```ChainedIterator``` does not support spill, causing ```UnsafeExternalSorter``` to take up a lot of execution memory, ```allocatePage``` fails, throw ```SparkOutOfMemoryError``` Unable to acquire xxx bytes of memory, got 0

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
add ut

",spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars0.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26323,https://github.com/apache/spark/pull/26323,https://github.com/apache/spark/pull/26323.diff,https://github.com/apache/spark/pull/26323.patch
154,https://api.github.com/repos/apache/spark/issues/26319,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26319/labels{/name},https://api.github.com/repos/apache/spark/issues/26319/comments,https://api.github.com/repos/apache/spark/issues/26319/events,https://github.com/apache/spark/pull/26319,514524544,MDExOlB1bGxSZXF1ZXN0MzM0MTI3NDU4,26319,[SPARK-29594][SQL] Create a Dataset from a Sequence of Case class where‚Ä¶,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-10-30T09:46:37Z,2019-12-18T17:15:25Z,,NONE,"‚Ä¶ a field name started with a number: improved exception handling.

Added test.

### What changes were proposed in this pull request?

This pr is to fix a bug discovered by me, [SPARK-29594], when creating a Dataset using .toDS() in a sequence of a case class that had a field name starting with a number.
The exception was improved, now explains why it breaks, and gives a work around of the problem.

### Why are the changes needed?

Bug fix

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Tests were added, sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
",spark,apache,PedroCorreiaLuis,44203579,MDQ6VXNlcjQ0MjAzNTc5,https://avatars1.githubusercontent.com/u/44203579?v=4,,https://api.github.com/users/PedroCorreiaLuis,https://github.com/PedroCorreiaLuis,https://api.github.com/users/PedroCorreiaLuis/followers,https://api.github.com/users/PedroCorreiaLuis/following{/other_user},https://api.github.com/users/PedroCorreiaLuis/gists{/gist_id},https://api.github.com/users/PedroCorreiaLuis/starred{/owner}{/repo},https://api.github.com/users/PedroCorreiaLuis/subscriptions,https://api.github.com/users/PedroCorreiaLuis/orgs,https://api.github.com/users/PedroCorreiaLuis/repos,https://api.github.com/users/PedroCorreiaLuis/events{/privacy},https://api.github.com/users/PedroCorreiaLuis/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26319,https://github.com/apache/spark/pull/26319,https://github.com/apache/spark/pull/26319.diff,https://github.com/apache/spark/pull/26319.patch
155,https://api.github.com/repos/apache/spark/issues/26317,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26317/labels{/name},https://api.github.com/repos/apache/spark/issues/26317/comments,https://api.github.com/repos/apache/spark/issues/26317/events,https://github.com/apache/spark/pull/26317,514496118,MDExOlB1bGxSZXF1ZXN0MzM0MTAzODMw,26317,[SPARK-29628][SQL] Forcibly create a temporary view in CREATE VIEW if referencing a temporary view,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2019-10-30T08:57:38Z,2019-12-02T03:12:56Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
When creating permanent view based on temporary view, it will be created as a temporary view. Previously, Spark this was not allowed.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
To match the behavior of postgreSQL.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
No
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
UT added.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,amanomer,40591404,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26317,https://github.com/apache/spark/pull/26317,https://github.com/apache/spark/pull/26317.diff,https://github.com/apache/spark/pull/26317.patch
156,https://api.github.com/repos/apache/spark/issues/26309,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26309/labels{/name},https://api.github.com/repos/apache/spark/issues/26309/comments,https://api.github.com/repos/apache/spark/issues/26309/events,https://github.com/apache/spark/pull/26309,514420591,MDExOlB1bGxSZXF1ZXN0MzM0MDQxODcx,26309,[SPARK-29544][CORE] collect the runtime statistics of row count in map stage,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-10-30T06:03:20Z,2019-11-14T01:55:05Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Similar with the approach of collecting data size, this PR collect the row count info when shuffle write and wrap it in MapStatus, then driver can get the row count info from the returned MapStatus.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
In order to optimize the skewed partition, we need collect the row count statistics in map stage. 
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
No
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
unit tests
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,JkSelf,11972570,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26309,https://github.com/apache/spark/pull/26309,https://github.com/apache/spark/pull/26309.diff,https://github.com/apache/spark/pull/26309.patch
157,https://api.github.com/repos/apache/spark/issues/26297,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26297/labels{/name},https://api.github.com/repos/apache/spark/issues/26297/comments,https://api.github.com/repos/apache/spark/issues/26297/events,https://github.com/apache/spark/pull/26297,514071699,MDExOlB1bGxSZXF1ZXN0MzMzNzQ1Mzk4,26297,[SPARK-29665][SQL] refine the TableProvider interface,[],open,False,,[],,11,2019-10-29T16:53:29Z,2019-12-05T03:22:13Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Instead of having several overloads of `getTable` method in `TableProvider`, it's better to have 2 methods explicitly: `inferSchema` and `inferPartitioning`. With a single `getTable` method that takes everything: schema, partitioning and properties.

Spark supports: 1) infer both schema and partitioning. 2) specifies schema and infer partitioning 3) specifies both schema and partitioning. So `inferPartitioning` method takes `schema` as input, as schema must be known before inferring partitioning.

A lot of changes are made to file source v2, to move the schema inference from `FileTable` to `FileDataSourceV2`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This is inspired by the discussion in https://github.com/apache/spark/pull/25651#discussion_r328340643

It's better to let the APIs have explicit meanings.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests.",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26297,https://github.com/apache/spark/pull/26297,https://github.com/apache/spark/pull/26297.diff,https://github.com/apache/spark/pull/26297.patch
158,https://api.github.com/repos/apache/spark/issues/26293,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26293/labels{/name},https://api.github.com/repos/apache/spark/issues/26293/comments,https://api.github.com/repos/apache/spark/issues/26293/events,https://github.com/apache/spark/pull/26293,513803391,MDExOlB1bGxSZXF1ZXN0MzMzNTIxODY5,26293,[SPARK-29595][SQL] Insertion with named_struct should match by name,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-10-29T09:50:42Z,2019-11-19T06:47:52Z,,CONTRIBUTOR,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
When data is inserted into table having column of type named_struct, field names is mandatory to be written and with this PR, field names of StructType will always be verified.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
named_struct is not validating field names before insertion.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
No.
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
Tested manually.
```
$ create table str using parquet as(select named_struct('a', 1, 'b', 2) as data);
$ insert into str values named_struct('b', 4, 'a', 1);
Error in query: Cannot write incompatible data to table '`default`.`str`':
- Struct 'data' 0-th field name does not match (may be out of order): expected 'a', found 'b'
- Struct 'data' 1-th field name does not match (may be out of order): expected 'b', found 'a';
$ insert into str values named_struct('a', 4, 'b', 1);
$ select * from str;
{""a"":4,""b"":1}
{""a"":1,""b"":2}
```

<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,amanomer,40591404,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26293,https://github.com/apache/spark/pull/26293,https://github.com/apache/spark/pull/26293.diff,https://github.com/apache/spark/pull/26293.patch
159,https://api.github.com/repos/apache/spark/issues/26286,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26286/labels{/name},https://api.github.com/repos/apache/spark/issues/26286/comments,https://api.github.com/repos/apache/spark/issues/26286/events,https://github.com/apache/spark/pull/26286,513571401,MDExOlB1bGxSZXF1ZXN0MzMzMzM1MzM2,26286,[SPARK-26739][SQL] Standardized Join Types for DataFrames,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-10-28T21:36:20Z,2019-10-30T19:45:48Z,,CONTRIBUTOR,"Adding overloads for all join methods that take the JoinType parameter instead of string

Testing @deprecated marking for string version

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Adding overloaded versions of all `join` methods in `Dataset` that take the strongly-typed `JoinType` as the `joinType` parameter, instead of `String`.

Keeping `String` versions, marking deprecated, and simply calling the new versions.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

To make the `DataFrame` API easier to use.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

Yes, there are new join method overloads available that take `JoinType`.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

`sql` sbt tests were run.
",spark,apache,jeff303,3521562,MDQ6VXNlcjM1MjE1NjI=,https://avatars0.githubusercontent.com/u/3521562?v=4,,https://api.github.com/users/jeff303,https://github.com/jeff303,https://api.github.com/users/jeff303/followers,https://api.github.com/users/jeff303/following{/other_user},https://api.github.com/users/jeff303/gists{/gist_id},https://api.github.com/users/jeff303/starred{/owner}{/repo},https://api.github.com/users/jeff303/subscriptions,https://api.github.com/users/jeff303/orgs,https://api.github.com/users/jeff303/repos,https://api.github.com/users/jeff303/events{/privacy},https://api.github.com/users/jeff303/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26286,https://github.com/apache/spark/pull/26286,https://github.com/apache/spark/pull/26286.diff,https://github.com/apache/spark/pull/26286.patch
160,https://api.github.com/repos/apache/spark/issues/26281,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26281/labels{/name},https://api.github.com/repos/apache/spark/issues/26281/comments,https://api.github.com/repos/apache/spark/issues/26281/events,https://github.com/apache/spark/pull/26281,513214684,MDExOlB1bGxSZXF1ZXN0MzMzMDQ0NTI0,26281,[SPARK-29618] remove V1_BATCH_WRITE table capability,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-10-28T10:20:57Z,2019-10-30T08:35:41Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Build the `BatchWrite` in the planner and get rid of the `V1_BATCH_WRITE` table capability.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
It's always better to make the API simpler and easier to implement. When I was working on v1 read fallback API at https://github.com/apache/spark/pull/26231 , I realized that we don't need a table capability for it, because we create the `Scan` object in the planner. We can do the same thing for v1 write fallback API as well.

This can also reduce duplicated code of creating the `WriteBuilder`.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26281,https://github.com/apache/spark/pull/26281,https://github.com/apache/spark/pull/26281.diff,https://github.com/apache/spark/pull/26281.patch
161,https://api.github.com/repos/apache/spark/issues/26280,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26280/labels{/name},https://api.github.com/repos/apache/spark/issues/26280/comments,https://api.github.com/repos/apache/spark/issues/26280/events,https://github.com/apache/spark/pull/26280,513186670,MDExOlB1bGxSZXF1ZXN0MzMzMDIyMzkw,26280,[SPARK-14922][SPARK-17732][SPARK-23866][SQL] Support partition filter in ALTER TABLE DROP PARTITION and batch dropping PARTITIONS,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2019-10-28T09:23:11Z,2019-12-19T10:08:58Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Spark only  can drop partitions by exact values. For instance, Spark doesn't support:

>  ALTER TABLE mytable DROP PARTITION(mydate < '2018-04-06') 

The PR adds the support to this syntax.

The PR takes input from the effort in #19691 by @DazhuangSu ,  in #20999 by @mgaido91 ,
, there are some related PRs: #15987,#16036,#19691,#15704.
some related issues:
https://issues.apache.org/jira/browse/SPARK-14922
https://issues.apache.org/jira/browse/SPARK-17732
https://issues.apache.org/jira/browse/SPARK-23866
### How was this patch tested?
Add unittests.",spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars3.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26280,https://github.com/apache/spark/pull/26280,https://github.com/apache/spark/pull/26280.diff,https://github.com/apache/spark/pull/26280.patch
162,https://api.github.com/repos/apache/spark/issues/26272,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26272/labels{/name},https://api.github.com/repos/apache/spark/issues/26272/comments,https://api.github.com/repos/apache/spark/issues/26272/events,https://github.com/apache/spark/pull/26272,512943888,MDExOlB1bGxSZXF1ZXN0MzMyODQwMTQ0,26272,[SPARK-27736][Core][SHUFFLE] Improve handling of FetchFailures caused by ExternalShuffleService losing track of executor registrations,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-10-27T09:48:00Z,2019-10-31T09:36:01Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
As described in https://issues.apache.org/jira/browse/SPARK-27736, if a single external shuffle service process reboots and fails to recover the list of registered executors, a lot FetchFailedExceptions would be thrown and it would cause application failed eventually.

In this PR, I let externalBlockClient can query  whether executors are registered on the External Shuffle Service.
And when fetchFailedException thrown, I will query whether the executors on this host are registered.
 If not,  unregister relative output.


### Why are the changes needed?
This PR improves handling of FetchFailures caused by ExternalShuffleService losing track of executor registrations 

### Does this PR introduce any user-facing change?
No.


### How was this patch tested?

Added UT.",spark,apache,turboFei,6757692,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26272,https://github.com/apache/spark/pull/26272,https://github.com/apache/spark/pull/26272.diff,https://github.com/apache/spark/pull/26272.patch
163,https://api.github.com/repos/apache/spark/issues/26270,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26270/labels{/name},https://api.github.com/repos/apache/spark/issues/26270/comments,https://api.github.com/repos/apache/spark/issues/26270/events,https://github.com/apache/spark/pull/26270,512908892,MDExOlB1bGxSZXF1ZXN0MzMyODE3MzA3,26270,[SPARK-26544][SQL] Escape struct string in spark thriftserver to keep alignment with hive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-10-27T02:04:53Z,2019-12-15T10:51:25Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
In this PR, I proposed to escape strings when serializing Map/Array/Struct type in Spark ThriftServer, so that the result is always a valid JSON, which is what Hive does. 
Add a conf to make sure it doesn't break compatibility.

### Why are the changes needed?
The string serialized from Map/Array/Struct type in Spark ThriftServer is not a valid JSON when  the elements of Map/Array/Struct contain special characters such as `""`. 

For example, select a field whose type is map<string, string>, the spark thrift server returns
BEFORE
```{""author_id"":""123"",""log_pb"":""{""impr_id"":""20181231""}"",""request_id"":""001""}```

AFTER
```{""author_id"":""123"", ""log_pb"":""{\""impr_id\"":\""20181231\""}"",""request_id"":""001""}```

### Does this PR introduce any user-facing change?
No


### How was this patch tested?
HiveResultSuite.scala 
",spark,apache,WangGuangxin,1312321,MDQ6VXNlcjEzMTIzMjE=,https://avatars0.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26270,https://github.com/apache/spark/pull/26270,https://github.com/apache/spark/pull/26270.diff,https://github.com/apache/spark/pull/26270.patch
164,https://api.github.com/repos/apache/spark/issues/26257,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26257/labels{/name},https://api.github.com/repos/apache/spark/issues/26257/comments,https://api.github.com/repos/apache/spark/issues/26257/events,https://github.com/apache/spark/pull/26257,512597013,MDExOlB1bGxSZXF1ZXN0MzMyNTc5NDM3,26257,[SPARK-29606][SQL] Improve EliminateOuterJoin performance,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-10-25T16:03:13Z,2019-11-18T23:38:55Z,,MEMBER,"### What changes were proposed in this pull request?

This PR try to improve `EliminateOuterJoin` performance via avoid generating too many constraints. For example:
```scala
import org.apache.spark.sql.catalyst.plans.logical.Project
spark.sql(""CREATE TABLE IF NOT EXISTS spark_29606(a int, b int, c int) USING parquet"")
spark.sql(""SELECT a as a1, b as b1, c as c1, abc as abc1 FROM (SELECT a, b, c, a + b + c as abc FROM spark_29606) t"")
  .queryExecution.analyzed.asInstanceOf[Project].validConstraints.toSeq.sortBy(_.toString).foreach(println)
```

**Before this PR**:
```
(((a#5 + b#6) + c#7) <=> abc#0)
(((a#5 + b#6) + c#7) <=> abc1#4)
(((a#5 + b#6) + c1#3) <=> abc#0)
(((a#5 + b#6) + c1#3) <=> abc1#4)
(((a#5 + b1#2) + c#7) <=> abc#0)
(((a#5 + b1#2) + c#7) <=> abc1#4)
(((a#5 + b1#2) + c1#3) <=> abc#0)
(((a#5 + b1#2) + c1#3) <=> abc1#4)
(((a1#1 + b#6) + c#7) <=> abc#0)
(((a1#1 + b#6) + c#7) <=> abc1#4)
(((a1#1 + b#6) + c1#3) <=> abc#0)
(((a1#1 + b#6) + c1#3) <=> abc1#4)
(((a1#1 + b1#2) + c#7) <=> abc#0)
(((a1#1 + b1#2) + c#7) <=> abc1#4)
(((a1#1 + b1#2) + c1#3) <=> abc#0)
(((a1#1 + b1#2) + c1#3) <=> abc1#4)
(a#5 <=> a1#1)
(abc#0 <=> abc1#4)
(b#6 <=> b1#2)
(c#7 <=> c1#3)
```

**After this PR**:
```
(((a#5 + b#6) + c#7) <=> abc#0)
(a#5 <=> a1#1)
(abc#0 <=> abc1#4)
(b#6 <=> b1#2)
(c#7 <=> c1#3)
```



### Why are the changes needed?
Improve `EliminateOuterJoin` performance.

**Before this PR**:
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 9323
Total time: 15.995000924 seconds

Rule                                                                                               Effective Time / Total Time                     Effective Runs / Total Runs

org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin                                         0 / 13359017999                                 0 / 4
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations                                   990683801 / 991674120                           2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables                                      0 / 443718064                                   0 / 18
org.apache.spark.sql.catalyst.analysis.DecimalPrecision                                            43087519 / 81709524                             2 / 18
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions                               63414650 / 63414650                             1 / 1
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences                                  42587256 / 62760566                             5 / 18
...
```

**After this PR**:
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 9323
Total time: 3.03253427 seconds

Rule                                                                                               Effective Time / Total Time                     Effective Runs / Total Runs

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations                                   1130336633 / 1131323257                         2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables                                      0 / 448236638                                   0 / 18
org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin                                         0 / 107133411                                   0 / 4
org.apache.spark.sql.catalyst.analysis.DecimalPrecision                                            43965067 / 84085638                             2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences                                  44448673 / 66690250                             5 / 18
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions                               59631169 / 59631169                             1 / 1
...
```


### Does this PR introduce any user-facing change?

No.


### How was this patch tested?

Unit test.
",spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26257,https://github.com/apache/spark/pull/26257,https://github.com/apache/spark/pull/26257.diff,https://github.com/apache/spark/pull/26257.patch
165,https://api.github.com/repos/apache/spark/issues/26242,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26242/labels{/name},https://api.github.com/repos/apache/spark/issues/26242/comments,https://api.github.com/repos/apache/spark/issues/26242/events,https://github.com/apache/spark/pull/26242,511787743,MDExOlB1bGxSZXF1ZXN0MzMxOTE5MTg0,26242,[SPARK-29586][SQL]changge jdbc method param lowerBound and upperBound DataType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-10-24T08:26:01Z,2019-10-28T13:36:07Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

changge param dataType

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
 partionColum timestamp and date dataType is not support

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",spark,apache,MrDLontheway,29403543,MDQ6VXNlcjI5NDAzNTQz,https://avatars2.githubusercontent.com/u/29403543?v=4,,https://api.github.com/users/MrDLontheway,https://github.com/MrDLontheway,https://api.github.com/users/MrDLontheway/followers,https://api.github.com/users/MrDLontheway/following{/other_user},https://api.github.com/users/MrDLontheway/gists{/gist_id},https://api.github.com/users/MrDLontheway/starred{/owner}{/repo},https://api.github.com/users/MrDLontheway/subscriptions,https://api.github.com/users/MrDLontheway/orgs,https://api.github.com/users/MrDLontheway/repos,https://api.github.com/users/MrDLontheway/events{/privacy},https://api.github.com/users/MrDLontheway/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26242,https://github.com/apache/spark/pull/26242,https://github.com/apache/spark/pull/26242.diff,https://github.com/apache/spark/pull/26242.patch
166,https://api.github.com/repos/apache/spark/issues/26231,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26231/labels{/name},https://api.github.com/repos/apache/spark/issues/26231/comments,https://api.github.com/repos/apache/spark/issues/26231/events,https://github.com/apache/spark/pull/26231,511386770,MDExOlB1bGxSZXF1ZXN0MzMxNTk0NDE5,26231,[SPARK-29572][SQL] add v1 read fallback API in DS v2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-10-23T15:03:40Z,2019-12-24T08:34:27Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add a `V1Scan` interface, so that data source v1 implementations can migrate to DS v2 much easier.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
It's a lot of work to migrate v1 sources to DS v2. The new API added here can allow v1 sources to go through v2 code paths without implementing all the Batch, Stream, PartitionReaderFactory, ... stuff.

We already have a v1 write fallback API after https://github.com/apache/spark/pull/25348

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
new test suite",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26231,https://github.com/apache/spark/pull/26231,https://github.com/apache/spark/pull/26231.diff,https://github.com/apache/spark/pull/26231.patch
167,https://api.github.com/repos/apache/spark/issues/26221,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26221/labels{/name},https://api.github.com/repos/apache/spark/issues/26221/comments,https://api.github.com/repos/apache/spark/issues/26221/events,https://github.com/apache/spark/pull/26221,511049715,MDExOlB1bGxSZXF1ZXN0MzMxMzE5MTk4,26221,[WIP][SPARK-29108][SQL] Add new module sql/thriftserver and  add v11 thrift protocol,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406606402, 'node_id': 'MDU6TGFiZWwxNDA2NjA2NDAy', 'url': 'https://api.github.com/repos/apache/spark/labels/TESTS', 'name': 'TESTS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-10-23T03:32:20Z,2019-10-28T14:46:31Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

First step as we discussed in https://github.com/apache/spark/pull/25721#issuecomment-544540930

Now we just add a new module and implement thrift protocol v11, prepare for next work.

### Why are the changes needed?
implement a new thriftserver by spark


### Does this PR introduce any user-facing change?
No for now


### How was this patch tested?
Don't need test now
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26221,https://github.com/apache/spark/pull/26221,https://github.com/apache/spark/pull/26221.diff,https://github.com/apache/spark/pull/26221.patch
168,https://api.github.com/repos/apache/spark/issues/26219,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26219/labels{/name},https://api.github.com/repos/apache/spark/issues/26219/comments,https://api.github.com/repos/apache/spark/issues/26219/events,https://github.com/apache/spark/pull/26219,511005528,MDExOlB1bGxSZXF1ZXN0MzMxMjg0MTkw,26219,[SPARK-29563][SQL] CREATE TABLE LIKE should look up catalog/table like v2 commands,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2019-10-23T00:44:24Z,2019-11-12T16:20:26Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Change to make sure ```CREATE TABLES LIKE```  statement go through the same catalog/table resolution framework of v2 commands.


### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users.

### Does this PR introduce any user-facing change?
Yes. Attempting to execute ```CREATE TABLE LIKE``` on v2 catalog results in an error.


### How was this patch tested?
Added unit tests.
",spark,apache,dilipbiswal,14225158,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26219,https://github.com/apache/spark/pull/26219,https://github.com/apache/spark/pull/26219.diff,https://github.com/apache/spark/pull/26219.patch
169,https://api.github.com/repos/apache/spark/issues/26213,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26213/labels{/name},https://api.github.com/repos/apache/spark/issues/26213/comments,https://api.github.com/repos/apache/spark/issues/26213/events,https://github.com/apache/spark/pull/26213,510724848,MDExOlB1bGxSZXF1ZXN0MzMxMDQ5MTcz,26213,[SPARK-29550][SQL] - enhance session catalog locking,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-10-22T15:11:23Z,2019-10-25T13:30:00Z,,NONE,"### What changes were proposed in this pull request?
In my streaming application``spark.streaming.concurrentJobs`` is set to 50 which is used as size for underlying thread pool. I perform several sql operations on dataframes and automatically create/alter tables/view in runtime. I order to do that i invoke ``create ... if not exists operations`` on driver on each batch invocation. Once i noticed that  most of batch time is spent on driver but not on executors. I made a thread dump and figured out that most of the threads are blocked on SessionCatalog operation waiting for a lock.  

Existing implementation of SessionCatalog uses a single lock which is used almost by all the methods to guard ``currentDb`` and ``tempViews`` variables. I propose to enhance locking behaviour of SessionCatalog by :

1. Employing ``ReadWriteLock`` which allows to execute read operations concurrently. 
2. Replace synchronized with the corresponding read or write lock.

Also it's possible to go even further and strip locks for ``currentDb`` and ``tempViews`` but i'm not sure whether it's possible from the implementation point of view. 
Probably someone will help me with this?



### How was this patch tested?
Only via existing test suits.
",spark,apache,choojoyq,13151161,MDQ6VXNlcjEzMTUxMTYx,https://avatars0.githubusercontent.com/u/13151161?v=4,,https://api.github.com/users/choojoyq,https://github.com/choojoyq,https://api.github.com/users/choojoyq/followers,https://api.github.com/users/choojoyq/following{/other_user},https://api.github.com/users/choojoyq/gists{/gist_id},https://api.github.com/users/choojoyq/starred{/owner}{/repo},https://api.github.com/users/choojoyq/subscriptions,https://api.github.com/users/choojoyq/orgs,https://api.github.com/users/choojoyq/repos,https://api.github.com/users/choojoyq/events{/privacy},https://api.github.com/users/choojoyq/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26213,https://github.com/apache/spark/pull/26213,https://github.com/apache/spark/pull/26213.diff,https://github.com/apache/spark/pull/26213.patch
170,https://api.github.com/repos/apache/spark/issues/26206,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26206/labels{/name},https://api.github.com/repos/apache/spark/issues/26206/comments,https://api.github.com/repos/apache/spark/issues/26206/events,https://github.com/apache/spark/pull/26206,510586838,MDExOlB1bGxSZXF1ZXN0MzMwOTM0Nzkx,26206,[SPARK-29551][CORE] Fix a bug about fetch failed when an executor is lost,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2019-10-22T11:16:07Z,2019-11-14T10:04:28Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

When an executor lost with some reason and some things (e.g. the external shuffle service or host lost on the executor's host.) happened, and the executor loses time happens to be reduce stage fetch failed from it which is really bad, the previous only call `mapOutputTracker.unregisterMapOutput(shuffleId, mapIndex, bmAddress) ` to mark clear shuffle status for the mapper at this time , but the other mappers shuffle status on the executor are also unnavailable and the DagScheduler does Not  know that, so the reduce stages will fetch failed again when fetch them,  the unavailable shuffle status can only be resubmitted by a nest retry stage which is the regression.

As we all know that the previous will call ` mapOutputTracker.removeOutputsOnHost(host) `  to  mark clear shuffle status on the host  or `mapOutputTracker.removeOutputsOnExecutor(execId) ` to  mark clear shuffle status on the executor  when reduce stage fetches failed and the executor is active, while it does NOT nothing when the executor lost happened, which  is really bad .

So we should distinguish the failedEpoch of 'executor lost' from the fetchFailedEpoch of 'fetch failed' to solve the above problem.
 
### Why are the changes needed?

The regression  has been described above.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Add unittests.",spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars3.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26206,https://github.com/apache/spark/pull/26206,https://github.com/apache/spark/pull/26206.diff,https://github.com/apache/spark/pull/26206.patch
171,https://api.github.com/repos/apache/spark/issues/26201,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26201/labels{/name},https://api.github.com/repos/apache/spark/issues/26201/comments,https://api.github.com/repos/apache/spark/issues/26201/events,https://github.com/apache/spark/pull/26201,510423084,MDExOlB1bGxSZXF1ZXN0MzMwODAwMDcy,26201,[SPARK-29543][SS][UI] Init structured streaming ui,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,42,2019-10-22T04:32:08Z,2019-12-12T03:33:03Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

- Add two pages
   - ""/sql/streaming"": Streaming Query Page, providing some aggregate information for running/complete streaming queries.
  - ""/sql/streaming/statistics"": Streaming Query Statistics Page, providing detailed information for streaming query, including `Input Rate`, `Process Rate`, `Input Rows`, `Batch Duration` and `Operation Duration`
- Add two new metric `totalInputRecords` and `batchDuration` 
- Add two html resource files: `streaming-page.css` and `streaming-page.js`

![image](https://user-images.githubusercontent.com/7402327/67258613-15eafd80-f4c4-11e9-8d46-3fec8a07ff08.png)
![image](https://user-images.githubusercontent.com/7402327/67258631-2b602780-f4c4-11e9-93df-03e1e060cf12.png)
![image](https://user-images.githubusercontent.com/7402327/67259136-e38ecf80-f4c6-11e9-82df-42cc8d4f73df.png)
![image](https://user-images.githubusercontent.com/7402327/67260466-0f618380-f4ce-11e9-9a2c-8ab3480a59d3.png)

### Why are the changes needed?

It helps users to better monitor Structured Streaming job.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

- new added and existing UTs
- manual test",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26201,https://github.com/apache/spark/pull/26201,https://github.com/apache/spark/pull/26201.diff,https://github.com/apache/spark/pull/26201.patch
172,https://api.github.com/repos/apache/spark/issues/26197,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26197/labels{/name},https://api.github.com/repos/apache/spark/issues/26197/comments,https://api.github.com/repos/apache/spark/issues/26197/events,https://github.com/apache/spark/pull/26197,510342290,MDExOlB1bGxSZXF1ZXN0MzMwNzM0NDMw,26197,[SPARK-29577][MLLIB] Implement p-value simulation and unit tests for chi2 test,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-10-21T23:01:32Z,2019-11-23T00:31:37Z,,NONE,"### What changes were proposed in this pull request?
This PR implements monte-carlo simulation of p-values for the ChiSqTest in mllib. For other implementations, see the following references:
* https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/chisq.test
* https://en.wikipedia.org/wiki/Generalized_p-value

### Why are the changes needed?
While monte-carlo simulation is a common approach to estimate p-values, a robust scalable implementation in Spark was non-trivial, so we hope others can re-use these efforts.

### Does this PR introduce any user-facing change?
We provide a new boolean parameter `simulatePValue` to the ChiSqTest so that users can request p-value simulation, and also an integer parameter `numDraw` so that users can specify the number of draws to take. The `getChi2Digest` method is also exposed in case users find value in the digest object itself which allows extraction of arbitrary quantiles, cdf, etc.

### How was this patch tested?
This PR also implements the `ChiSqTestSuite` with some tests to verify that both the ChiSqTest itself and the new p-value simulation are working correctly by evaluating that test cases expected to pass and fail a chi squared test actually work as expected. 

We ran these tests with the following results: 
```
$ build/mvn package -pl mllib -Dtest=none -DwildcardSuites=org.apache.spark.mllib.stat.test.ChiSqTestSuite
...
ChiSqTestSuite:
- theoretical chi2 test
- simulated/empirical chi2 test
Run completed in 1 minute, 22 seconds.
Total number of tests run: 2
Suites: completed 2, aborted 0
Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0
All tests passed.
...
[INFO] BUILD SUCCESS
```",spark,apache,atronchi,4906224,MDQ6VXNlcjQ5MDYyMjQ=,https://avatars2.githubusercontent.com/u/4906224?v=4,,https://api.github.com/users/atronchi,https://github.com/atronchi,https://api.github.com/users/atronchi/followers,https://api.github.com/users/atronchi/following{/other_user},https://api.github.com/users/atronchi/gists{/gist_id},https://api.github.com/users/atronchi/starred{/owner}{/repo},https://api.github.com/users/atronchi/subscriptions,https://api.github.com/users/atronchi/orgs,https://api.github.com/users/atronchi/repos,https://api.github.com/users/atronchi/events{/privacy},https://api.github.com/users/atronchi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26197,https://github.com/apache/spark/pull/26197,https://github.com/apache/spark/pull/26197.diff,https://github.com/apache/spark/pull/26197.patch
173,https://api.github.com/repos/apache/spark/issues/26193,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26193/labels{/name},https://api.github.com/repos/apache/spark/issues/26193/comments,https://api.github.com/repos/apache/spark/issues/26193/events,https://github.com/apache/spark/pull/26193,509924113,MDExOlB1bGxSZXF1ZXN0MzMwMzc0NDIy,26193,[WIP][SPARK-25065][k8s] Allow setting up correct logging configuration on driver and executor.,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-10-21T12:15:59Z,2019-10-22T21:18:01Z,,MEMBER,"This is work in progress, because I wanted some early feedback as to overall approach taken. At the moment with current code, this mounting feature does not work in client mode of submission. The reason is, in client mode kubernetes does not create any resources. 

My question is, what is the correct way to create a config map, while in the client mode. If it is created in a place like ExecutorPodAllocator, then since it is run repeatedly, then we would not want the config map to be created again and again on each executor pod allocation. So, what would be correct place to do it.

### What changes were proposed in this pull request?
A new feature to propagate correct logging configuration file either by picking up from user specified logger configuration file on the classpath or through user specified k8s config map.

### Why are the changes needed?
Please see referenced JIRA.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually tested.
Added some unit tests as well. Since this is in WIP, more tests and docs may be added as needed, once this PR is finalised. 
",spark,apache,ScrapCodes,992952,MDQ6VXNlcjk5Mjk1Mg==,https://avatars3.githubusercontent.com/u/992952?v=4,,https://api.github.com/users/ScrapCodes,https://github.com/ScrapCodes,https://api.github.com/users/ScrapCodes/followers,https://api.github.com/users/ScrapCodes/following{/other_user},https://api.github.com/users/ScrapCodes/gists{/gist_id},https://api.github.com/users/ScrapCodes/starred{/owner}{/repo},https://api.github.com/users/ScrapCodes/subscriptions,https://api.github.com/users/ScrapCodes/orgs,https://api.github.com/users/ScrapCodes/repos,https://api.github.com/users/ScrapCodes/events{/privacy},https://api.github.com/users/ScrapCodes/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26193,https://github.com/apache/spark/pull/26193,https://github.com/apache/spark/pull/26193.diff,https://github.com/apache/spark/pull/26193.patch
174,https://api.github.com/repos/apache/spark/issues/26162,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26162/labels{/name},https://api.github.com/repos/apache/spark/issues/26162/comments,https://api.github.com/repos/apache/spark/issues/26162/events,https://github.com/apache/spark/pull/26162,508995798,MDExOlB1bGxSZXF1ZXN0MzI5NjcxMDgy,26162,[SPARK-29438][SS] Use partition ID of StateStoreAwareZipPartitionsRDD for determining partition ID of state store in stream-stream join,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2019-10-18T10:31:43Z,2019-12-06T12:45:48Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Credit to @uncleGen for discovering the problem and providing simple reproducer as UT. New UT in this patch is borrowed from #26156 and I'm retaining a commit from #26156 (except unnecessary part on this path) to properly give a credit.

This patch fixes the issue that partition ID could be mis-assigned when the query contains UNION and stream-stream join is placed on the right side. We assume the range of partition IDs as `(0 ~ number of shuffle partitions - 1)` for stateful operators, but when we use stream-stream join on the right side of UNION, the range of partition ID of task goes to `(number of partitions in left side, number of partitions in left side + number of shuffle partitions - 1)`, which `number of partitions in left side` can be changed in some cases (new UT points out the one of the cases). 

The root reason of bug is that stream-stream join picks the partition ID from TaskContext, which wouldn't be same as partition ID from source if union is being used. Hopefully we can pick the right partition ID from source in StateStoreAwareZipPartitionsRDD - this patch leverages that partition ID.

### Why are the changes needed?

This patch will fix the broken of assumption of partition range on stateful operator, as well as fix the issue reported in JIRA issue SPARK-29438.

### Does this PR introduce any user-facing change?

Yes, if their query is using UNION and stream-stream join is placed on the right side. They may encounter the problem to read state from checkpoint and may need to discard checkpoint to continue.

### How was this patch tested?

Added UT which fails on current master branch, and passes with this patch.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26162,https://github.com/apache/spark/pull/26162,https://github.com/apache/spark/pull/26162.diff,https://github.com/apache/spark/pull/26162.patch
175,https://api.github.com/repos/apache/spark/issues/26161,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26161/labels{/name},https://api.github.com/repos/apache/spark/issues/26161/comments,https://api.github.com/repos/apache/spark/issues/26161/events,https://github.com/apache/spark/pull/26161,508978888,MDExOlB1bGxSZXF1ZXN0MzI5NjU3NTg0,26161,[SPARK-27900][CORE][K8s] Add `spark.driver.killOnOOMError` flag in cluster mode,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-10-18T09:58:06Z,2019-11-08T20:45:11Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

- Adds a flag to make the driver exit in case of an oom error in cluster mode (enabled by default).
- Adds integration tests for the K8s manager.
- Adds verbose flag support within the driver's container.

### Why are the changes needed?
See for details Spark-27900. In addition, this follows the discussion here: #24796. Without this pods on K8s will keep running although Spark has failed. In addition current behavior creates a problem to the Spark Operator and any other operator as it cannot detect failure at the K8s level.

### How was this patch tested?
Manually by launching SparkPi with a large number 100000000 which leads to an oom due to the large number of tasks allocated.

```
$kubectl logs spark-pi-driver -n spark
19/10/18 09:47:02 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/10/18 09:47:02 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.6:33435 with 413.9 MiB RAM, BlockManagerId(2, 172.17.0.6, 33435, None)
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError=""kill -9 %p""
#   Executing /bin/sh -c ""kill -9 14""...
```
Also there are two integration tests for the K8s resource manager. Testing might be needed for the other managers.",spark,apache,skonto,7945591,MDQ6VXNlcjc5NDU1OTE=,https://avatars1.githubusercontent.com/u/7945591?v=4,,https://api.github.com/users/skonto,https://github.com/skonto,https://api.github.com/users/skonto/followers,https://api.github.com/users/skonto/following{/other_user},https://api.github.com/users/skonto/gists{/gist_id},https://api.github.com/users/skonto/starred{/owner}{/repo},https://api.github.com/users/skonto/subscriptions,https://api.github.com/users/skonto/orgs,https://api.github.com/users/skonto/repos,https://api.github.com/users/skonto/events{/privacy},https://api.github.com/users/skonto/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26161,https://github.com/apache/spark/pull/26161,https://github.com/apache/spark/pull/26161.diff,https://github.com/apache/spark/pull/26161.patch
176,https://api.github.com/repos/apache/spark/issues/26150,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26150/labels{/name},https://api.github.com/repos/apache/spark/issues/26150/comments,https://api.github.com/repos/apache/spark/issues/26150/events,https://github.com/apache/spark/pull/26150,508413125,MDExOlB1bGxSZXF1ZXN0MzI5MjA5NTI4,26150,[SPARK-29501][SQL] SupportsPushDownRequiredColumns should report pruned schema immediately,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-10-17T11:31:37Z,2019-10-22T11:32:46Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
`SupportsPushDownRequiredColumns` should return the pruned schema, instead of building the `Scan` and get `readSchema` from `Scan`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
I found this problem while developing the v1 read fallback API following https://github.com/apache/spark/pull/25348.

The problem is that, v1 read fallback API needs to rely on `ScanBuilder` to do filter pushdown and column pruning, and create a v1 `BaseRelation` at the end.

However, the `SupportsPushDownRequiredColumns` is not well designed. Spark must create the v2 `Scan` to get the result of column pruning: the pruned schema. This is not possible for data sources implementing v1 read fallback API.

By doing this change, we also make it easier to implement DS v2: users don't need to implement schema twice (in `Table` and `Scan`) if they don't support column pruning.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26150,https://github.com/apache/spark/pull/26150,https://github.com/apache/spark/pull/26150.diff,https://github.com/apache/spark/pull/26150.patch
177,https://api.github.com/repos/apache/spark/issues/26144,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26144/labels{/name},https://api.github.com/repos/apache/spark/issues/26144/comments,https://api.github.com/repos/apache/spark/issues/26144/events,https://github.com/apache/spark/pull/26144,508032810,MDExOlB1bGxSZXF1ZXN0MzI4OTA0NjIw,26144,[SPARK-27903][SQL] Improve the error messages of SQL parser,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-10-16T18:48:24Z,2019-10-29T07:23:06Z,,CONTRIBUTOR,"Adding grammar rules to capture extra left and extra right parens for expressions and subqueries

Creating reusable parenthesized query rule and plugging into all the places that use it (and updating AstBuilder accordingly)

Adding new test to ErrorParserSuite for asserting the new type of errors

Removing now irrelevant test from ErrorParserSuite

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Tweaking the SQL grammar file in the following ways:

1. adding parenthesized query rule and swapping that out in various places that had the same definition
1. adding rule for detecting ""dangling"" (extra left or extra right) parentheses for parenthesized expressions within primary expressions, and subqueries

Also updating the `AstBuilder` in accordance with these changes.  Finally, added some tests for the new unbalanced parentheses `ParseException` instances.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

To help make the error messages more user friendly, and also to allow people to more quickly narrow down on unbalanced parentheses within large, complicated expressions.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

There is a new type of parse error that can be shown, but there is no net behavior change to the SQL parser.

Consider two simple queries.

1. `select (r + 1))`
2. `select ((r + 1)`


Before, the error messages would be the following, respectively.

```
org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input ')' expecting <EOF>(line 1, pos 14)

== SQL ==
select (r + 1)) 
--------------^^^
org.apache.spark.sql.catalyst.parser.ParseException: 
no viable alternative at input '((r + 1) '(line 1, pos 16)

== SQL ==
select ((r + 1) 
----------------^^^
```

After:

```
org.apache.spark.sql.catalyst.parser.ParseException: 
Unbalanced parentheses (extra right)(line 1, pos 14)

== SQL ==
select (r + 1)) 
--------------^^^

org.apache.spark.sql.catalyst.parser.ParseException: 
Unbalanced parentheses (extra left)(line 1, pos 7)

== SQL ==
select ((r + 1) 
-------^^^
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

SQL module `sbt` tests run and confirmed.",spark,apache,jeff303,3521562,MDQ6VXNlcjM1MjE1NjI=,https://avatars0.githubusercontent.com/u/3521562?v=4,,https://api.github.com/users/jeff303,https://github.com/jeff303,https://api.github.com/users/jeff303/followers,https://api.github.com/users/jeff303/following{/other_user},https://api.github.com/users/jeff303/gists{/gist_id},https://api.github.com/users/jeff303/starred{/owner}{/repo},https://api.github.com/users/jeff303/subscriptions,https://api.github.com/users/jeff303/orgs,https://api.github.com/users/jeff303/repos,https://api.github.com/users/jeff303/events{/privacy},https://api.github.com/users/jeff303/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26144,https://github.com/apache/spark/pull/26144,https://github.com/apache/spark/pull/26144.diff,https://github.com/apache/spark/pull/26144.patch
178,https://api.github.com/repos/apache/spark/issues/26141,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26141/labels{/name},https://api.github.com/repos/apache/spark/issues/26141/comments,https://api.github.com/repos/apache/spark/issues/26141/events,https://github.com/apache/spark/pull/26141,507848448,MDExOlB1bGxSZXF1ZXN0MzI4NzU3NTEz,26141,[SPARK-29492][SQL]Reset HiveSession's SessionState conf's ClassLoader when sync mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-10-16T13:21:34Z,2019-11-22T07:02:03Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
When we use pyhive connect to SparkThriftServer, it will run statement in sync mode.
When we query data of hive table , it will check serde class in HiveTableScanExec#addColumnMetadataToConf

https://github.com/apache/spark/blob/5a482e72091c8db940408905e8c044f7f5d7814f/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScanExec.scala#L123

Since we run statement in sync mode, it will use HiveSession's SessionState,  and use it's conf's classLoader. then error happened.
We should reset it when we start run sql in sync mode.
### Why are the changes needed?
Fix bug

### Does this PR introduce any user-facing change?
NO


### How was this patch tested?
UT",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26141,https://github.com/apache/spark/pull/26141,https://github.com/apache/spark/pull/26141.diff,https://github.com/apache/spark/pull/26141.patch
179,https://api.github.com/repos/apache/spark/issues/26118,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26118/labels{/name},https://api.github.com/repos/apache/spark/issues/26118/comments,https://api.github.com/repos/apache/spark/issues/26118/events,https://github.com/apache/spark/pull/26118,506908883,MDExOlB1bGxSZXF1ZXN0MzI4MDAwMTIw,26118,[SPARK-24915][Python] Fix Row handling with Schema.,[],open,False,,[],,20,2019-10-14T22:43:04Z,2019-12-24T09:55:06Z,,NONE,"### What changes were proposed in this pull request?

This change implements a special handling of `pyspark.sql.Row` within the conversion of a `StructType` into an internal SQL object (`toInternal()`) to fix [SPARK-24915](https://issues.apache.org/jira/browse/SPARK-24915).

Previously, `Row` was processed as `tuple` (since it inherits from `tuple`). In particular, it was expected that values come in the ""right"" order. This works if the internal order of the `Row` (sorted by key in the current implementation) corresponds to the order of fields in the schema. If the fields have a different order *and* need special treatment (e.g. `_needConversion` is `True`) then exceptions happened when creating dataframes.

With this change, it will be processed as a `dict` if it has named columns and as tuple otherwise.

## Design

Re `asDict`: I first had an implementation for `Row` as type. However, that implementation would fail for fields that are unknown to the `Row` object, this is inconsistent with the handling of `dict`s. The most consistent implementation is to convert the `Row` to `dict`.

Note: The underlying problem is that `Row` inherits from `tuple`. This is visible in the tests, too. for `assertEqual` the `Row`s `Row(a=..., b=...)` and `Row(b=..., a=...)` are *not* equal because they are compared as lists (and the order is wrong) while a direct comparison returns `True` (For this reason the tests compare based on `asDict`).

### Why are the changes needed?

The code part being changed relies on `Row`s being RDD-style tuples but breaks for `Row`s created with `kwargs`.

This change fixes SPARK-24915, creating data frames from (pyspark.sql.)Rows which failed if the order of fields in the schema differed from the (internal) order of fields in the Row and the schema is ""complicated"".

Complicated can be if one type of the schema is nested (as in the JIRA issue) or one field needs conversion (e.g. `DateType()`)

Without the change, the following examples fail:

*From JIRA issue:*

```
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, Row

conf = SparkConf().setMaster(""local"").setAppName(""repro"") 
context = SparkContext(conf=conf) 
session = SparkSession(context)
schema = StructType([
    StructField('field2', StructType([StructField('sub_field', StringType(), False)]), False),
    StructField('field1', StringType(), False),
])
data = [Row(field1=""Hello"", field2=Row(sub_field='world'))]
df = session.createDataFrame(data, schema=schema) # this will throw a ValueError
df.show()
```

*Date example:*

```
import datetime as dt
from pyspark.sql import Row, SparkSession
from pyspark.sql.types import StringType, DateType, StructField, StructType
spark = SparkSession.builder.master(""local"").getOrCreate()
schema = StructType([
    StructField(""join_date"", DateType(), False),
    StructField(""name"", StringType(),False),
])
rows = [Row(name='Alice', age=23, join_date=dt.datetime(2019,10,1,23,45,56)),]
spark.createDataFrame(rows, schema=schema).collect()
```

### Does this PR introduce any user-facing change?

This change is not introducing User-facing changes for existing, working pyspark code.

Code that previously caused exceptions b/c of the fixed bug will now work (which - technically - is a user-facing change).

### How was this patch tested?

#### Standard Tests

`ARROW_PRE_0_15_IPC_FORMAT=1 ./dev/run-tests` succeeded on my machine

Python: 3.7.4  
Spark: master (`a42d894a4090c97a90ce23b0989163909ebf548d`)  
OS: MacOS 10.14.6. 

#### New Tests

I added the following tests in module `pyspark.sql.tests.test_types`:

- `test_create_dataframe_from_rows_mixed_with_datetype`: schema with date field doesn't cause exception
- `test_create_dataframe_from_rows_with_nested_row`: schema with nested field doesn't cause exception
- `test_create_dataframe_from_tuple_rows`: Regression test: RDD-style `Row`s still work

The latter corresponds to the test case from SPARK_24915.
",spark,apache,qudade,7327644,MDQ6VXNlcjczMjc2NDQ=,https://avatars0.githubusercontent.com/u/7327644?v=4,,https://api.github.com/users/qudade,https://github.com/qudade,https://api.github.com/users/qudade/followers,https://api.github.com/users/qudade/following{/other_user},https://api.github.com/users/qudade/gists{/gist_id},https://api.github.com/users/qudade/starred{/owner}{/repo},https://api.github.com/users/qudade/subscriptions,https://api.github.com/users/qudade/orgs,https://api.github.com/users/qudade/repos,https://api.github.com/users/qudade/events{/privacy},https://api.github.com/users/qudade/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26118,https://github.com/apache/spark/pull/26118,https://github.com/apache/spark/pull/26118.diff,https://github.com/apache/spark/pull/26118.patch
180,https://api.github.com/repos/apache/spark/issues/26102,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26102/labels{/name},https://api.github.com/repos/apache/spark/issues/26102/comments,https://api.github.com/repos/apache/spark/issues/26102/events,https://github.com/apache/spark/pull/26102,506169366,MDExOlB1bGxSZXF1ZXN0MzI3NDUyNDgx,26102,[SPARK-29448][SQL] Support the `INTERVAL` type by Parquet datasource,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2019-10-12T10:38:28Z,2019-12-11T00:11:08Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

In the PR, I propose to support Catalyst's `CalendarIntervalType` in the Parquet datasource. Interval values are saved as parquet `INTERVAL` logical type according to the format specification - https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#interval .

Parquet format allows to store intervals in millisecond precision. Because of this restriction, values of Spark's `INTERVAL` type have to be truncated to milliseconds before storing to parquet files. 

### Why are the changes needed?
- Spark users will be able to load interval columns stored to parquet files in other systems
- Datasets with interval columns can be stored to parquet files for future processing

### Does this PR introduce any user-facing change?
Yes. Before, write to parquet files fails with the error:
```
scala> spark.range(1).selectExpr(""interval 10 year as i"").write.parquet(""intervals"")
org.apache.spark.sql.AnalysisException: Cannot save interval data type into external storage.;
```
After:
```
scala> spark.range(1).selectExpr(""interval 10 year as i"").write.parquet(""intervals"")
scala> spark.read.parquet(""intervals"").show(false)
+-----------------+
|i                |
+-----------------+
|interval 10 years|
+-----------------+
```

### How was this patch tested?
- Add tests to `ParquetSchemaSuite` and `ParquetIOSuite`
- by end-to-end test in `ParquetQuerySuite` which writes intervals and read them back
",spark,apache,MaxGekk,1580697,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26102,https://github.com/apache/spark/pull/26102,https://github.com/apache/spark/pull/26102.diff,https://github.com/apache/spark/pull/26102.patch
181,https://api.github.com/repos/apache/spark/issues/26100,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26100/labels{/name},https://api.github.com/repos/apache/spark/issues/26100/comments,https://api.github.com/repos/apache/spark/issues/26100/events,https://github.com/apache/spark/pull/26100,506140795,MDExOlB1bGxSZXF1ZXN0MzI3NDMyMzM0,26100,[SPARK-28834][SQL] Support sessionInitStatement in dataframe jdbc write path,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-10-12T06:26:01Z,2019-10-12T23:00:21Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
After each database session is opened to the remote DB and before starting to read data, this option `sessionInitStatement`  executes a custom SQL statement to do session initialization, this change enables this behavior for writing data.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Session initialization also need for writing data out of spark

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
it",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26100,https://github.com/apache/spark/pull/26100,https://github.com/apache/spark/pull/26100.diff,https://github.com/apache/spark/pull/26100.patch
182,https://api.github.com/repos/apache/spark/issues/26090,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26090/labels{/name},https://api.github.com/repos/apache/spark/issues/26090/comments,https://api.github.com/repos/apache/spark/issues/26090/events,https://github.com/apache/spark/pull/26090,505774473,MDExOlB1bGxSZXF1ZXN0MzI3MTM2NzE5,26090,[SPARK-29302][CORE][SQL] Fix writing file collision in dynamic partition overwrite mode within speculative execution,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-10-11T10:36:33Z,2019-10-12T22:58:39Z,,NONE,"### What changes were proposed in this pull request?
When inserting into a partitioned DataSource table (would not reproduced if using a Hive table) with dynamic partition overwrite and speculative execution, attempts of same task will try to write same files.

This PR reuse FileOutputCommitter to avoid write collision, and rename files in staging directory to final output directory using the original logic in HadoopMapReduceCommitProtocol#commitJob.


### Why are the changes needed?
Task failed is this circumstance.


### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
This patch is tested by existing tests in org.apache.spark.sql.sources.InsertSuite.
",spark,apache,Clarkkkkk,19434270,MDQ6VXNlcjE5NDM0Mjcw,https://avatars2.githubusercontent.com/u/19434270?v=4,,https://api.github.com/users/Clarkkkkk,https://github.com/Clarkkkkk,https://api.github.com/users/Clarkkkkk/followers,https://api.github.com/users/Clarkkkkk/following{/other_user},https://api.github.com/users/Clarkkkkk/gists{/gist_id},https://api.github.com/users/Clarkkkkk/starred{/owner}{/repo},https://api.github.com/users/Clarkkkkk/subscriptions,https://api.github.com/users/Clarkkkkk/orgs,https://api.github.com/users/Clarkkkkk/repos,https://api.github.com/users/Clarkkkkk/events{/privacy},https://api.github.com/users/Clarkkkkk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26090,https://github.com/apache/spark/pull/26090,https://github.com/apache/spark/pull/26090.diff,https://github.com/apache/spark/pull/26090.patch
183,https://api.github.com/repos/apache/spark/issues/26082,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26082/labels{/name},https://api.github.com/repos/apache/spark/issues/26082/comments,https://api.github.com/repos/apache/spark/issues/26082/events,https://github.com/apache/spark/pull/26082,505380860,MDExOlB1bGxSZXF1ZXN0MzI2ODI1OTQ5,26082,[SPARK-29431][WebUI] Improve Web UI / Sql tab visualization with cached dataframes.,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-10-10T16:29:13Z,2019-11-26T00:09:05Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
With this pull request I want to improve the Web UI / SQL tab visualization. The principal problem that I find is when you have a cache in your plan, the SQL visualization don‚Äôt show any information about the part of the plan that has been cached.

Before the change
![image](https://user-images.githubusercontent.com/12819544/66587418-aa7f6280-eb8a-11e9-80cf-bf10d6c0abea.png)
After the change 
![image](https://user-images.githubusercontent.com/12819544/66587526-ddc1f180-eb8a-11e9-92de-c3b3f5657b66.png)

### Why are the changes needed?
When we have a SQL plan with cached dataframes we lose the graphical information of this dataframe in the sql tab

### Does this PR introduce any user-facing change?
Yes, in the sql tab

### How was this patch tested?
Unit testing and manual tests throught spark shell
",spark,apache,planga82,12819544,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26082,https://github.com/apache/spark/pull/26082,https://github.com/apache/spark/pull/26082.diff,https://github.com/apache/spark/pull/26082.patch
184,https://api.github.com/repos/apache/spark/issues/26076,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26076/labels{/name},https://api.github.com/repos/apache/spark/issues/26076/comments,https://api.github.com/repos/apache/spark/issues/26076/events,https://github.com/apache/spark/pull/26076,504884746,MDExOlB1bGxSZXF1ZXN0MzI2NDQwMzA5,26076,[SPARK-29419][SQL] Fix Encoder thread-safety bug in createDataset(Seq),"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-10-09T20:44:32Z,2019-12-02T15:14:34Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR fixes a thread-safety bug in `SparkSession.createDataset(Seq)`: if the caller-supplied `Encoder` is used in multiple threads then createDataset's usage of the encoder may lead to incorrect / corrupt results because the Encoder's internal mutable state will be updated from multiple threads.

Here is an example demonstrating the problem:

```scala
import org.apache.spark.sql._

val enc = implicitly[Encoder[(Int, Int)]]

val datasets = (1 to 100).par.map { _ =>
  val pairs = (1 to 100).map(x => (x, x))
  spark.createDataset(pairs)(enc)
}

datasets.reduce(_ union _).collect().foreach {
  pair => require(pair._1 == pair._2, s""Pair elements are mismatched: $pair"")
}
```

Before this PR's change, the above example fails because Spark produces corrupted records where different input records' fields have been co-mingled.

This bug is similar to SPARK-22355 / #19577, a similar problem in `Dataset.collect()`.

The fix implemented here is based on #24735's updated version of the `Datataset.collect()` bugfix: use `.copy()`. For consistency, I used same [code comment](https://github.com/apache/spark/blob/d841b33ba3a9b0504597dbccd4b0d11fa810abf3/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3414) / explanation as that PR.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Tested manually using the example listed above.

Thanks to @smcnamara-stripe for identifying this bug.",spark,apache,JoshRosen,50748,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26076,https://github.com/apache/spark/pull/26076,https://github.com/apache/spark/pull/26076.diff,https://github.com/apache/spark/pull/26076.patch
185,https://api.github.com/repos/apache/spark/issues/26044,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26044/labels{/name},https://api.github.com/repos/apache/spark/issues/26044/comments,https://api.github.com/repos/apache/spark/issues/26044/events,https://github.com/apache/spark/pull/26044,503595960,MDExOlB1bGxSZXF1ZXN0MzI1NDE3NjU1,26044,[SPARK-29375][SQL] Exchange reuse across all subquery levels,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2019-10-07T17:45:27Z,2019-12-15T15:13:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This PR:
- enables exchange reuse across all subquery levels
- adds minor optimization by using a map of canonicalized plans to look up reusable exchanges

Example query:
```
SELECT
  (SELECT max(a.key) FROM testData AS a JOIN testData AS b ON b.key = a.key),
  a.key
FROM testData AS a
JOIN testData AS b ON b.key = a.key
```
Plan before this PR:
```
*(5) Project [Subquery scalar-subquery#270, [id=#605] AS scalarsubquery()#277, key#13]
:  +- Subquery scalar-subquery#270, [id=#605]
:     +- *(6) HashAggregate(keys=[], functions=[max(key#13)], output=[max(key)#276])
:        +- Exchange SinglePartition, true, [id=#601]
:           +- *(5) HashAggregate(keys=[], functions=[partial_max(key#13)], output=[max#281])
:              +- *(5) Project [key#13]
:                 +- *(5) SortMergeJoin [key#13], [key#273], Inner
:                    :- *(2) Sort [key#13 ASC NULLS FIRST], false, 0
:                    :  +- Exchange hashpartitioning(key#13, 5), true, [id=#584]
:                    :     +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#13]
:                    :        +- Scan[obj#12]
:                    +- *(4) Sort [key#273 ASC NULLS FIRST], false, 0
:                       +- Exchange hashpartitioning(key#273, 5), true, [id=#592]
:                          +- *(3) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#273]
:                             +- Scan[obj#12]
+- *(5) SortMergeJoin [key#13], [key#271], Inner
   :- *(2) Sort [key#13 ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#617]
   :     +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#13]
   :        +- Scan[obj#12]
   +- *(4) Sort [key#271 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(key#271, 5), true, [id=#625]
         +- *(3) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#271]
            +- Scan[obj#12]
```
Plan after this PR:
```
*(5) Project [Subquery scalar-subquery#240, [id=#193] AS scalarsubquery()#247, key#13]
:  +- Subquery scalar-subquery#240, [id=#193]
:     +- *(6) HashAggregate(keys=[], functions=[max(key#13)], output=[max(key)#246])
:        +- Exchange SinglePartition, true, [id=#189]
:           +- *(5) HashAggregate(keys=[], functions=[partial_max(key#13)], output=[max#251])
:              +- *(5) Project [key#13]
:                 +- *(5) SortMergeJoin [key#13], [key#243], Inner
:                    :- *(2) Sort [key#13 ASC NULLS FIRST], false, 0
:                    :  +- Exchange hashpartitioning(key#13, 5), true, [id=#145]
:                    :     +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#13]
:                    :        +- Scan[obj#12]
:                    +- *(4) Sort [key#243 ASC NULLS FIRST], false, 0
:                       +- ReusedExchange [key#243], Exchange hashpartitioning(key#13, 5), true, [id=#145]
+- *(5) SortMergeJoin [key#13], [key#241], Inner
   :- *(2) Sort [key#13 ASC NULLS FIRST], false, 0
   :  +- ReusedExchange [key#13], Exchange hashpartitioning(key#13, 5), true, [id=#145]
   +- *(4) Sort [key#241 ASC NULLS FIRST], false, 0
      +- ReusedExchange [key#241], Exchange hashpartitioning(key#13, 5), true, [id=#145]
```

### Why are the changes needed?
Performance improvement. 

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Added a new UT.
",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26044,https://github.com/apache/spark/pull/26044,https://github.com/apache/spark/pull/26044.diff,https://github.com/apache/spark/pull/26044.patch
186,https://api.github.com/repos/apache/spark/issues/26016,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26016/labels{/name},https://api.github.com/repos/apache/spark/issues/26016/comments,https://api.github.com/repos/apache/spark/issues/26016/events,https://github.com/apache/spark/pull/26016,502176740,MDExOlB1bGxSZXF1ZXN0MzI0MzA4NDQw,26016,[SPARK-24914][SQL] New statistic to improve data size estimate for columnar storage formats,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2019-10-03T16:44:00Z,2019-12-18T11:53:14Z,,CONTRIBUTOR,"### Why are the changes needed?

Before this change Spark estimated the table size as the sum of all the file sizes. This estimate can be way too low at columnar file formats where huge data can be compressed into a very small file because of serialization (like dictionary encoding) and compression. 

This PR introduces a new statistic called `deserFactor` which is calculated for columnar file formats as a ratio of actual data size (raw data size) to file size which is used for scaling up the file size to improve the estimate of in-memory data size and having a better query optimization (i.e., join strategy decision). This way the OOM error which is the result of a wrongly chosen broadcast join strategy can be avoided.

In case of partitioned table the factors are calculated for each files and the maximum of these factors is taken. Spark stores this factor in the meta store and reuses it so the table can grow without having to recompute this statistic. The stored factor can be removed only by a `TRUNCATE` or a `DROP` table so even a subsequent `ANALYZE TABLE` where the calculation is disabled keeps the old value.

Although this intended to be a generic solution for each columnar file formats this PR currently only focusing on the ORC file format.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

The StatisticsSuite is extended with a new test: `SPARK-24914 - test deserialization factor (ORC)` which checks:
- the factor calculation and application 
- keeping and using the old factor when the calculation is switched off
- calculating it for multiple partitions
- removing the factor at `TRUNCATE`",spark,apache,attilapiros,2017933,MDQ6VXNlcjIwMTc5MzM=,https://avatars1.githubusercontent.com/u/2017933?v=4,,https://api.github.com/users/attilapiros,https://github.com/attilapiros,https://api.github.com/users/attilapiros/followers,https://api.github.com/users/attilapiros/following{/other_user},https://api.github.com/users/attilapiros/gists{/gist_id},https://api.github.com/users/attilapiros/starred{/owner}{/repo},https://api.github.com/users/attilapiros/subscriptions,https://api.github.com/users/attilapiros/orgs,https://api.github.com/users/attilapiros/repos,https://api.github.com/users/attilapiros/events{/privacy},https://api.github.com/users/attilapiros/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26016,https://github.com/apache/spark/pull/26016,https://github.com/apache/spark/pull/26016.diff,https://github.com/apache/spark/pull/26016.patch
187,https://api.github.com/repos/apache/spark/issues/26005,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26005/labels{/name},https://api.github.com/repos/apache/spark/issues/26005/comments,https://api.github.com/repos/apache/spark/issues/26005/events,https://github.com/apache/spark/pull/26005,501743492,MDExOlB1bGxSZXF1ZXN0MzIzOTU4Nzc2,26005,[SPARK-29163][SQL] Simplify Hadoop Configuration access in DataSourcev2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2019-10-02T21:46:07Z,2019-11-14T00:16:33Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This adds a trait that data source v2 users can mix-in to have a the Hadoop configuration serialized and broadcasted.

### Why are the changes needed?

All of our Hadoop Data Source V2 implementations do a variation of this internally. We should reduce the copy pasta code inside of Spark and simplify this for external data source implementations as well.

### Does this PR introduce any user-facing change?

Not for people building Spark applications, but for people building DataSource V2 implementations this standardizes how to access the Hadoop conf.

### How was this patch tested?

Spark's existing V2 data sources are now using this feature, meaning the Hadoop configuration serialization and broadcasting will now be exercised through their tests. Validated by the fact I screwed it up first and they failed.",spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars1.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26005,https://github.com/apache/spark/pull/26005,https://github.com/apache/spark/pull/26005.diff,https://github.com/apache/spark/pull/26005.patch
188,https://api.github.com/repos/apache/spark/issues/26000,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/26000/labels{/name},https://api.github.com/repos/apache/spark/issues/26000/comments,https://api.github.com/repos/apache/spark/issues/26000/events,https://github.com/apache/spark/pull/26000,501516410,MDExOlB1bGxSZXF1ZXN0MzIzNzczODYw,26000,[SPARK-29330][CORE][YARN] Allow users to chose the name of Spark Shuffle service,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-10-02T14:13:30Z,2019-12-06T17:53:59Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR allows users to configure used Spark Shuffle Service name to be able to run vanilla Spark on HDP Hadoop clusters.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

As of now, Spark uses hardcoded value `spark_shuffle` as the name of the Shuffle Service.

HDP distribution of Spark, on the other hand, uses [`spark2_shuffle`](https://github.com/hortonworks/spark2-release/blob/HDP-3.1.0.0-78-tag/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L117). This is done to be able to run both Spark 1.6 and Spark 2.x on the same Hadoop cluster.

Running vanilla Spark on HDP cluster with only Spark 2.x shuffle service (HDP favor) running becomes impossible due to the shuffle service name mismatch.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

The test cases were changed to use the new option.
Patched version of `spark_yarn` library was used to run jobs on HDP cluster.",spark,apache,nonsleepr,2337602,MDQ6VXNlcjIzMzc2MDI=,https://avatars3.githubusercontent.com/u/2337602?v=4,,https://api.github.com/users/nonsleepr,https://github.com/nonsleepr,https://api.github.com/users/nonsleepr/followers,https://api.github.com/users/nonsleepr/following{/other_user},https://api.github.com/users/nonsleepr/gists{/gist_id},https://api.github.com/users/nonsleepr/starred{/owner}{/repo},https://api.github.com/users/nonsleepr/subscriptions,https://api.github.com/users/nonsleepr/orgs,https://api.github.com/users/nonsleepr/repos,https://api.github.com/users/nonsleepr/events{/privacy},https://api.github.com/users/nonsleepr/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/26000,https://github.com/apache/spark/pull/26000,https://github.com/apache/spark/pull/26000.diff,https://github.com/apache/spark/pull/26000.patch
189,https://api.github.com/repos/apache/spark/issues/25987,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25987/labels{/name},https://api.github.com/repos/apache/spark/issues/25987/comments,https://api.github.com/repos/apache/spark/issues/25987/events,https://github.com/apache/spark/pull/25987,500798535,MDExOlB1bGxSZXF1ZXN0MzIzMTkyNzQ0,25987,"[SPARK-29314][SS] Don't overwrite the metric ""updated"" of state operator to 0 if empty batch is run","[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-10-01T10:13:41Z,2019-12-06T17:01:42Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch fixes the behavior of ProgressReporter which always overwrite the value of ""updated"" of state operator to 0 if there's no new data. The behavior is correct only when we copy the state progress from ""previous"" executed plan, meaning no batch has been run. (Nonzero value of ""updated"" would be odd if batch didn't run, so it was correct.) 

It was safe to assume no data is no batch, but SPARK-24156 enables empty data can run the batch if Spark needs to deal with watermark. After the patch, it only overwrites the value if both two conditions are met: 1) no data 2) no batch. 

### Why are the changes needed?

Currently Spark doesn't reflect correct metrics when empty batch is run and this patch fixes it.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Modified UT. Note that FlatMapGroupsWithState increases the value of ""updated"" when state rows are removed.
Also manually tested via below query (not a simple query to test with spark-shell, as you'll meet closure issue in spark-shell while playing with state func):

> query

```
case class RunningCount(count: Long)

object TestFlatMapGroupsWithState {
  def main(args: Array[String]): Unit = {
    import org.apache.spark.sql.SparkSession

    val ss = SparkSession
      .builder()
      .appName(""TestFlatMapGroupsWithState"")
      .getOrCreate()

    ss.conf.set(""spark.sql.shuffle.partitions"", ""5"")

    import ss.implicits._

    val stateFunc = (key: String, values: Iterator[String], state: GroupState[RunningCount]) => {
      if (state.hasTimedOut) {
        // End users are not restricted to remove the state here - they can update the
        // state as well. For example, event time session window would have list of
        // sessions here and it cannot remove entire state.
        state.update(RunningCount(-1))
        Iterator((key, ""-1""))
      } else {
        val count = state.getOption.map(_.count).getOrElse(0L) + values.size
        state.update(RunningCount(count))
        state.setTimeoutDuration(""1 seconds"")
        Iterator((key, count.toString))
      }
    }

    implicit val sqlContext = ss.sqlContext
    val inputData = MemoryStream[String]

    val result = inputData
      .toDF()
      .as[String]
      .groupByKey { v => v }
      .flatMapGroupsWithState(OutputMode.Append(), GroupStateTimeout.ProcessingTimeTimeout())(stateFunc)

    val query = result
      .writeStream
      .format(""memory"")
      .option(""queryName"", ""test"")
      .outputMode(""append"")
      .trigger(Trigger.ProcessingTime(""5 second""))
      .start()

    Thread.sleep(1000)

    var chIdx: Long = 0

    while (true) {
      (chIdx to chIdx + 4).map { idx => inputData.addData(idx.toString) }
      chIdx += 5
      // intentionally sleep much more than trigger to enable ""empty"" batch
      Thread.sleep(10 * 1000)
    }
  }
}
```

> before the patch (batch 3 which was an ""empty"" batch)

```
{
   ""id"":""de945a5c-882b-4dae-aa58-cb8261cbaf9e"",
   ""runId"":""f1eb6d0d-3cd5-48b2-a03b-5e989b6c151b"",
   ""name"":""test"",
   ""timestamp"":""2019-11-18T07:00:25.005Z"",
   ""batchId"":3,
   ""numInputRows"":0,
   ""inputRowsPerSecond"":0.0,
   ""processedRowsPerSecond"":0.0,
   ""durationMs"":{
      ""addBatch"":1664,
      ""getBatch"":0,
      ""latestOffset"":0,
      ""queryPlanning"":29,
      ""triggerExecution"":1789,
      ""walCommit"":51
   },
   ""stateOperators"":[
      {
         ""numRowsTotal"":10,
         ""numRowsUpdated"":0,
         ""memoryUsedBytes"":5130,
         ""customMetrics"":{
            ""loadedMapCacheHitCount"":15,
            ""loadedMapCacheMissCount"":0,
            ""stateOnCurrentVersionSizeBytes"":2722
         }
      }
   ],
   ""sources"":[
      {
         ""description"":""MemoryStream[value#1]"",
         ""startOffset"":9,
         ""endOffset"":9,
         ""numInputRows"":0,
         ""inputRowsPerSecond"":0.0,
         ""processedRowsPerSecond"":0.0
      }
   ],
   ""sink"":{
      ""description"":""MemorySink"",
      ""numOutputRows"":5
   }
}
```

> after the patch (batch 3 which was an ""empty"" batch)

```
{
   ""id"":""7cb41623-6b9a-408e-ae02-6796ec636fa0"",
   ""runId"":""17847710-ddfe-45f5-a7ab-b160e149382f"",
   ""name"":""test"",
   ""timestamp"":""2019-11-18T07:02:25.005Z"",
   ""batchId"":3,
   ""numInputRows"":0,
   ""inputRowsPerSecond"":0.0,
   ""processedRowsPerSecond"":0.0,
   ""durationMs"":{
      ""addBatch"":1196,
      ""getBatch"":0,
      ""latestOffset"":0,
      ""queryPlanning"":30,
      ""triggerExecution"":1333,
      ""walCommit"":46
   },
   ""stateOperators"":[
      {
         ""numRowsTotal"":10,
         ""numRowsUpdated"":5,
         ""memoryUsedBytes"":5130,
         ""customMetrics"":{
            ""loadedMapCacheHitCount"":15,
            ""loadedMapCacheMissCount"":0,
            ""stateOnCurrentVersionSizeBytes"":2722
         }
      }
   ],
   ""sources"":[
      {
         ""description"":""MemoryStream[value#1]"",
         ""startOffset"":9,
         ""endOffset"":9,
         ""numInputRows"":0,
         ""inputRowsPerSecond"":0.0,
         ""processedRowsPerSecond"":0.0
      }
   ],
   ""sink"":{
      ""description"":""MemorySink"",
      ""numOutputRows"":5
   }
}
```

""numRowsUpdated"" is `0` in ""stateOperators"" before applying the patch which is ""wrong"", as we ""update"" the state when timeout occurs. After applying the patch, it correctly represents the ""numRowsUpdated"" as `5` in ""stateOperators"".",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25987,https://github.com/apache/spark/pull/25987,https://github.com/apache/spark/pull/25987.diff,https://github.com/apache/spark/pull/25987.patch
190,https://api.github.com/repos/apache/spark/issues/25977,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25977/labels{/name},https://api.github.com/repos/apache/spark/issues/25977/comments,https://api.github.com/repos/apache/spark/issues/25977/events,https://github.com/apache/spark/pull/25977,500336237,MDExOlB1bGxSZXF1ZXN0MzIyODMwMTg2,25977,[SPARK-29268][SQL]isolationOn value is wrong in case of spark.sql.hive.metastore.jars != builtin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-09-30T15:01:35Z,2019-10-11T21:57:07Z,,CONTRIBUTOR,"
### What changes were proposed in this pull request?
Failed to start **spark-sql** when using **Derby metastore**   and **isolatedLoader** is enabled
i.e. spark.sql.hive.metastore.jars=maven or spark.sql.hive.metastore.jars=classpath

when `spark.sql.hive.metastore.jars != builtin`  `isolationOn` value is true, it should be taken from the `isCliSessionState`


### Why are the changes needed?

`isolationOn`  flag is made same as case when `spark.sql.hive.metastore.jars == builtin` case logic


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Added UT and also tested manually
**case 1 :  spark.sql.hive.metastore.jars = classpath**
![usingClassPath](https://user-images.githubusercontent.com/35216143/65888973-11985c80-e3be-11e9-9d2a-99aaa6981c52.png)

**case 2:  spark.sql.hive.metastore.jars = maven**
![UsingMaven_1](https://user-images.githubusercontent.com/35216143/65889132-558b6180-e3be-11e9-8575-4711918bce1b.png)
![UsingMaven_2](https://user-images.githubusercontent.com/35216143/65889141-591ee880-e3be-11e9-996f-0f72f29042aa.png)

",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25977,https://github.com/apache/spark/pull/25977,https://github.com/apache/spark/pull/25977.diff,https://github.com/apache/spark/pull/25977.patch
191,https://api.github.com/repos/apache/spark/issues/25965,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25965/labels{/name},https://api.github.com/repos/apache/spark/issues/25965/comments,https://api.github.com/repos/apache/spark/issues/25965/events,https://github.com/apache/spark/pull/25965,499934199,MDExOlB1bGxSZXF1ZXN0MzIyNTE2MTcw,25965,[SPARK-26425][SS] Add more constraint checks in file streaming source to avoid checkpoint corruption,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-09-29T15:44:37Z,2019-12-06T12:45:26Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Credits to @tdas who reported and described the fix to [SPARK-26425](https://issues.apache.org/jira/browse/SPARK-26425). I just followed the description of the issue.

This patch adds more checks on file streaming source so that multiple concurrent runs of streaming query don't mess up the status of query/checkpoint. This patch addresses two different spots which are having a bit different issues:

1. HDFSMetadataLog.getLatest()

This is pretty weird to allow reading from non-latest batch metadata and treat it as latest. It only happens when the query succeeds to find the latest batch file from listing but the file is deleted just before reading. It should have treated as critical and end users should be indicated this as it means metadata is being modified from other query (or manually) which is unsafe.

2. FileStreamSource.fetchMaxOffset()

In structured streaming, we don't allow multiple streaming queries to run with same checkpoint (including concurrent runs of same query), so query should fail if it fails to write the metadata of specific batch ID due to same batch ID being written by others.

As described in JIRA issue, assertion is already applied to the `offsetLog` for the same reason.

https://github.com/apache/spark/blob/8167714cab93a5c06c23f92c9077fe8b9677ab28/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L394-L402

### Why are the changes needed?

This prevents the inconsistent behavior on streaming query and lets query fail instead.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

N/A, as the change is simple and obvious, and it's really hard to artificially reproduce the issue.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25965,https://github.com/apache/spark/pull/25965,https://github.com/apache/spark/pull/25965.diff,https://github.com/apache/spark/pull/25965.patch
192,https://api.github.com/repos/apache/spark/issues/25963,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25963/labels{/name},https://api.github.com/repos/apache/spark/issues/25963/comments,https://api.github.com/repos/apache/spark/issues/25963/events,https://github.com/apache/spark/pull/25963,499870850,MDExOlB1bGxSZXF1ZXN0MzIyNDc5MTM4,25963,[SPARK-28137][SQL] Add Postgresql function to_number.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-09-29T07:14:02Z,2019-11-22T02:08:23Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
`Postgresql` and `Oracle` have the function `to_number` to convert a string to number. 
The implement and support syntax has many different between `Postgresql` and `Oracle`. So, this PR mainly follows the implement of `to_number` in `Postgresql`.

This PR references the doc https://www.postgresql.org/docs/12/functions-formatting.html
Oracle doc is https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TO_NUMBER.html#GUID-D4807212-AFD7-48A7-9AED-BEC3E8809866

The syntax like:
> select to_number('12,454.8-', '99G999D9S');
-12454.8

### Why are the changes needed?
This PR adds a new function.


### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
New UT.
",spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25963,https://github.com/apache/spark/pull/25963,https://github.com/apache/spark/pull/25963.diff,https://github.com/apache/spark/pull/25963.patch
193,https://api.github.com/repos/apache/spark/issues/25944,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25944/labels{/name},https://api.github.com/repos/apache/spark/issues/25944/comments,https://api.github.com/repos/apache/spark/issues/25944/events,https://github.com/apache/spark/pull/25944,498962445,MDExOlB1bGxSZXF1ZXN0MzIxNzgyNTQw,25944,[SPARK-29254][SQL] Failed to include jars passed in through --jars when isolatedLoader is enabled,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-09-26T15:32:55Z,2019-09-29T04:06:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Failed to include jars passed in through --jars when isolatedLoader is enabled(spark.sql.hive.metastore.jars != builtin). How to reproduce:
```scala
  test(""SPARK-29254: include jars passed in through --jars when isolatedLoader is enabled"") {
    def getJarFromUrl(urlString: String): File = {
      val fileName = urlString.split(""/"").last
      Utils.doFetchFile(urlString, Utils.createTempDir(), fileName, new SparkConf, null, null)
    }
    val repository = ""https://repository.apache.org/content/repositories/releases/""
    val unusedJar = TestUtils.createJarWithClasses(Seq.empty)
    val jar1 = TestUtils.createJarWithClasses(Seq(""SparkSubmitClassA""))
    val jar2 = TestUtils.createJarWithClasses(Seq(""SparkSubmitClassB""))
    // download Hive 2.1.1, a non builtinHiveVersion(1.2.1) version for testing
    val jar3 = getJarFromUrl(s""${repository}org/apache/hive/hive-contrib/"" +
      s""2.1.1/hive-contrib-2.1.1.jar"").getCanonicalPath
    val jar4 = getJarFromUrl(s""${repository}org/apache/hive/hcatalog/hive-hcatalog-core/"" +
      s""2.1.1/hive-hcatalog-core-2.1.1.jar"").getCanonicalPath
    val jarsString = Seq(jar1, jar2, jar3, jar4).map(j => j.toString).mkString("","")
    val args = Seq(
      ""--class"", SparkSubmitClassLoaderTest.getClass.getName.stripSuffix(""$""),
      ""--name"", ""SparkSubmitClassLoaderTest"",
      ""--master"", ""local-cluster[2,1,1024]"",
      ""--conf"", ""spark.ui.enabled=false"",
      ""--conf"", ""spark.master.rest.enabled=false"",
      ""--conf"", ""spark.sql.hive.metastore.version=2.1.1"",
      ""--conf"", ""spark.sql.hive.metastore.jars=maven"",
      ""--driver-java-options"", ""-Dderby.system.durability=test"",
      ""--jars"", jarsString,
      unusedJar.toString, ""SparkSubmitClassA"", ""SparkSubmitClassB"")
    runSparkSubmit(args)
  }
```
It will get ClassNotFoundException:
```shell
2019-09-25 22:11:42.854 - stderr> 19/09/25 22:11:42 ERROR log: error in initSerDe: java.lang.ClassNotFoundException Class org.apache.hive.hcatalog.data.JsonSerDe not found
2019-09-25 22:11:42.854 - stderr> java.lang.ClassNotFoundException: Class org.apache.hive.hcatalog.data.JsonSerDe not found
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:84)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:77)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:289)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:271)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:663)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:646)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:898)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:937)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:539)
2019-09-25 22:11:42.854 - stderr> 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:311)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:245)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:244)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:294)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:537)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:284)
2019-09-25 22:11:42.854 - stderr> 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:242)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:325)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:132)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
2019-09-25 22:11:42.854 - stderr> 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:225)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3372)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3368)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:225)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:87)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.hive.test.TestHiveSparkSession.sql(TestHive.scala:238)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:550)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.hive.SparkSubmitClassLoaderTest$.main(HiveSparkSubmitSuite.scala:638)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.sql.hive.SparkSubmitClassLoaderTest.main(HiveSparkSubmitSuite.scala)
2019-09-25 22:11:42.855 - stderr> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2019-09-25 22:11:42.855 - stderr> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2019-09-25 22:11:42.855 - stderr> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2019-09-25 22:11:42.855 - stderr> 	at java.lang.reflect.Method.invoke(Method.java:498)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:179)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:202)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:89)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:980)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:989)
2019-09-25 22:11:42.855 - stderr> 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

### Why are the changes needed?
Fix but.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Add an UT
",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25944,https://github.com/apache/spark/pull/25944,https://github.com/apache/spark/pull/25944.diff,https://github.com/apache/spark/pull/25944.patch
194,https://api.github.com/repos/apache/spark/issues/25943,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25943/labels{/name},https://api.github.com/repos/apache/spark/issues/25943/comments,https://api.github.com/repos/apache/spark/issues/25943/events,https://github.com/apache/spark/pull/25943,498862923,MDExOlB1bGxSZXF1ZXN0MzIxNzAxNjc3,25943,[WIP][SPARK-29261][SQL][CORE] Support recover live entities from KVStore for (SQL)AppStatusListener,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2019-09-26T12:48:04Z,2019-11-06T22:10:03Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR enables SQLAppStatusListener and AppStatusListener to recover
live entities from KVStore(InMemoryStore or LevelDB). And it mainly adds
a method named `recoverLiveEntities()`, which does the recover work, in 
(SQL)AppStatusListener. And it  adds a series of `toLiveXXX()` methods for KVStore
object types, in order to convert a KVStore object into a live entity.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

SPARK-28594 and SPARK-28867 are both planed to do incremental replay
in HistoryServer basing on snapshotting mechanism. And when a KVStore
is restored from a snapshot, we need to recover those still running objects
(e.g. jobs, stages, tasks) into live entities. Then, we could continue to replay
with the following events. 

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Added tests in `AppStatusListenerSuite` and `SQLAppStatusListenerSuite`",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25943,https://github.com/apache/spark/pull/25943,https://github.com/apache/spark/pull/25943.diff,https://github.com/apache/spark/pull/25943.patch
195,https://api.github.com/repos/apache/spark/issues/25919,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25919/labels{/name},https://api.github.com/repos/apache/spark/issues/25919/comments,https://api.github.com/repos/apache/spark/issues/25919/events,https://github.com/apache/spark/pull/25919,497764986,MDExOlB1bGxSZXF1ZXN0MzIwODMyNzI4,25919,[SPARK-15616][SQL] Hive table supports partition pruning in JoinSelection,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2019-09-24T15:31:53Z,2019-12-02T09:12:27Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
A new optimizer strategy called `PruneHiveTablePartitions` is added, which calculates table size as the total size of pruned partitions. Thus, Spark planner can pick up `BroadcastJoin` if the size of pruned partitions is under broadcast join threshold.

### Why are the changes needed?
This is a performance improvement.


### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Added unit tests.

This is based on #18193, credits should go to @lianhuiwang.",spark,apache,advancedxy,807537,MDQ6VXNlcjgwNzUzNw==,https://avatars3.githubusercontent.com/u/807537?v=4,,https://api.github.com/users/advancedxy,https://github.com/advancedxy,https://api.github.com/users/advancedxy/followers,https://api.github.com/users/advancedxy/following{/other_user},https://api.github.com/users/advancedxy/gists{/gist_id},https://api.github.com/users/advancedxy/starred{/owner}{/repo},https://api.github.com/users/advancedxy/subscriptions,https://api.github.com/users/advancedxy/orgs,https://api.github.com/users/advancedxy/repos,https://api.github.com/users/advancedxy/events{/privacy},https://api.github.com/users/advancedxy/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25919,https://github.com/apache/spark/pull/25919,https://github.com/apache/spark/pull/25919.diff,https://github.com/apache/spark/pull/25919.patch
196,https://api.github.com/repos/apache/spark/issues/25917,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25917/labels{/name},https://api.github.com/repos/apache/spark/issues/25917/comments,https://api.github.com/repos/apache/spark/issues/25917/events,https://github.com/apache/spark/pull/25917,497707285,MDExOlB1bGxSZXF1ZXN0MzIwNzg1MDYy,25917,[SPARK-29225][SQL] Change Spark SQL `DESC FORMATTED` format same with Hive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-09-24T13:58:28Z,2019-10-25T09:16:47Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
As I have mentioned in [SPARK-29225](https://issues.apache.org/jira/browse/SPARK-29225)
Spark SQL `DESC FORMATTED table_name` format have so many different point form hive's format.
Since some JDBC query interaction platform like `HUE` use  Hive format to parse information form the query result.
Such as column information: 
Hue use `DESC FORMATTED table` query to get result and parse column information by it's result format.  
Spark SQL show different format, it cause problem when show result. 



### Why are the changes needed?
For better interaction with JDBC query platform like Hue. 



### Does this PR introduce any user-facing change?
Create table like below:
```
create table order_partition(
    id string,
    name string,
    num int,
    order_number string, 
   event_time string)
PARTITIONED BY(event_month string)
```

Then call `DESC FORMATTED order_partition` .

Origin Format:
![image](https://user-images.githubusercontent.com/46485123/65567346-77cd4b80-df88-11e9-805f-d2ec05c6272c.png)

Current format:
![image](https://user-images.githubusercontent.com/46485123/65567359-84ea3a80-df88-11e9-966d-46254af14707.png)

**Changes**
When call `DESC FORMATTED table_name`

1. Add header before no-partition columns
2. Add empty line between header and columns
3. Add empty line between normal column and `#Partition information`
4. Remove partition columns in normal column information, only show below `#Partition information`

### How was this patch tested?
MT",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25917,https://github.com/apache/spark/pull/25917,https://github.com/apache/spark/pull/25917.diff,https://github.com/apache/spark/pull/25917.patch
197,https://api.github.com/repos/apache/spark/issues/25911,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25911/labels{/name},https://api.github.com/repos/apache/spark/issues/25911/comments,https://api.github.com/repos/apache/spark/issues/25911/events,https://github.com/apache/spark/pull/25911,497502617,MDExOlB1bGxSZXF1ZXN0MzIwNjIwMjIw,25911,[SPARK-29223][SQL][SS] Enable global timestamp per topic while specifying offset by timestamp in Kafka source,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-09-24T07:14:49Z,2019-10-01T20:56:03Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch is a follow-up of SPARK-26848 (#23747).  In SPARK-26848, we decided to open possibility to let end users set individual timestamp per partition. But in many cases, specifying timestamp represents the intention that we would want to go back to specific timestamp and reprocess records, which should be applied to all topics and partitions.

This patch proposes to provide a way to set a global timestamp across partitions for a topic, so that end users can set all offsets by specific timestamp easily. 

The patch doesn't provide a way to set global timestamp across topics, as it would require modification of format of `startingOffsetsByTimestamp`/`endingOffsetsByTimestamp`, which may not be intuitive to understand.

### Why are the changes needed?

This would helps end users to set timestamp for reprocessing from specific point of time much easier. It also remedies the requirement of knowing number of partitions to set offsets.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UTs to verify the new feature.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25911,https://github.com/apache/spark/pull/25911,https://github.com/apache/spark/pull/25911.diff,https://github.com/apache/spark/pull/25911.patch
198,https://api.github.com/repos/apache/spark/issues/25907,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25907/labels{/name},https://api.github.com/repos/apache/spark/issues/25907/comments,https://api.github.com/repos/apache/spark/issues/25907/events,https://github.com/apache/spark/pull/25907,497348776,MDExOlB1bGxSZXF1ZXN0MzIwNDk4MDg4,25907,[SPARK-29206][SHUFFLE] Make number of shuffle server threads a multiple of number of chunk fetch handler threads.,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-09-23T21:56:32Z,2019-10-04T20:02:59Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
We propose to change the configuration of Netty server threads and chunk fetch handler threads to make sure the former is always a multiple of the latter. This change is necessary to make sure the RPC timeout issues can be fully resolved with the dedicated chunk fetch handler EventLoopGroup. SPARK-29206 has more details on the explanations behind this change.

### How was this patch tested?
Verified that after configuring the number of threads for both thread pools appropriately, we no longer see the RPC timeout issues when shuffle service gets really busy.
A custom Spark shuffle service stress testing suite was used for this purpose.",spark,apache,Victsm,4633527,MDQ6VXNlcjQ2MzM1Mjc=,https://avatars0.githubusercontent.com/u/4633527?v=4,,https://api.github.com/users/Victsm,https://github.com/Victsm,https://api.github.com/users/Victsm/followers,https://api.github.com/users/Victsm/following{/other_user},https://api.github.com/users/Victsm/gists{/gist_id},https://api.github.com/users/Victsm/starred{/owner}{/repo},https://api.github.com/users/Victsm/subscriptions,https://api.github.com/users/Victsm/orgs,https://api.github.com/users/Victsm/repos,https://api.github.com/users/Victsm/events{/privacy},https://api.github.com/users/Victsm/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25907,https://github.com/apache/spark/pull/25907,https://github.com/apache/spark/pull/25907.diff,https://github.com/apache/spark/pull/25907.patch
199,https://api.github.com/repos/apache/spark/issues/25899,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25899/labels{/name},https://api.github.com/repos/apache/spark/issues/25899/comments,https://api.github.com/repos/apache/spark/issues/25899/events,https://github.com/apache/spark/pull/25899,496949394,MDExOlB1bGxSZXF1ZXN0MzIwMTczNTQ4,25899,[SPARK-29089][SQL] Parallelize blocking FileSystem calls in DataSource#checkAndGlobPathIfNecessary,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2019-09-23T08:05:56Z,2019-12-11T15:50:44Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
See JIRA: https://issues.apache.org/jira/browse/SPARK-29089
Mailing List: http://apache-spark-developers-list.1001551.n3.nabble.com/DataFrameReader-bottleneck-in-DataSource-checkAndGlobPathIfNecessary-when-reading-S3-files-td27828.html

When using DataFrameReader#csv to read many files on S3, globbing and fs.exists on DataSource#checkAndGlobPathIfNecessary becomes a bottleneck.

From the mailing list discussions, an improvement that can be made is to parallelize the blocking FS calls:

> - have SparkHadoopUtils differentiate between files returned by globStatus(), and which therefore exist, and those which it didn't glob for -it will only need to check those. 
> - add parallel execution to the glob and existence checks

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Verifying/globbing files happens on the driver, and if this operations take a long time (for example against S3), then the entire cluster has to wait, potentially sitting idle. This change hopes to make this process faster.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

I added a test suite `DataSourceSuite` - open to suggestions for better naming.

See [here](https://github.com/apache/spark/pull/25899#issuecomment-534380034) and [here](https://github.com/apache/spark/pull/25899#issuecomment-534069194) for some measurements",spark,apache,cozos,2646862,MDQ6VXNlcjI2NDY4NjI=,https://avatars0.githubusercontent.com/u/2646862?v=4,,https://api.github.com/users/cozos,https://github.com/cozos,https://api.github.com/users/cozos/followers,https://api.github.com/users/cozos/following{/other_user},https://api.github.com/users/cozos/gists{/gist_id},https://api.github.com/users/cozos/starred{/owner}{/repo},https://api.github.com/users/cozos/subscriptions,https://api.github.com/users/cozos/orgs,https://api.github.com/users/cozos/repos,https://api.github.com/users/cozos/events{/privacy},https://api.github.com/users/cozos/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25899,https://github.com/apache/spark/pull/25899,https://github.com/apache/spark/pull/25899.diff,https://github.com/apache/spark/pull/25899.patch
200,https://api.github.com/repos/apache/spark/issues/25870,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25870/labels{/name},https://api.github.com/repos/apache/spark/issues/25870/comments,https://api.github.com/repos/apache/spark/issues/25870/events,https://github.com/apache/spark/pull/25870,496417878,MDExOlB1bGxSZXF1ZXN0MzE5Nzc1MTUz,25870,[SPARK-27936][K8S] Support python deps,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2019-09-20T15:33:03Z,2019-11-08T20:45:16Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Supports python client deps from the launcher fs.
### Why are the changes needed?
This is a feature that was added for java deps. This PR adds support fo rpythona s well.

### Does this PR introduce any user-facing change?
yes

### How was this patch tested?
Manually running different scenarios and via examining the driver & executors logs. Also there is an integration test added.
I verified that the python resources are added to the spark file server and they are named properly so they dont fail the executors. Note here that as previously the following will not work:
primary resource `A.py`: uses a closure defined in submited pyfile `B.py`, context.py only adds to the pythonpath files with certain extension eg. zip, egg, jar.",spark,apache,skonto,7945591,MDQ6VXNlcjc5NDU1OTE=,https://avatars1.githubusercontent.com/u/7945591?v=4,,https://api.github.com/users/skonto,https://github.com/skonto,https://api.github.com/users/skonto/followers,https://api.github.com/users/skonto/following{/other_user},https://api.github.com/users/skonto/gists{/gist_id},https://api.github.com/users/skonto/starred{/owner}{/repo},https://api.github.com/users/skonto/subscriptions,https://api.github.com/users/skonto/orgs,https://api.github.com/users/skonto/repos,https://api.github.com/users/skonto/events{/privacy},https://api.github.com/users/skonto/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25870,https://github.com/apache/spark/pull/25870,https://github.com/apache/spark/pull/25870.diff,https://github.com/apache/spark/pull/25870.patch
201,https://api.github.com/repos/apache/spark/issues/25863,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25863/labels{/name},https://api.github.com/repos/apache/spark/issues/25863/comments,https://api.github.com/repos/apache/spark/issues/25863/events,https://github.com/apache/spark/pull/25863,496119447,MDExOlB1bGxSZXF1ZXN0MzE5NTM2NDk3,25863,[SPARK-28945][SPARK-29037][CORE][SQL] Fix the issue that spark gives duplicate result and support concurrent file source write operations write to different partitions in the same table.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,67,2019-09-20T03:05:42Z,2019-12-09T19:09:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

For InsertIntoHadoopFsRelation operations.

Case A:
Application appA insert overwrite table table_a with static partition overwrite.
But it was killed when committing tasks, because one task is hang.
And parts of its committed tasks output is kept under /path/table_a/_temporary/0/.

Then we rerun appA. It will reuse the staging dir  /path/table_a/_temporary/0/.
It executes successfully.
But it also commit the data reminded by killed application to destination dir.



Case B:

Application appA insert overwrite table table_a.

Application appB insert overwrite table table_a, too.

They execute concurrently, and they may all use `/path/table_a/_temporary/0/` as workPath.

And their result may be corruptted.



In this PR, we set a staging output path for InsertIntoHadoopFsRelation operation.

The output path is a multi level path and is composed of specified partition key-value pairs formatted `.spark-staging-${depth}/p1=v1/p2=v2/.../pn=vn`.

We can detect the conflict by checking the existence of relative staging output path.

For example:

table_a(c1 string, a string, b string,...) partitioned by (a, b , ...)

When writing to partition `a=1`, we need to check the existence of

1. .spark-staging-0
2. .spark-staging-1/a=1
3. .spark-staging-2/a=1
   ...
4. .spark-staging-numPartitions/a=1

When we found relative staging output path is existed, an exception would be thrown.

Thanks @advancedxy for providing the prototype of solution and valuable suggestions.

### Why are the changes needed?

Data may be corrupted without this PR.

### Does this PR introduce any user-facing change?

User can see the exists of staging output path and may need clean up these path manually, when this path is belong to a killed application and has not been cleaned up gracefully.

### How was this patch tested?

Added unit test.",spark,apache,turboFei,6757692,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25863,https://github.com/apache/spark/pull/25863,https://github.com/apache/spark/pull/25863.diff,https://github.com/apache/spark/pull/25863.patch
202,https://api.github.com/repos/apache/spark/issues/25844,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25844/labels{/name},https://api.github.com/repos/apache/spark/issues/25844/comments,https://api.github.com/repos/apache/spark/issues/25844/events,https://github.com/apache/spark/pull/25844,495556685,MDExOlB1bGxSZXF1ZXN0MzE5MDgxMTg1,25844,[SPARK-29167][SQL] Make Metrics of Analyzer/Optimizer use Scientific counting  human readable,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-09-19T03:44:44Z,2019-09-28T05:05:05Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Current metrics of Analyzer/Optimizer use Scientific counting is not human readable.
![image](https://user-images.githubusercontent.com/46485123/65211874-c5524000-dad2-11e9-9f53-a85a50fe84ec.png)

change it to a human readable format

### Why are the changes needed?
Make metrics more human readable


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Manuel Test
![image](https://user-images.githubusercontent.com/46485123/65212250-0008a800-dad4-11e9-8c7f-6e807fe3a0a9.png)
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25844,https://github.com/apache/spark/pull/25844,https://github.com/apache/spark/pull/25844.diff,https://github.com/apache/spark/pull/25844.patch
203,https://api.github.com/repos/apache/spark/issues/25840,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25840/labels{/name},https://api.github.com/repos/apache/spark/issues/25840/comments,https://api.github.com/repos/apache/spark/issues/25840/events,https://github.com/apache/spark/pull/25840,495526860,MDExOlB1bGxSZXF1ZXN0MzE5MDU4NjI5,25840,[SPARK-29166][SQL] Add parameters to limit the number of dynamic partitions for data source table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2019-09-19T01:30:24Z,2019-12-09T09:30:15Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Dynamic partition in Hive table has some restrictions like limitation of the max number of partitions. 

¬†Configuration | ¬†Default | ¬†Note
-- | -- | --
¬†hive.exec.max.dynamic.partitions.pernode | 100¬† | ¬†Maximum number of dynamic partitions allowed to be created in each mapper/reducer node
¬†hive.exec.max.dynamic.partitions | 1000 | ¬†Maximum number of dynamic partitions allowed to be created in total
hive.exec.max.created.files | 100000 | Maximum number of HDFS files created by all mappers/reducers in a MapReduce job

Ref [DynamicPartitionInserts](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-DynamicPartitionInserts) and [Tutorial-Dynamic-PartitionInsert](https://cwiki.apache.org/confluence/display/Hive/Tutorial#Tutorial-Dynamic-PartitionInsert)

It's very useful to prevent to create mistake partitions like ID. Also it can protect the NameNode from mass RPC calls of creating.

Data source table also needs similar limitations.

### Why are the changes needed?
Add a parameter to limit the number of dynamic partitions for data source table.
By default, the max number of partitions is Int.MaxValue. It's nearly no limitation.
When the parameter value we set is reached, it will throw SparkException and abort the job.

¬†Configuration | ¬†Default | ¬†Note
-- | -- | --
¬†spark.sql.dynamic.partition.maxPartitionsPerTask | Int.MaxValue¬† | ¬†Maximum number of dynamic partitions allowed to be created in each task
¬†spark.sql.dynamic.partition.maxPartitions | Int.MaxValue | ¬†Maximum total number of dynamic partitions allowed to be created by one DML
spark.sql.dynamic.partition.maxCreatedFiles | Int.MaxValue | Maximum total number of files allowed to be created in dynamic partitions write by one DML

### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Add a unit test.",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25840,https://github.com/apache/spark/pull/25840,https://github.com/apache/spark/pull/25840.diff,https://github.com/apache/spark/pull/25840.patch
204,https://api.github.com/repos/apache/spark/issues/25827,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25827/labels{/name},https://api.github.com/repos/apache/spark/issues/25827/comments,https://api.github.com/repos/apache/spark/issues/25827/events,https://github.com/apache/spark/pull/25827,494990695,MDExOlB1bGxSZXF1ZXN0MzE4NjI5MjMx,25827,[SPARK-29128][SQL] Split predicate code in OR expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2019-09-18T05:05:22Z,2019-12-13T17:04:04Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pr is to split predicate code in OR expressions. When I checked if method bytecode size in `BenchmarkQueryTest` went over the OpenJDK default limit (8000) or not in #25788, I found [TPCDSQuerySuite.modified-q3](https://github.com/apache/spark/blob/master/sql/core/src/test/resources/tpcds-modifiedQueries/q3.sql) had too big functions. That's because too long OR chains in the query generate too long code in a single function;

`modified-q3` generates [the code](https://gist.github.com/maropu/9bfdcf9f8b694ad68ad6b3dc67fddb7c#file-non-split-case-in-spark-xxxxx-tpcdsquerysuite-modified-q3) below in the master 
```
== Subtree 2 / 4 (maxMethodCodeSize:12497; maxConstantPoolSize:732(1.12% used); numInnerClasses:1) ==
                                    ^^^^^^
*(3) HashAggregate(keys=[d_year#9, i_brand#62, i_brand_id#61], functions=[partial_sum(UnscaledValue(ss_net_profit#53))], output=[d_year#9, i_brand#62, i_brand_id#61, sum#85L])
+- *(3) Project [d_year#9, ss_net_profit#53, i_brand_id#61, i_brand#62]
....
/* 365 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 366 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 367 */       columnartorow_nextBatch_0();
/* 368 */     }
/* 369 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 370 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 371 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 372 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 373 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 374 */         do {
/* 375 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 376 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 377 */
/* 378 */           boolean filter_value_2 = !columnartorow_isNull_0;
/* 379 */           if (!filter_value_2) continue;
/* 380 */
/* 381 */           boolean filter_value_12 = false;
/* 382 */           filter_value_12 = columnartorow_value_0 >= 2415355;
/* 383 */           boolean filter_value_11 = false;
/* 384 */
.....
too long code
```

This pr split the predicate code into [small functions](https://gist.github.com/maropu/f2f8dba8fe74b50fc0b8ba73ecfbb5d2) below;
```
== Subtree 2 / 4 (maxMethodCodeSize:688; maxConstantPoolSize:949(1.45% used); numInnerClasses:1) ==
                                    ^^^^
*(3) HashAggregate(keys=[d_year#9, i_brand#62, i_brand_id#61], functions=[partial_sum(UnscaledValue(ss_net_profit#53))], output=[d_year#9, i_brand#62, i_brand_id#61, sum#85L])
+- *(3) Project [d_year#9, ss_net_profit#53, i_brand_id#61, i_brand#62]
...
/* 3285 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 3286 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 3287 */       columnartorow_nextBatch_0();
/* 3288 */     }
/* 3289 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 3290 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 3291 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 3292 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 3293 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 3294 */         do {
/* 3295 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 3296 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 3297 */
/* 3298 */           boolean filter_value_2 = !columnartorow_isNull_0;
/* 3299 */           if (!filter_value_2) continue;
/* 3300 */
/* 3301 */           boolean filter_value_213 = filter_or_8(columnartorow_value_0, columnartorow_isNull_0);
/* 3302 */           boolean filter_value_5 = true;
/* 3303 */
/* 3304 */           if (!filter_value_213) {
/* 3305 */             boolean filter_value_421 = filter_or_17(columnartorow_value_0, columnartorow_isNull_0);
/* 3306 */             filter_value_5 = filter_value_421;
/* 3307 */           }
/* 3308 */           boolean filter_value_4 = true;
/* 3309 */
/* 3310 */           if (!filter_value_5) {
/* 3311 */             boolean filter_value_630 = filter_or_26(columnartorow_value_0, columnartorow_isNull_0);
/* 3312 */             boolean filter_value_422 = true;
/* 3313 */
/* 3314 */             if (!filter_value_630) {
/* 3315 */               boolean filter_value_838 = filter_or_35(columnartorow_value_0, columnartorow_isNull_0);
/* 3316 */               filter_value_422 = filter_value_838;
/* 3317 */             }
/* 3318 */             filter_value_4 = filter_value_422;
/* 3319 */           }
/* 3320 */           boolean filter_value_3 = true;
/* 3321 */
/* 3322 */           if (!filter_value_4) {
/* 3323 */             boolean filter_value_1048 = filter_or_44(columnartorow_value_0, columnartorow_isNull_0);
/* 3324 */             boolean filter_value_840 = true;
/* 3325 */
/* 3326 */             if (!filter_value_1048) {
/* 3327 */               boolean filter_value_1256 = filter_or_53(columnartorow_value_0, columnartorow_isNull_0);
/* 3328 */               filter_value_840 = filter_value_1256;
/* 3329 */             }
/* 3330 */             boolean filter_value_839 = true;
/* 3331 */
/* 3332 */             if (!filter_value_840) {
/* 3333 */               boolean filter_value_1465 = filter_or_62(columnartorow_value_0, columnartorow_isNull_0);
/* 3334 */               boolean filter_value_1257 = true;
/* 3335 */
/* 3336 */               if (!filter_value_1465) {
/* 3337 */                 boolean filter_value_1673 = filter_or_71(columnartorow_value_0, columnartorow_isNull_0);
/* 3338 */                 filter_value_1257 = filter_value_1673;
/* 3339 */               }
/* 3340 */               filter_value_839 = filter_value_1257;
/* 3341 */             }
/* 3342 */             filter_value_3 = filter_value_839;
/* 3343 */           }
/* 3344 */           if (!filter_value_3) continue;
/* 3345 */
/* 3346 */           boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 3347 */           int columnartorow_value_1 = columnartorow_isNull_1 ? -1 : (columnartorow_mutableStateArray_2[1].getInt(columnartorow_rowIdx_0));
/* 3348 */
/* 3349 */           boolean filter_value_1676 = !columnartorow_isNull_1;
/* 3350 */           if (!filter_value_1676) continue;
/* 3351 */
```

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For better generated code.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing unit tests.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25827,https://github.com/apache/spark/pull/25827,https://github.com/apache/spark/pull/25827.diff,https://github.com/apache/spark/pull/25827.patch
205,https://api.github.com/repos/apache/spark/issues/25811,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25811/labels{/name},https://api.github.com/repos/apache/spark/issues/25811/comments,https://api.github.com/repos/apache/spark/issues/25811/events,https://github.com/apache/spark/pull/25811,494382074,MDExOlB1bGxSZXF1ZXN0MzE4MTQ1MDY2,25811,[SPARK-29111][CORE] Support snapshot/restore on KVStore,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2019-09-17T03:35:00Z,2019-12-08T13:47:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This patch proposes adding new feature to KVStore, snapshot (dump) & restore. The structure of snapshot file is also described to the design doc, but it's alike the structure of delta/snapshot file in HDFSBackedStateStoreProvider.

Here's the format of snapshot file. The file is in binary format, and follows the way how DataOutputStream writes the types of ‚Äúint‚Äù and ‚Äúbyte[]‚Äù, hence it follows ‚Äúbig-endian‚Äù when writing ‚Äúint‚Äù type of value.

* write(int) : https://github.com/jdk-mirror/openjdk8/blob/d15d915ceb0f6a5bfab067a23a9e4a674dac3da8/java/io/DataOutputStream.java#L187-L202
* write(byte[]) : https://github.com/jdk-mirror/openjdk8/blob/master/java/io/FilterOutputStream.java#L80-L127

The file will start with metadata information, as KVStore has at most one metadata object being stored.

> the length of metadata class name (int) | metadata class name (byte[]) | the length of serialized metadata object (int) | serialized metadata object (byte[]) | -2 (int)

If there's no metadata stored in KVStore, the length of metadata class name would be -2, which denotes following fields will not be presented.

And the file will contain normal objects for each type which format is below:

> the length of class name (int) | class name (byte[]) | the length of `#1` serialized object (int) | `#1` serialized object (byte[]) | ... (`#2`, `#3`, ...) | -2 (int)

Above format will be repeated for each type, and when no object is left for dumping, we simply put -1 in the place where the length of class name is expected to mark the end of file.

### Why are the changes needed?

The new feature will be used as a building block for [SPARK-28870](https://issues.apache.org/jira/browse/SPARK-28870). The patch is intended to be separate with the issue, as I would like to make each PR smaller.

### Does this PR introduce any user-facing change?

No, as KVStore interface is defined as `@Private`.

### How was this patch tested?

Added new UTs.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25811,https://github.com/apache/spark/pull/25811,https://github.com/apache/spark/pull/25811.diff,https://github.com/apache/spark/pull/25811.patch
206,https://api.github.com/repos/apache/spark/issues/25795,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25795/labels{/name},https://api.github.com/repos/apache/spark/issues/25795/comments,https://api.github.com/repos/apache/spark/issues/25795/events,https://github.com/apache/spark/pull/25795,493702533,MDExOlB1bGxSZXF1ZXN0MzE3NjE0NDI5,25795,[WIP][SPARK-29037][Core] Spark gives duplicate result when an application was killed,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2019-09-15T06:46:01Z,2019-09-26T13:05:15Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Case:
Application **appA** insert overwrite table **table_a** with static partition overwrite.
But it was killed when committing tasks,  because one task is hang.
And parts of its committed tasks output is kept under `/path/table_a/_temporary/0/`.

Then we run application **appB** insert overwrite table **table_a** with dynamic partition overwrite.
It executes successfully.
But it also commit the data under `/path/table_a/_temporary/0/` to destination dir.

In this PR, we skip the FileOutputCommitter.{setupJob, commitJob, abortJob} operations when dynamicPartitionOverwrite.

### Why are the changes needed?
Data may corrupt without this PR.

### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Existing Unit Test.
",spark,apache,turboFei,6757692,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25795,https://github.com/apache/spark/pull/25795,https://github.com/apache/spark/pull/25795.diff,https://github.com/apache/spark/pull/25795.patch
207,https://api.github.com/repos/apache/spark/issues/25773,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25773/labels{/name},https://api.github.com/repos/apache/spark/issues/25773/comments,https://api.github.com/repos/apache/spark/issues/25773/events,https://github.com/apache/spark/pull/25773,492659098,MDExOlB1bGxSZXF1ZXN0MzE2Nzg4MjI3,25773,[WIP][SPARK-29059][SQL] Ability to read Materialized views of Hive Table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-09-12T08:25:54Z,2019-09-17T00:46:07Z,,NONE,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Adding support to use Hive Materialized views([MV](https://cwiki.apache.org/confluence/display/Hive/Materialized+views)) in Spark. 
Gist of changes
- New optimizer rule that substitutes MV for a source table, if applicable
- Implication engine - which would figure out if an expression exp1 implies another expression exp2 i.e., if exp1 => exp2 is a tautology. This is similar to RexImplication checker in Apache Calcite.
- Hive client changes to read MV metadata from metastore

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently, Spark does not support reading MV of a Hive table.
This change adds the ability to use a MV in place of the source table, where it finds that the cost of running a query on MV is lesser than that of running on the source table.

### Does this PR introduce any user-facing change?
No


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

- Added Unit tests

",spark,apache,karuppayya,5082742,MDQ6VXNlcjUwODI3NDI=,https://avatars0.githubusercontent.com/u/5082742?v=4,,https://api.github.com/users/karuppayya,https://github.com/karuppayya,https://api.github.com/users/karuppayya/followers,https://api.github.com/users/karuppayya/following{/other_user},https://api.github.com/users/karuppayya/gists{/gist_id},https://api.github.com/users/karuppayya/starred{/owner}{/repo},https://api.github.com/users/karuppayya/subscriptions,https://api.github.com/users/karuppayya/orgs,https://api.github.com/users/karuppayya/repos,https://api.github.com/users/karuppayya/events{/privacy},https://api.github.com/users/karuppayya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25773,https://github.com/apache/spark/pull/25773,https://github.com/apache/spark/pull/25773.diff,https://github.com/apache/spark/pull/25773.patch
208,https://api.github.com/repos/apache/spark/issues/25748,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25748/labels{/name},https://api.github.com/repos/apache/spark/issues/25748/comments,https://api.github.com/repos/apache/spark/issues/25748/events,https://github.com/apache/spark/pull/25748,491904708,MDExOlB1bGxSZXF1ZXN0MzE2MTgzMzQ2,25748,[SPARK-28904][K8S] Create mount for PvTestSuite,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2019-09-10T21:21:03Z,2019-11-01T14:42:30Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

Automatically creates a temporary directory and mounts it into minikube for testing the persistent volume. Also minor style changes with replacing strings with the defined constants.

### Why are the changes needed?


The PVTestsSuite doesn't work out of the box on minikube 1.3.1


### Does this PR introduce any user-facing change?

No

### How was this patch tested?
Existing PVTestsSuite pass without manually creating mount
",spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars1.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25748,https://github.com/apache/spark/pull/25748,https://github.com/apache/spark/pull/25748.diff,https://github.com/apache/spark/pull/25748.patch
209,https://api.github.com/repos/apache/spark/issues/25721,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25721/labels{/name},https://api.github.com/repos/apache/spark/issues/25721/comments,https://api.github.com/repos/apache/spark/issues/25721/events,https://github.com/apache/spark/pull/25721,490747957,MDExOlB1bGxSZXF1ZXN0MzE1MjcwMzQw,25721,[WIP][SPARK-29018][SQL] Implement Spark Thrift Server with it's own code base on PROTOCOL_VERSION_V9,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-09-08T12:25:00Z,2019-10-24T02:01:13Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
Current SparkThriftServer need to fit so much hive version problem and it implement too much unused feature from HiveServer2. Maybe we should implement thrift server  with spark's own code. 

Changes:

1. Construct RowSet with StructType and Row
2. Old Thrift server avoid conflicts, pass a HiveConf for execution, remove all action about HiveMetaStore to class DelegationTokenHandler
3. Remove unnecessary action for Hive execution.
4. Add `hive-service` API code dependency for `beeline`
5. Implement some class to handle version problem such as `org.apache.spark.sql.hive.thriftserver.cli.Type`, `org.apache.spark.sql.hive.thriftserver.utils.LogHelper/VariableSubstitution`
6. Remove class ThrfitserverShimUtils and dir for hive version v1.2.1 & v2.3.5
7. Based on PROTOCOL_VERSION_V9, backwards compatibleÔºå when protocol version update, we can just  replace old and  add some more method since protocol is backwards compatible.




### Why are the changes needed?
Get rider of HiveServer2 API 's limit and fit better for Spark


### Does this PR introduce any user-facing change?
NO


### How was this patch tested?
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25721,https://github.com/apache/spark/pull/25721,https://github.com/apache/spark/pull/25721.diff,https://github.com/apache/spark/pull/25721.patch
210,https://api.github.com/repos/apache/spark/issues/25717,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25717/labels{/name},https://api.github.com/repos/apache/spark/issues/25717/comments,https://api.github.com/repos/apache/spark/issues/25717/events,https://github.com/apache/spark/pull/25717,490470141,MDExOlB1bGxSZXF1ZXN0MzE1MDcwNzI2,25717,[SPARK-29013][SQL] Structurally equivalent subexpression elimination,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2019-09-06T18:42:58Z,2019-09-17T20:04:53Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

We do semantically equivalent subexpression elimination in SparkSQL. However, for some expressions that are not semantically equivalent, but structurally equivalent, current subexpression elimination generates too many similar functions. These functions share same computation structure but only differ in input slots of current processing row.

For example, expression a is input[1] + input[2], expression b is input[3] + input[4]. They are not semantically equivalent in SparkSQL, but they have the same computation on different input data.

For such expressions, we can generate just one function, and pass in input slots during runtime.

It can reduce the length of generated code text, and save compilation time.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

For complex query, current sub-expression elimination could generate too many similar functions. It leads long generated code text and increases compilation time.

For example, run the following query:

```scala
val df = spark.range(2).selectExpr((0 to 5000).map(i => s""id as field_$i""): _*)
df.createOrReplaceTempView(""spark64kb"")
val data = spark.sql(""select * from spark64kb limit 10"")
data.describe()
```

The longest compilation time observed in this query is 25816.203394ms. After this patch, the same compilation is reduced to 9143.778397ms.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

This doesn't introduce user-facing change.
This feature is controlled by a SQL config `spark.sql.structuralSubexpressionElimination.enabled`.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Added tests.
",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25717,https://github.com/apache/spark/pull/25717,https://github.com/apache/spark/pull/25717.diff,https://github.com/apache/spark/pull/25717.patch
211,https://api.github.com/repos/apache/spark/issues/25707,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25707/labels{/name},https://api.github.com/repos/apache/spark/issues/25707/comments,https://api.github.com/repos/apache/spark/issues/25707/events,https://github.com/apache/spark/pull/25707,490089687,MDExOlB1bGxSZXF1ZXN0MzE0NzY0NjE5,25707,[SPARK-28587][SQL] Explicitly cast JDBC partition string literals to timestamp/date,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-09-06T01:59:31Z,2019-09-16T18:11:13Z,,MEMBER,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pr proposes to add explicit casts for generated JDBC partition string literals. In the current master, that logic depends on implicit casts of datasource DBMSs. For example;
```
// This assumes we have a relation testdb(t timestamp) in PostgreSQL
scala> val df = spark.read.format(""jdbc"")
  .option(""url"", ""jdbc:postgresql..."")
  .option(""dbtable"", ""testdb"")
  .option(""partitionColumn"", ""t"")
  .option(""lowerBound"", ""1972-07-04 03:30:00"")
  .option(""upperBound"", ""1972-07-27 14:11:05"")
  .option(""numPartitions"", 2)
  .load()
```
The query above generates  `""t"" < '1972-07-15 20:50:32.5' or ""t"" is null""` and `""t"" >= '1972-07-15 20:50:32.5'` internally for where clauses. Since `t` is timestamp, the clauses depend on implicit casts of PostgreSQL. The current one looks ok in most databases, but I believe explicit casts are more reasonable.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To support JDBC partitioning broadly.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing tests.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25707,https://github.com/apache/spark/pull/25707,https://github.com/apache/spark/pull/25707.diff,https://github.com/apache/spark/pull/25707.patch
212,https://api.github.com/repos/apache/spark/issues/25695,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25695/labels{/name},https://api.github.com/repos/apache/spark/issues/25695/comments,https://api.github.com/repos/apache/spark/issues/25695/events,https://github.com/apache/spark/pull/25695,489700834,MDExOlB1bGxSZXF1ZXN0MzE0NDYwNjY0,25695,[SPARK-28992][K8S] Support update dependencies from hdfs when task run on executor pods,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2019-09-05T11:46:31Z,2019-11-19T07:33:07Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?


Here is a case:¬†

```shell
bin/spark-submit¬† --class com.github.ehiggs.spark.terasort.TeraSort hdfs://hz-cluster10/user/kyuubi/udf/spark-terasort-1.1-SNAPSHOT-jar-with-dependencies.jar hdfs://hz-cluster10/user/kyuubi/terasort/1000g hdfs://hz-cluster10/user/kyuubi/terasort/1000g-out1
```
Spark supports add jar logic and¬†application-jar from hdfs - -¬†¬†http://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit

Take spark on yarn for example, it creates a _spark_hadoop_conf_.xml file and upload the hadoop distribute cache, the executor processes can use this to identify where their dependencies located.

But on k8s, i tried and failed to update dependencies.
```scala
19/09/04 08:08:52 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (newAPIHadoopFile at TeraSort.scala:60) failed in 1.058 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 9, 100.66.0.75, executor 2): java.lang.IllegalArgumentException: java.net.UnknownHostException: hz-cluster10
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
	at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1881)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:737)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:522)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7(Executor.scala:869)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7$adapted(Executor.scala:860)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:149)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:860)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

We need to update dependencies for task, some of them could located on hdfs

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

manually test & add uts
",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25695,https://github.com/apache/spark/pull/25695,https://github.com/apache/spark/pull/25695.diff,https://github.com/apache/spark/pull/25695.patch
213,https://api.github.com/repos/apache/spark/issues/25651,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25651/labels{/name},https://api.github.com/repos/apache/spark/issues/25651/comments,https://api.github.com/repos/apache/spark/issues/25651/events,https://github.com/apache/spark/pull/25651,488157294,MDExOlB1bGxSZXF1ZXN0MzEzMjM4MTk1,25651,[SPARK-28948][SQL] Support passing all Table metadata in TableProvider,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2019-09-02T12:07:19Z,2019-12-09T05:33:19Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Currently Data Source V2 has 2 major use cases:
1. users plug in a custom catalog, which is tightly coupled with its own data. For example, users can plug in a cassandra catalog, and use Spark to read/write cassandra tables directly.
2. users read/write the external data as a table directly via `DataFrameReader/Writer`, or register it as a table in Spark.

Use case 1 is newly introduced in the master branch, which greatly improves the user experience when interacting with external storage systems that have catalogs, e.g. cassandra, JDBC, etc.

Use case 2 is the main use case of Data Source V1, which works well if the external storage system doesn't have a catalog, e.g. parquet files on S3.

However, use case 2 is not well supported. For example
```
class MyTableProvider extends TableProvider ...
sql(""CREATE TABLE t USING com.abc.MyTableProvider"")
```
This fails with `AnalysisException: com.abc.MyTableProvider is not a valid Spark SQL Data Source`. The session catalog always treats table provider as v1 source.

To support it, this PR updates `TableProvider#getTable` to accept additional table metadata info. The expected behaviors are defined in https://docs.google.com/document/d/1oaS0eIVL1WsCjr4CqIpRv6CGkS5EoMQrngn3FsY1d-Q/edit?usp=sharing


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Make Data Source V2 supports the use case that is supported by Data Source V1.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes, it's a new feature

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
a new test suite",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25651,https://github.com/apache/spark/pull/25651,https://github.com/apache/spark/pull/25651.diff,https://github.com/apache/spark/pull/25651.patch
214,https://api.github.com/repos/apache/spark/issues/25644,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25644/labels{/name},https://api.github.com/repos/apache/spark/issues/25644/comments,https://api.github.com/repos/apache/spark/issues/25644/events,https://github.com/apache/spark/pull/25644,487883946,MDExOlB1bGxSZXF1ZXN0MzEzMDMyMzQw,25644,[SPARK-28940][SQL] Subquery reuse across all subquery levels,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-09-01T13:08:15Z,2019-10-29T13:58:07Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This PR enables subquery reuse across all subquery levels.
Please note that this feature works for adaptive query execution since https://github.com/apache/spark/pull/25471 but this PR:
- enables reuse for static execution (including Dynamic Partition Pruning) as well
- refactors AQE subquery reuse code a bit

Example query:
```
SELECT (SELECT avg(key) FROM testData), (SELECT (SELECT avg(key) FROM testData))
FROM testData
LIMIT 1
```
Plan before this PR:
```
CollectLimit 1
+- *(1) Project [Subquery scalar-subquery#268, [id=#231] AS scalarsubquery()#276, Subquery scalar-subquery#270, [id=#266] AS scalarsubquery()#277]
   :  :- Subquery scalar-subquery#268, [id=#231]
   :  :  +- *(2) HashAggregate(keys=[], functions=[avg(cast(key#13 as bigint))], output=[avg(key)#272])
   :  :     +- Exchange SinglePartition, true, [id=#227]
   :  :        +- *(1) HashAggregate(keys=[], functions=[partial_avg(cast(key#13 as bigint))], output=[sum#282, count#283L])
   :  :           +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#13]
   :  :              +- Scan[obj#12]
   :  +- Subquery scalar-subquery#270, [id=#266]
   :     +- *(1) Project [Subquery scalar-subquery#269, [id=#263] AS scalarsubquery()#275]
   :        :  +- Subquery scalar-subquery#269, [id=#263]
   :        :     +- *(2) HashAggregate(keys=[], functions=[avg(cast(key#13 as bigint))], output=[avg(key)#274])
   :        :        +- Exchange SinglePartition, true, [id=#259]
   :        :           +- *(1) HashAggregate(keys=[], functions=[partial_avg(cast(key#13 as bigint))], output=[sum#286, count#287L])
   :        :              +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#13]
   :        :                 +- Scan[obj#12]
   :        +- *(1) Scan OneRowRelation[]
   +- *(1) SerializeFromObject
      +- Scan[obj#12]
```
Plan after this PR:
```
CollectLimit 1
+- *(1) Project [ReusedSubquery Subquery scalar-subquery#241, [id=#148] AS scalarsubquery()#248, Subquery scalar-subquery#242, [id=#164] AS scalarsubquery()#249]
   :  :- ReusedSubquery Subquery scalar-subquery#241, [id=#148]
   :  +- Subquery scalar-subquery#242, [id=#164]
   :     +- *(1) Project [Subquery scalar-subquery#241, [id=#148] AS scalarsubquery()#247]
   :        :  +- Subquery scalar-subquery#241, [id=#148]
   :        :     +- *(2) HashAggregate(keys=[], functions=[avg(cast(key#13 as bigint))], output=[avg(key)#246])
   :        :        +- Exchange SinglePartition, true, [id=#144]
   :        :           +- *(1) HashAggregate(keys=[], functions=[partial_avg(cast(key#13 as bigint))], output=[sum#258, count#259L])
   :        :              +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#13]
   :        :                 +- Scan[obj#12]
   :        +- *(1) Scan OneRowRelation[]
   +- *(1) SerializeFromObject
      +- Scan[obj#12]
```

### Why are the changes needed?

Performance improvement.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added new UTs.",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25644,https://github.com/apache/spark/pull/25644,https://github.com/apache/spark/pull/25644.diff,https://github.com/apache/spark/pull/25644.patch
215,https://api.github.com/repos/apache/spark/issues/25618,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25618/labels{/name},https://api.github.com/repos/apache/spark/issues/25618/comments,https://api.github.com/repos/apache/spark/issues/25618/events,https://github.com/apache/spark/pull/25618,486864475,MDExOlB1bGxSZXF1ZXN0MzEyMjQxNDU4,25618,[SPARK-28908][SS]Implement Kafka EOS sink for Structured Streaming,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-08-29T10:07:39Z,2019-09-16T18:08:35Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Implement Kafka sink exactly-once semantics with transaction Kafka producer.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
1. Added UT

- write to transaction kafka
- restart from failed send, resend data
- recover from failed commit, resume producer and commit
- recover from other task failed commit, resume producer and recommit

2. Failover Test

situation | result
-- | --
task retry when sending data to Kafka | no data loss
application failed after successfully sending data to Kafka | no data loss when job attempt or application restart
job failed after store producer meta-info to HDFS | no data loss when job attempt or application restart
task retry when commit transaction | job failed and¬†no data loss when job attempt or application restart and resume transaction
executor crash down | job failed and¬†no data loss when job attempt or application restart

",spark,apache,wenxuanguan,7418459,MDQ6VXNlcjc0MTg0NTk=,https://avatars1.githubusercontent.com/u/7418459?v=4,,https://api.github.com/users/wenxuanguan,https://github.com/wenxuanguan,https://api.github.com/users/wenxuanguan/followers,https://api.github.com/users/wenxuanguan/following{/other_user},https://api.github.com/users/wenxuanguan/gists{/gist_id},https://api.github.com/users/wenxuanguan/starred{/owner}{/repo},https://api.github.com/users/wenxuanguan/subscriptions,https://api.github.com/users/wenxuanguan/orgs,https://api.github.com/users/wenxuanguan/repos,https://api.github.com/users/wenxuanguan/events{/privacy},https://api.github.com/users/wenxuanguan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25618,https://github.com/apache/spark/pull/25618,https://github.com/apache/spark/pull/25618.diff,https://github.com/apache/spark/pull/25618.patch
216,https://api.github.com/repos/apache/spark/issues/25609,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25609/labels{/name},https://api.github.com/repos/apache/spark/issues/25609/comments,https://api.github.com/repos/apache/spark/issues/25609/events,https://github.com/apache/spark/pull/25609,486289580,MDExOlB1bGxSZXF1ZXN0MzExNzcyNDg5,25609,[SPARK-28896][K8S] Support defining HADOOP_CONF_DIR and config map at the same time,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,34,2019-08-28T10:22:06Z,2019-12-10T23:02:05Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Changes in this pull request will support users to define `HADOOP_CONF_DIR` and `spark.kubernetes.hadoop.configMapName` at the same time. When both of them are defined, Spark will take precedence over the config map to be mounted on the driver pod. This enables the spark client process to communicate any Hadoop cluster if it needs.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The *BasicDriverFeatureStep* for Spark on Kubernetes will upload the files/jars specified by --files/‚Äìjars to a Hadoop compatible file system configured by spark.kubernetes.file.upload.path. While using HADOOP_CONF_DIR, the spark-submit process can recognize the file system, but when using spark.kubernetes.hadoop.configMapName which only will be mount on the Pods not applied back to our client process. 
 
```scala
 Kent@KentsMacBookPro ÓÇ∞ ~/Documents/spark-on-k8s/spark-3.0.0-SNAPSHOT-bin-2.7.3 ÓÇ∞ bin/spark-submit --conf spark.kubernetes.file.upload.path=hdfs://hz-cluster10/user/kyuubi/udf --jars /Users/Kent/Documents/spark-on-k8s/spark-3.0.0-SNAPSHOT-bin-2.7.3/hadoop-lzo-0.4.20-SNAPSHOT.jar --conf spark.kerberos.keytab=/Users/Kent/Downloads/kyuubi.keytab --conf spark.kerberos.principal=kyuubi/dev@HADOOP.HZ.NETEASE.COM --conf  spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf  --name hehe --deploy-mode cluster --class org.apache.spark.examples.HdfsTest   local:///opt/spark/examples/jars/spark-examples_2.12-3.0.0-SNAPSHOT.jar hdfs://hz-cluster10/user/kyuubi/hive_db/kyuubi.db/hive_tbl
Listening for transport dt_socket at address: 50014
# spark.master=k8s://https://10.120.238.100:7443
19/08/27 17:21:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/08/27 17:21:07 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
Listening for transport dt_socket at address: 50014
Exception in thread ""main"" org.apache.spark.SparkException: Uploading file /Users/Kent/Documents/spark-on-k8s/spark-3.0.0-SNAPSHOT-bin-2.7.3/hadoop-lzo-0.4.20-SNAPSHOT.jar failed...
	at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:287)
	at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:246)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:237)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadAndTransformFileUris(KubernetesUtils.scala:245)
	at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.$anonfun$getAdditionalPodSystemProperties$1(BasicDriverFeatur# spark.master=k8s://https://10.120.238.100:7443
eStep.scala:165)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.getAdditionalPodSystemProperties(BasicDriverFeatureStep.scala:163)
	at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.$anonfun$buildFromFeatures$3(KubernetesDriverBuilder.scala:60)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:89)
	at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.buildFromFeatures(KubernetesDriverBuilder.scala:58)
	at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:101)
	at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$10(KubernetesClientApplication.scala:236)
	at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$10$adapted(KubernetesClientApplication.scala:229)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:229)
	at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:198)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:179)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:89)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:999)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1008)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: hz-cluster10
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
	at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1881)
	at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:278)
	... 30 more
Caused by: java.net.UnknownHostException: hz-cluster10
	... 43 more
```

Other related spark configurations
```
spark.master=k8s://https://10.120.238.100:7443
# spark.master=k8s://https://10.120.238.253:7443
spark.kubernetes.container.image=harbor-inner.sparkonk8s.netease.com/tenant1-project1/spark:v3.0.0-20190813
# spark.kubernetes.driver.container.image=harbor-inner.sparkonk8s.netease.com/tenant1-project1/spark:v3.0.0-20190813
# spark.kubernetes.executor.container.image=harbor-inner.sparkonk8s.netease.com/tenant1-project1/spark:v3.0.0-20190813
spark.executor.instanses=5
spark.kubernetes.namespace=ns1
spark.kubernetes.container.image.pullSecrets=mysecret
spark.kubernetes.hadoop.configMapName=hz10-hadoop-dir
spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf
spark.kerberos.principal=kyuubi/dev@HADOOP.HZ.NETEASE.COM
spark.kerberos.keytab=/Users/Kent/Downloads/kyuubi.keytab
```


### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

I guess this pr will now use config map of Hadoop from k8s cluster to the local client process if there are files to upload to Hadoop.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

manually tested with spark + k8s cluster + standalone kerberized hdfs cluster
add an unit test",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25609,https://github.com/apache/spark/pull/25609,https://github.com/apache/spark/pull/25609.diff,https://github.com/apache/spark/pull/25609.patch
217,https://api.github.com/repos/apache/spark/issues/25602,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25602/labels{/name},https://api.github.com/repos/apache/spark/issues/25602/comments,https://api.github.com/repos/apache/spark/issues/25602/events,https://github.com/apache/spark/pull/25602,486100531,MDExOlB1bGxSZXF1ZXN0MzExNjIyODA3,25602,[SPARK-28613][SQL] Add config option for limiting uncompressed result size in SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-08-28T00:33:41Z,2019-12-24T09:48:15Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
This PR adds a new config option `spark.sql.driver.maxUncompressedResultSize` (defaulting to empty which preserves the same behavior as before this PR).
If this config option is present then the size of the uncompressed, decoded result of SQL actions (e.g., collect) will be limited to its value.

### Why are the changes needed?
The main problem with the existing `spark.driver.maxResultSize` is that it only enforces the size of the compressed data (of the compressed byte rdd). The actual uncompressed size can be much larger. Thus, `spark.driver.maxResultSize` is no good mechanism for protecting the driver against OOMs when using spark sql.

Adding this new config option provides an additional, better way for protecting the driver against OOMs during collects.

### Does this PR introduce any user-facing change?
No. 


### How was this patch tested?
I added a new unit test in `SparkPlanSuite.scala`
",spark,apache,dvogelbacher,37212524,MDQ6VXNlcjM3MjEyNTI0,https://avatars0.githubusercontent.com/u/37212524?v=4,,https://api.github.com/users/dvogelbacher,https://github.com/dvogelbacher,https://api.github.com/users/dvogelbacher/followers,https://api.github.com/users/dvogelbacher/following{/other_user},https://api.github.com/users/dvogelbacher/gists{/gist_id},https://api.github.com/users/dvogelbacher/starred{/owner}{/repo},https://api.github.com/users/dvogelbacher/subscriptions,https://api.github.com/users/dvogelbacher/orgs,https://api.github.com/users/dvogelbacher/repos,https://api.github.com/users/dvogelbacher/events{/privacy},https://api.github.com/users/dvogelbacher/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25602,https://github.com/apache/spark/pull/25602,https://github.com/apache/spark/pull/25602.diff,https://github.com/apache/spark/pull/25602.patch
218,https://api.github.com/repos/apache/spark/issues/25577,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25577/labels{/name},https://api.github.com/repos/apache/spark/issues/25577/comments,https://api.github.com/repos/apache/spark/issues/25577/events,https://github.com/apache/spark/pull/25577,484949215,MDExOlB1bGxSZXF1ZXN0MzEwNjk3MTYy,25577,[WIP][CORE][SPARK-28867] InMemoryStore checkpoint to speed up replay log file in HistoryServer,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,49,2019-08-25T16:23:34Z,2019-10-19T14:45:43Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR aims to improve the replay performance in HistoryServer by periodically checkpoint InMemoryStore in an in-completed application and achieve incremental replay.The main idea
is, for an in-completed application, we periodically (normally every N events num) checkpoint InMemoryStore with processed events num(X) into event log dir. And in HistoryServer, it reconstructs InMemoryStore from checkpoint file and gets X. Then, we could skip X events while replaying the log file basing on the partial InMemoryStore. Note that we should also recover those live entities from the the partial InMemoryStore in AppStatusListener to perform incremental replay. For a completed application, HistoryServer could just reconstructs InMemoryStore and no need to do replay.

And in this PR, we only focus on handling InMemoryStore in HistoryServer, while LevelDB is planed to be handled in similar way in following PR.

Basic experiment on a completed application of 20055 events shows the improvement of this optimization:

without optimization  | with optimization(including deserialization time)
 :-: | :-: | 
4343 | 78(70)
4512 | 92(85)
4475 | 74(68)
4254 | 93(78)
4126 | 81(71)

Work TODO

- [ ] compression support when checkpoint InMemoryStore
- [ ] More accurate conversion from wrapper data to live entity
- [ ] checkpoint file cleaning in HistoryServer
- [ ] overcome frequently StackOverError in deserialization
- [ ] SparkListenerStageExecutorMetrics synchronization between EventLoggingListener and AppStatusListener when log stage executor metrics is enabled
- [ ] unit tests

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Change is needed because HistoryServer now could be very slow to replay a large log file at the first time and it always re-replay an in-progress log file after it changes which leads to low efficiency. 

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->

Yes, if user wants to use this optimization by several new configurations.  

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Only tested manually yet, still work in process.",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25577,https://github.com/apache/spark/pull/25577,https://github.com/apache/spark/pull/25577.diff,https://github.com/apache/spark/pull/25577.patch
219,https://api.github.com/repos/apache/spark/issues/25569,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25569/labels{/name},https://api.github.com/repos/apache/spark/issues/25569/comments,https://api.github.com/repos/apache/spark/issues/25569/events,https://github.com/apache/spark/pull/25569,484710265,MDExOlB1bGxSZXF1ZXN0MzEwNTMyODY1,25569,"[SPARK-28863][SQL] Introduce AlreadyPlanned, a node that speeds-up planning","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-08-23T21:00:53Z,2019-09-23T02:15:06Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?

This PR introduces a LogicalNode AlreadyPlanned, and related physical plan and preparation rule.

With the DataSourceV2 write operations, we have a way to fallback to the V1 writer APIs using InsertableRelation. The gross part is that we're in physical land, but the InsertableRelation takes a logical plan, so we have to pass the logical plans to these physical nodes, and then potentially go through re-planning.

A useful primitive could be specifying that a plan is ready for execution through a logical node AlreadyPlanned. This would wrap a physical plan, and then we can go straight to execution. In addition, if you have queries which you re-run over again with minor parameter changes, you can leverage this node to avoid re-planning potentially huge query plans. An example of this could be Parametrized Views in SQL Server.

### Why are the changes needed?

To avoid having a physical plan that is disconnected from the physical plan that is being executed in V1WriteFallback execution. When a physical plan node executes a logical plan, the inner query is not connected to the running physical plan. The physical plan that actually runs is not visible through the Spark UI and its metrics are not exposed. In some cases, the EXPLAIN plan doesn't show it.

### Does this PR introduce any user-facing change?

Nope

### How was this patch tested?

V1FallbackWriterSuite tests that writes still work, and we also introduce AlreadyPlannedSuite",spark,apache,brkyvz,5243515,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25569,https://github.com/apache/spark/pull/25569,https://github.com/apache/spark/pull/25569.diff,https://github.com/apache/spark/pull/25569.patch
220,https://api.github.com/repos/apache/spark/issues/25552,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25552/labels{/name},https://api.github.com/repos/apache/spark/issues/25552/comments,https://api.github.com/repos/apache/spark/issues/25552/events,https://github.com/apache/spark/pull/25552,483879102,MDExOlB1bGxSZXF1ZXN0MzA5ODY2MjM2,25552,[SPARK-28849][CORE] Add a number to control transferTo calls to avoid infinite loop in some occasional cases,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-08-22T09:34:46Z,2019-09-16T18:50:07Z,,CONTRIBUTOR,"This patch propose a way to detect the infinite loop issue in `transferTo` and make it fail fast. The detailed issue is shown in https://issues.apache.org/jira/browse/SPARK-28849. 

Here propose a new undocumented configuration and a way to track the number of `transferTo` calls, if the number is larger than the specified value, it will jump out of the loop and throw an exception. 

Typically this will not be happened, and user don't need to set this configuration explicitly, in our environment it happens occasionally and cause the task hung infinitely.
",spark,apache,jerryshao,850797,MDQ6VXNlcjg1MDc5Nw==,https://avatars2.githubusercontent.com/u/850797?v=4,,https://api.github.com/users/jerryshao,https://github.com/jerryshao,https://api.github.com/users/jerryshao/followers,https://api.github.com/users/jerryshao/following{/other_user},https://api.github.com/users/jerryshao/gists{/gist_id},https://api.github.com/users/jerryshao/starred{/owner}{/repo},https://api.github.com/users/jerryshao/subscriptions,https://api.github.com/users/jerryshao/orgs,https://api.github.com/users/jerryshao/repos,https://api.github.com/users/jerryshao/events{/privacy},https://api.github.com/users/jerryshao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25552,https://github.com/apache/spark/pull/25552,https://github.com/apache/spark/pull/25552.diff,https://github.com/apache/spark/pull/25552.patch
221,https://api.github.com/repos/apache/spark/issues/25514,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25514/labels{/name},https://api.github.com/repos/apache/spark/issues/25514/comments,https://api.github.com/repos/apache/spark/issues/25514/events,https://github.com/apache/spark/pull/25514,483027805,MDExOlB1bGxSZXF1ZXN0MzA5MTg2MDAw,25514,[SPARK-28784][SS] Use CheckpointFileManager in StreamExecution/StreamingQueryManager for checkpoint dirs,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2019-08-20T18:58:04Z,2019-12-05T23:46:10Z,,NONE,"### What changes were proposed in this pull request?
After `SPARK-23966 Refactoring all checkpoint file writing logic in a common CheckpointFileManager interface`, the CheckpointFileManager interface was created to handle all structured streaming checkpointing operations and helps users to choose how they wish to write checkpointing files atomically.
StreamExecution and StreamingQueryManager still uses some FileSystem operations without using the CheckpointFileManager.
For instance,
- https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L137
- https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L392

Instead, StreamExecution and StreamingQueryManager should use CheckpointFileManager for these operations.

### Why are the changes needed?
This change will allow users to use CheckpointFileManager for structured streaming checkpointing files without need for a separate FileSystem implementation for the same.


### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing tests",spark,apache,shrutig,1889572,MDQ6VXNlcjE4ODk1NzI=,https://avatars3.githubusercontent.com/u/1889572?v=4,,https://api.github.com/users/shrutig,https://github.com/shrutig,https://api.github.com/users/shrutig/followers,https://api.github.com/users/shrutig/following{/other_user},https://api.github.com/users/shrutig/gists{/gist_id},https://api.github.com/users/shrutig/starred{/owner}{/repo},https://api.github.com/users/shrutig/subscriptions,https://api.github.com/users/shrutig/orgs,https://api.github.com/users/shrutig/repos,https://api.github.com/users/shrutig/events{/privacy},https://api.github.com/users/shrutig/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25514,https://github.com/apache/spark/pull/25514,https://github.com/apache/spark/pull/25514.diff,https://github.com/apache/spark/pull/25514.patch
222,https://api.github.com/repos/apache/spark/issues/25495,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25495/labels{/name},https://api.github.com/repos/apache/spark/issues/25495/comments,https://api.github.com/repos/apache/spark/issues/25495/events,https://github.com/apache/spark/pull/25495,482249020,MDExOlB1bGxSZXF1ZXN0MzA4NTU5NDA2,25495,[PYTHON][SQL][WIP] repr(schema) and schema.toString produce runnable code,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-08-19T11:10:16Z,2019-09-16T18:10:36Z,,NONE,"# What changes were proposed in this pull request?
repr(schema) produces runnable python code
schema.toString produce runnable scala code

### Why are the changes needed?
Previously, schema.toString produced scala code that wasn't runnable because field-names weren't quoted.  Even worse, repr(schema) in python produced the same non-runnable scala code.  This resolves both issues, so that runnable Scala and Python are available.

### Does this PR introduce any user-facing change?
Yes, see above.

### How was this patch tested?
pyspark/sql/tests/test_types.py now has test_repr()
",spark,apache,dougbateman,309661,MDQ6VXNlcjMwOTY2MQ==,https://avatars0.githubusercontent.com/u/309661?v=4,,https://api.github.com/users/dougbateman,https://github.com/dougbateman,https://api.github.com/users/dougbateman/followers,https://api.github.com/users/dougbateman/following{/other_user},https://api.github.com/users/dougbateman/gists{/gist_id},https://api.github.com/users/dougbateman/starred{/owner}{/repo},https://api.github.com/users/dougbateman/subscriptions,https://api.github.com/users/dougbateman/orgs,https://api.github.com/users/dougbateman/repos,https://api.github.com/users/dougbateman/events{/privacy},https://api.github.com/users/dougbateman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25495,https://github.com/apache/spark/pull/25495,https://github.com/apache/spark/pull/25495.diff,https://github.com/apache/spark/pull/25495.patch
223,https://api.github.com/repos/apache/spark/issues/25452,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25452/labels{/name},https://api.github.com/repos/apache/spark/issues/25452/comments,https://api.github.com/repos/apache/spark/issues/25452/events,https://github.com/apache/spark/pull/25452,480585783,MDExOlB1bGxSZXF1ZXN0MzA3MjUyNDcx,25452,"[SPARK-28710][SQL]to fix replace function, spark should call drop and create function","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-08-14T09:51:18Z,2019-12-17T07:05:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

create or replace function **X as 'Y' using jar Z**;  does not work if the X is already present and is created with some other jar lets say xyz.

As per current implementation spark calls alter Function API of Hive, as of now Hive only alter name, owner, class name, type but not resource URI. After calling alter function only  **X and Y**  are updated **but not Z.**

So when the select is performed on X UDF it throws class not found exception.

Observation 1: Temporary function does not have this problem as it is handled by spark logic 

Observation 2: For permanent function Spark calls the Hive to alter function , as of now Hive only alter name, owner, class name, type but not resource URI.

[SparkCode](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala#L86)

[Hive Code](https://github.com/apache/hive/blob/master/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L9914)

 As per [Hive Documentation](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Create/Drop/ReloadFunction) it does not supports create or replace command for function

## How was this patch tested?

Added UT and also manually tested

Before Fix:
![image](https://user-images.githubusercontent.com/35216143/63011564-308e6e00-bea6-11e9-8b1d-271847a96018.png)

After Fix:
![image](https://user-images.githubusercontent.com/35216143/63011970-fa9db980-bea6-11e9-8d7f-1aece951ca84.png)



",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25452,https://github.com/apache/spark/pull/25452,https://github.com/apache/spark/pull/25452.diff,https://github.com/apache/spark/pull/25452.patch
224,https://api.github.com/repos/apache/spark/issues/25448,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25448/labels{/name},https://api.github.com/repos/apache/spark/issues/25448/comments,https://api.github.com/repos/apache/spark/issues/25448/events,https://github.com/apache/spark/pull/25448,480512125,MDExOlB1bGxSZXF1ZXN0MzA3MTkzNDAx,25448,[SPARK-28697][SQL] Invalidate Database/Table names starting with underscore,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-08-14T06:55:44Z,2019-09-16T18:10:17Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

I think we should disallow if a identifier starts with _ for create database and create table
Partially we can see its effect in SPARK-28697 where as the table name starts with _ (like _sampleTable) , the FileFormat assumes it to be a hidden folder and do not list it which causes unusual behavior

## How was this patch tested?

Avoiding creating tables and databases with names starting from underscore. Added test case for same",spark,apache,ajithme,22072336,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25448,https://github.com/apache/spark/pull/25448,https://github.com/apache/spark/pull/25448.diff,https://github.com/apache/spark/pull/25448.patch
225,https://api.github.com/repos/apache/spark/issues/25436,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25436/labels{/name},https://api.github.com/repos/apache/spark/issues/25436/comments,https://api.github.com/repos/apache/spark/issues/25436/events,https://github.com/apache/spark/pull/25436,480278989,MDExOlB1bGxSZXF1ZXN0MzA3MDA2NzA4,25436,[WIP]Support minPartitions for batch and Kafka source v1,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-08-13T17:35:56Z,2019-09-24T18:33:03Z,,MEMBER,"## What changes were proposed in this pull request?

Support minPartitions for batch and Kafka source v1. Still WIP

## How was this patch tested?

Jenkins",spark,apache,zsxwing,1000778,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25436,https://github.com/apache/spark/pull/25436,https://github.com/apache/spark/pull/25436.diff,https://github.com/apache/spark/pull/25436.patch
226,https://api.github.com/repos/apache/spark/issues/25416,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25416/labels{/name},https://api.github.com/repos/apache/spark/issues/25416/comments,https://api.github.com/repos/apache/spark/issues/25416/events,https://github.com/apache/spark/pull/25416,479496255,MDExOlB1bGxSZXF1ZXN0MzA2Mzc0NzAx,25416,[SPARK-28330][SQL] Support ANSI SQL: result offset clause in query expression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2019-08-12T07:08:07Z,2019-12-19T22:13:57Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This is a ANSI SQL and feature id is `F861`
```
<query expression> ::=
[ <with clause> ] <query expression body>
[ <order by clause> ] [ <result offset clause> ] [ <fetch first clause> ]

<result offset clause> ::=
OFFSET <offset row count> { ROW | ROWS }
```
For example:
```
SELECT customer_name, customer_gender FROM customer_dimension 
   WHERE occupation='Dancer' AND customer_city = 'San Francisco' ORDER BY customer_name;
    customer_name     | customer_gender
----------------------+-----------------
 Amy X. Lang          | Female
 Anna H. Li           | Female
 Brian O. Weaver      | Male
 Craig O. Pavlov      | Male
 Doug Z. Goldberg     | Male
 Harold S. Jones      | Male
 Jack E. Perkins      | Male
 Joseph W. Overstreet | Male
 Kevin . Campbell     | Male
 Raja Y. Wilson       | Male
 Samantha O. Brown    | Female
 Steve H. Gauthier    | Male
 William . Nielson    | Male
 William Z. Roy       | Male
(14 rows)

SELECT customer_name, customer_gender FROM customer_dimension 
   WHERE occupation='Dancer' AND customer_city = 'San Francisco' ORDER BY customer_name OFFSET 8;
   customer_name   | customer_gender
-------------------+-----------------
 Kevin . Campbell  | Male
 Raja Y. Wilson    | Male
 Samantha O. Brown | Female
 Steve H. Gauthier | Male
 William . Nielson | Male
 William Z. Roy    | Male
(6 rows)
```
There are some mainstream database support the syntax.
**PostgreSQL:**
https://www.postgresql.org/docs/11/queries-limit.html

**Vertica:**
https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Statements/SELECT/OFFSETClause.htm?zoom_highlight=offset

**MySQL:**
https://dev.mysql.com/doc/refman/5.6/en/select.html

## How was this patch tested?
new UT.
There are some show of the PR on my production environment.
```
spark-sql> select * from gja_test_partition;
a       A       ao      1
b       B       bo      1
c       C       co      1
d       D       do      1
e       E       eo      2
g       G       go      2
h       H       ho      2
j       J       jo      2
f       F       fo      3
k       K       ko      3
l       L       lo      4
i       I       io      4
Time taken: 6.618 s
spark-sql> select * from gja_test_partition offset 3;
d       D       do      1
e       E       eo      2
g       G       go      2
h       H       ho      2
j       J       jo      2
f       F       fo      3
k       K       ko      3
l       L       lo      4
i       I       io      4
Time taken: 6.368 s
spark-sql> select * from gja_test_partition limit 5 offset 3;
d       D       do      1
e       E       eo      2
g       G       go      2
h       H       ho      2
j       J       jo      2
Time taken: 25.141 s
spark-sql> select * from gja_test_partition order by key;
a       A       ao      1
b       B       bo      1
c       C       co      1
d       D       do      1
e       E       eo      2
f       F       fo      3
g       G       go      2
h       H       ho      2
i       I       io      4
j       J       jo      2
k       K       ko      3
l       L       lo      4
Time taken: 16.894 s
spark-sql> select * from gja_test_partition order by key offset 3;
d       D       do      1
e       E       eo      2
f       F       fo      3
g       G       go      2
h       H       ho      2
i       I       io      4
j       J       jo      2
k       K       ko      3
l       L       lo      4
Time taken: 19.191 s
spark-sql> select * from gja_test_partition order by key limit 5 offset 3;
d       D       do      1
e       E       eo      2
f       F       fo      3
g       G       go      2
h       H       ho      2
Time taken: 12.556 s
```",spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25416,https://github.com/apache/spark/pull/25416,https://github.com/apache/spark/pull/25416.diff,https://github.com/apache/spark/pull/25416.patch
227,https://api.github.com/repos/apache/spark/issues/25399,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25399/labels{/name},https://api.github.com/repos/apache/spark/issues/25399/comments,https://api.github.com/repos/apache/spark/issues/25399/events,https://github.com/apache/spark/pull/25399,478980706,MDExOlB1bGxSZXF1ZXN0MzA1OTg0OTYy,25399,[SPARK-28670][SQL] create function should thrown Exception if the resource is not found,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2019-08-09T13:13:48Z,2019-09-16T18:08:34Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Create temporary or permanent function it should throw AnalysisException if the resource is not found. Need to keep behavior consistent across permanent and temporary functions.

## How was this patch tested?

Added UT and also tested manually

**Before Fix**
If the UDF resource is not present then on creation of temporary function it throws AnalysisException where as for permanent function it does not throw. Permanent funtcion  throws AnalysisException only after select operation is performed.

**After Fix**

For temporary and permanent function check for the resource, if the UDF resource is not found then throw AnalysisException

![rt](https://user-images.githubusercontent.com/35216143/62781519-d1131580-bad5-11e9-9d58-69e65be86c03.png)
",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25399,https://github.com/apache/spark/pull/25399,https://github.com/apache/spark/pull/25399.diff,https://github.com/apache/spark/pull/25399.patch
228,https://api.github.com/repos/apache/spark/issues/25398,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25398/labels{/name},https://api.github.com/repos/apache/spark/issues/25398/comments,https://api.github.com/repos/apache/spark/issues/25398/events,https://github.com/apache/spark/pull/25398,478920014,MDExOlB1bGxSZXF1ZXN0MzA1OTM0OTU3,25398,[SPARK-28659][SQL] Use data source if convertible in insert overwrite directory,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2019-08-09T10:37:01Z,2019-12-10T02:18:01Z,,CONTRIBUTOR,"### What changes were proposed in this pull request?
In insert overwrite directory while using `STORED AS file_format`,  files are not compressed.
In this PR it is converted to `datasource`  if it is convertible, to make it inline with`CTAS` behavior which is fixed in this  [PR](https://github.com/apache/spark/pull/22514)

### Why are the changes needed?
To make the behavior inline with `CTAS` while using `STORED AS file_format`

### Does this PR introduce any user-facing change?
Yes, After the fix of this PR now `STORED AS file_format` will be converted to `datasource` if it is convertible 
**Before**
![before](https://user-images.githubusercontent.com/44489863/66637198-28a03f80-ec45-11e9-9eb1-69bc21643138.png)


**After**
![after](https://user-images.githubusercontent.com/44489863/66637058-eb3bb200-ec44-11e9-88a0-2b2ab242f88f.PNG)

### How was this patch tested?
New testcase is added",spark,apache,Udbhav30,44489863,MDQ6VXNlcjQ0NDg5ODYz,https://avatars2.githubusercontent.com/u/44489863?v=4,,https://api.github.com/users/Udbhav30,https://github.com/Udbhav30,https://api.github.com/users/Udbhav30/followers,https://api.github.com/users/Udbhav30/following{/other_user},https://api.github.com/users/Udbhav30/gists{/gist_id},https://api.github.com/users/Udbhav30/starred{/owner}{/repo},https://api.github.com/users/Udbhav30/subscriptions,https://api.github.com/users/Udbhav30/orgs,https://api.github.com/users/Udbhav30/repos,https://api.github.com/users/Udbhav30/events{/privacy},https://api.github.com/users/Udbhav30/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25398,https://github.com/apache/spark/pull/25398,https://github.com/apache/spark/pull/25398.diff,https://github.com/apache/spark/pull/25398.patch
229,https://api.github.com/repos/apache/spark/issues/25372,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25372/labels{/name},https://api.github.com/repos/apache/spark/issues/25372/comments,https://api.github.com/repos/apache/spark/issues/25372/events,https://github.com/apache/spark/pull/25372,477629062,MDExOlB1bGxSZXF1ZXN0MzA0OTE4Njcy,25372,[SPARK-28640][SQL] Do not show stack trace when default or session catalog is misconfigured,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-08-06T22:55:16Z,2019-09-16T18:52:17Z,,MEMBER,"## What changes were proposed in this pull request?

LookupCatalog's `sessionCatalog` and `defaultCatalog` logs an error message and the exception stack upon any nonfatal exception. When either catalog is misconfigured, this may clutter the console and alarm the user unnecessarily. It should be enough to print a warning and return None.

## How was this patch tested?

Manual test cases

Start Spark shell with either of these configurations:
- spark.sql.catalog.session=noclass
- spark.sql.default.catalog=def
- spark.sql.default.catalog=def and spark.sql.catalog.def=noclass

Enter `spark.sessionState.analyzer.defaultCatalog` or `spark.sessionState.analyzer.sessionCatalog` at the prompt, expect a warning and no stack trace.",spark,apache,jzhuge,1883812,MDQ6VXNlcjE4ODM4MTI=,https://avatars2.githubusercontent.com/u/1883812?v=4,,https://api.github.com/users/jzhuge,https://github.com/jzhuge,https://api.github.com/users/jzhuge/followers,https://api.github.com/users/jzhuge/following{/other_user},https://api.github.com/users/jzhuge/gists{/gist_id},https://api.github.com/users/jzhuge/starred{/owner}{/repo},https://api.github.com/users/jzhuge/subscriptions,https://api.github.com/users/jzhuge/orgs,https://api.github.com/users/jzhuge/repos,https://api.github.com/users/jzhuge/events{/privacy},https://api.github.com/users/jzhuge/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25372,https://github.com/apache/spark/pull/25372,https://github.com/apache/spark/pull/25372.diff,https://github.com/apache/spark/pull/25372.patch
230,https://api.github.com/repos/apache/spark/issues/25291,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25291/labels{/name},https://api.github.com/repos/apache/spark/issues/25291/comments,https://api.github.com/repos/apache/spark/issues/25291/events,https://github.com/apache/spark/pull/25291,474093504,MDExOlB1bGxSZXF1ZXN0MzAyMTAzOTA2,25291,[SPARK-28554][SQL] implement basic catalog functionalities for JDBC v2 with a DS v1 fallback API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2019-07-29T14:45:49Z,2019-09-16T18:53:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR implements the basic catalog functionalities for JDBC v2, so that users can register a JDBC catalog and create/access JDBC tables directly.

To make the PR small, this PR introduces a data source v1 fallback API so that we don't need to implement `ScanBuilder` and `WriteBuilder` for JDBC v1 right now.

Note that, this PR is orthogonal to https://github.com/apache/spark/pull/25211, which implements `ScanBuilder`, `WriterBuilder` for JDBC v2.

## How was this patch tested?

a new test suite.",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25291,https://github.com/apache/spark/pull/25291,https://github.com/apache/spark/pull/25291.diff,https://github.com/apache/spark/pull/25291.patch
231,https://api.github.com/repos/apache/spark/issues/25290,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25290/labels{/name},https://api.github.com/repos/apache/spark/issues/25290/comments,https://api.github.com/repos/apache/spark/issues/25290/events,https://github.com/apache/spark/pull/25290,474043975,MDExOlB1bGxSZXF1ZXN0MzAyMDYzNDY0,25290,[SPARK-28551][SQL]Add a Checker Rule when CTAS SQL with LOCATION ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-07-29T13:12:42Z,2019-09-16T18:11:42Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When we run SQL like 

`CRETE TABLE TBL `
`LOCATION 'PATH_URI`
`AS`
`SELECT QUERY `
It won't check PATH_URI status, if there is some data under this path, it will just overwrite this path.

If user careless write a path with important data store under the path, it will be a disaster„ÄÇ 

So I wish to add a checker rule in PreWriteChecker to avoid this situation happens

## How was this patch tested?
UT & MT
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25290,https://github.com/apache/spark/pull/25290,https://github.com/apache/spark/pull/25290.diff,https://github.com/apache/spark/pull/25290.patch
232,https://api.github.com/repos/apache/spark/issues/25280,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25280/labels{/name},https://api.github.com/repos/apache/spark/issues/25280/comments,https://api.github.com/repos/apache/spark/issues/25280/events,https://github.com/apache/spark/pull/25280,473773870,MDExOlB1bGxSZXF1ZXN0MzAxODUyMDIw,25280,[SPARK-28548][SQL] explain() shows wrong result for persisted DataFrames after some operations,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-07-28T18:48:09Z,2019-09-16T18:53:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

After some operations against Datasets and then persist them, Dataset.explain shows wrong result.
One of those operations is explain() itself.
An example here.

```
val df = spark.range(10)
df.explain
df.persist
df.explain
```

Expected result is like as follows.
```
== Physical Plan ==
*(1) ColumnarToRow
+- InMemoryTableScan [id#7L]
      +- InMemoryRelation [id#7L], StorageLevel(disk, memory, deserialized, 1 replicas)
            +- *(1) Range (0, 10, step=1, splits=12)
```

But I got this.
```
== Physical Plan ==
*(1) Range (0, 10, step=1, splits=12)
```

This issue is caused by `withCachedData` in `QueryExecution` is materialized early when `explain()` or such methods are called so this patch prevents it.

## How was this patch tested?
Additional test cases in `ExplainSuite.scala`",spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25280,https://github.com/apache/spark/pull/25280,https://github.com/apache/spark/pull/25280.diff,https://github.com/apache/spark/pull/25280.patch
233,https://api.github.com/repos/apache/spark/issues/25258,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25258/labels{/name},https://api.github.com/repos/apache/spark/issues/25258/comments,https://api.github.com/repos/apache/spark/issues/25258/events,https://github.com/apache/spark/pull/25258,473181350,MDExOlB1bGxSZXF1ZXN0MzAxMzk4ODM0,25258,[SPARK-19712][SQL] Move subquery rewrite to beginning of optimizer,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-07-26T05:18:44Z,2019-12-18T10:43:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently predicate subqueries (IN/EXISTS) are converted to Joins at the end of optimizer in RewritePredicateSubquery. This change moves the rewrite close to beginning of optimizer. The original idea was to keep the subquery expressions in Filter form so that we can push them down as deep as possible. One disadvantage is that, after the subqueries are rewritten in join form, they are not subjected to further optimizations. In this change, we convert the subqueries to join form early in the rewrite phase.  

I will combine the pullupCorrelatedPredicates and RewritePredicateSubquery in a follow-up PR.

## How was this patch tested?
A new test suite `LeftSemiAntiJoinAndSubqueryEquivalencySuite` is added to verify that the correlated subqueries and queries that explicitly use leftsemi/anti joins result in same plan after optmization.",spark,apache,dilipbiswal,14225158,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25258,https://github.com/apache/spark/pull/25258,https://github.com/apache/spark/pull/25258.diff,https://github.com/apache/spark/pull/25258.patch
234,https://api.github.com/repos/apache/spark/issues/25211,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25211/labels{/name},https://api.github.com/repos/apache/spark/issues/25211/comments,https://api.github.com/repos/apache/spark/issues/25211/events,https://github.com/apache/spark/pull/25211,470692305,MDExOlB1bGxSZXF1ZXN0Mjk5NTc1OTI2,25211,[WIP][SPARK-24907][SQL] DataSourceV2 based connector for JDBC,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-07-20T16:36:48Z,2019-09-16T18:11:31Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This is a Work in Progress PR for DataSourceV2 based connector for JDBC. The goal is a MVP for both read and write path based on latest data source V2 apis. As of now the PR is *not* complete, but provided here for visibility on this work and comments to set us in the right direction. 

Another PR on related work is https://github.com/apache/spark/pull/21861. That uses older V2 apis, but some of the work there may be still relevant. Have requested author to consider merge if possible. 
@tengpeng @xianyin as FYI as they volunteered for contribution to the work going forward.

Readme.md added for high level work items. Find it at org/apache/spark/sql/execution/datasources/v2/jdbc/Readme.md

(Please fill in changes proposed in this fix)
The current PR implements the following ( will keep this updated we make progress on this)
- Scaffolding for read/write paths.
- First draft implementation of dataframe write(append) flow. Connector name is ""jdbcv2"". df.write.format(""jdbcv2"").mode(""append"") appends to Table if table exists. Create table not supported as of now.
- E2E test cases added in MsSqlServerIntegrationSuite.scala
- JDBCUtils is reused as and when easiliy possible, but further scope of refactoring to make it work for both V1 and V2 flows.

## How was this patch tested?
- Validation with SQLServer 2017 only.
- No unit test cases added for now.

(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
The path was mainly integration tested for write ( append) path.

Please review https://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,shivsood,1579057,MDQ6VXNlcjE1NzkwNTc=,https://avatars2.githubusercontent.com/u/1579057?v=4,,https://api.github.com/users/shivsood,https://github.com/shivsood,https://api.github.com/users/shivsood/followers,https://api.github.com/users/shivsood/following{/other_user},https://api.github.com/users/shivsood/gists{/gist_id},https://api.github.com/users/shivsood/starred{/owner}{/repo},https://api.github.com/users/shivsood/subscriptions,https://api.github.com/users/shivsood/orgs,https://api.github.com/users/shivsood/repos,https://api.github.com/users/shivsood/events{/privacy},https://api.github.com/users/shivsood/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25211,https://github.com/apache/spark/pull/25211,https://github.com/apache/spark/pull/25211.diff,https://github.com/apache/spark/pull/25211.patch
235,https://api.github.com/repos/apache/spark/issues/25205,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25205/labels{/name},https://api.github.com/repos/apache/spark/issues/25205/comments,https://api.github.com/repos/apache/spark/issues/25205/events,https://github.com/apache/spark/pull/25205,470404269,MDExOlB1bGxSZXF1ZXN0Mjk5MzkzMzI2,25205,[SPARK-28415][DSTREAMS] Add messageHandler to Kafka 10 direct stream API,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-07-19T16:01:21Z,2019-09-16T18:09:26Z,,NONE,"## What changes were proposed in this pull request?

This patch introduces messageHandler parameter that can be provided to Kafka DStream, which allows processing events received from Kafka at an early stage.

Lack of messageHandler parameter to KafkaUtils.createDirectStrem(...) in the new Kafka 10 API is what prevents us from upgrading our processes to use it, and here's why:

1. messageHandler() allowed parsing / filtering / projecting huge JSON files at an early stage (only a small subset of JSON fields is required for a process), without this current cluster configuration doesn't keep up with the traffic.
2. Transforming Kafka events right after a stream is created prevents from using HasOffsetRanges interface later. This means that whole message must be propagated to the end of a pipeline, which is very ineffective.

## How was this patch tested?

Unit tests are provided.
",spark,apache,spektom,158011,MDQ6VXNlcjE1ODAxMQ==,https://avatars2.githubusercontent.com/u/158011?v=4,,https://api.github.com/users/spektom,https://github.com/spektom,https://api.github.com/users/spektom/followers,https://api.github.com/users/spektom/following{/other_user},https://api.github.com/users/spektom/gists{/gist_id},https://api.github.com/users/spektom/starred{/owner}{/repo},https://api.github.com/users/spektom/subscriptions,https://api.github.com/users/spektom/orgs,https://api.github.com/users/spektom/repos,https://api.github.com/users/spektom/events{/privacy},https://api.github.com/users/spektom/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25205,https://github.com/apache/spark/pull/25205,https://github.com/apache/spark/pull/25205.diff,https://github.com/apache/spark/pull/25205.patch
236,https://api.github.com/repos/apache/spark/issues/25201,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25201/labels{/name},https://api.github.com/repos/apache/spark/issues/25201/comments,https://api.github.com/repos/apache/spark/issues/25201/events,https://github.com/apache/spark/pull/25201,470302995,MDExOlB1bGxSZXF1ZXN0Mjk5MzE2ODA2,25201,[SPARK-28419][SQL] Enable SparkThriftServer support proxy user's authentication .,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-07-19T12:37:08Z,2019-12-18T23:22:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

#25179
Since origin SparkThriftServer  can't proxy client user's authentication about hive and HDFS authentications.
this PR is to enable SparkThriftServer can proxy user's authorities.

For this pr, we first obtain HDFS token for current UGI, the it can truly proxy HDFS priority. Then for each proxy user, when we create hiveClient, we obtain a  Hive token for proxy user. Different user use different SparkSession.sharedState.
Then pass DFS toke to each Task, to proxy user's DFS behavior in Executors.


## How was this patch tested?

manual test",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25201,https://github.com/apache/spark/pull/25201,https://github.com/apache/spark/pull/25201.diff,https://github.com/apache/spark/pull/25201.patch
237,https://api.github.com/repos/apache/spark/issues/25198,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25198/labels{/name},https://api.github.com/repos/apache/spark/issues/25198/comments,https://api.github.com/repos/apache/spark/issues/25198/events,https://github.com/apache/spark/pull/25198,470103524,MDExOlB1bGxSZXF1ZXN0Mjk5MTU3ODkx,25198,[SPARK-28443][SQL] Spark sql add exception when create field type NullType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-07-19T02:12:44Z,2019-09-30T10:09:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

More detail see [25085](https://github.com/apache/spark/pull/25085).
This PR is to discuss details that `add exception when create field type NullType`

Now, it's ok to create table like:
```
val schema = new StructType().add(""c"", NullType)
spark.catalog.createTable(
          tableName = ""t"",
          source = ""json"",
          schema = schema,
          options = Map.empty[String, String])
```
But it's not the spark idea
## How was this patch tested?

UT
",spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars0.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25198,https://github.com/apache/spark/pull/25198,https://github.com/apache/spark/pull/25198.diff,https://github.com/apache/spark/pull/25198.patch
238,https://api.github.com/repos/apache/spark/issues/25180,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25180/labels{/name},https://api.github.com/repos/apache/spark/issues/25180/comments,https://api.github.com/repos/apache/spark/issues/25180/events,https://github.com/apache/spark/pull/25180,469287302,MDExOlB1bGxSZXF1ZXN0Mjk4NTUwMTA1,25180,[SPARK-28423][SQL] Merge Scan and Batch/Stream,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2019-07-17T15:27:03Z,2019-09-16T18:54:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

By design, `Scan` represents a logical data scan, `Batch`/`Stream` represents a physical data scan.

However, this doesn't match reality. The logical plan(`DataSourceV2Relation`) contains `Table` and the phyiscal plan(`BatchScanExec` and friends) contains `Scan`. The operator pushdown happens at planning time, so `Scan` and `Batch`/`Stream` are always created together in the planner rules. That said, `Table` is the actual logical data scan.

Since there is not much can be separated from `Scan` and `Batch`/`Stream`, almost all the existing DS v2 implementations either implement `Scan` and `Batch`/`Stream` together, or use anonymous class to implement `Scan`.

In addition, the write side API has no such separation either: it's just `WriterBuilder` -> `BatchWrite`/`StreamingWrite`.

This PR proposes to merge `Scan` and `Batch`/`Stream`, to match the write side API: `ScanBuilder` -> `BatchScan`/`MicroBatchScan`/`ContinuousScan`.

## How was this patch tested?

existing tests
",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25180,https://github.com/apache/spark/pull/25180,https://github.com/apache/spark/pull/25180.diff,https://github.com/apache/spark/pull/25180.patch
239,https://api.github.com/repos/apache/spark/issues/25118,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25118/labels{/name},https://api.github.com/repos/apache/spark/issues/25118/comments,https://api.github.com/repos/apache/spark/issues/25118/events,https://github.com/apache/spark/pull/25118,466951597,MDExOlB1bGxSZXF1ZXN0Mjk2Njk4NjY4,25118,[SPARK-27878][SQL] Support ARRAY(subquery) expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-07-11T15:08:56Z,2019-09-16T18:55:09Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR adds support for array creation from subquery so this example becomes valid:
```
select array(select c from t)
```

## How was this patch tested?

Added new UTs.
",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25118,https://github.com/apache/spark/pull/25118,https://github.com/apache/spark/pull/25118.diff,https://github.com/apache/spark/pull/25118.patch
240,https://api.github.com/repos/apache/spark/issues/25095,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25095/labels{/name},https://api.github.com/repos/apache/spark/issues/25095/comments,https://api.github.com/repos/apache/spark/issues/25095/events,https://github.com/apache/spark/pull/25095,466221234,MDExOlB1bGxSZXF1ZXN0Mjk2MTAyNTk0,25095,[SPARK-28332][SQL] SQLMetric wrong initValue,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-07-10T10:08:48Z,2019-10-26T23:08:29Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently SQLMetrics.createSizeMetric create a SQLMetric with initValue set to -1.

If there is a ShuffleMapStage with lots of Tasks which read 0 bytes data, these tasks will send the metric(the metric value still be the initValue with -1) to Driver,  then Driver do metric merge for this Stage in DAGScheduler.updateAccumulators, this will cause the merged metric value of this Stage set to be a negative value. 

This is incorrectÔºå we should set the initValue to 0 .

Another same case in SQLMetrics.createTimingMetric/createNanoTimingMetric

## How was this patch tested?
No more tests",spark,apache,windpiger,12979185,MDQ6VXNlcjEyOTc5MTg1,https://avatars3.githubusercontent.com/u/12979185?v=4,,https://api.github.com/users/windpiger,https://github.com/windpiger,https://api.github.com/users/windpiger/followers,https://api.github.com/users/windpiger/following{/other_user},https://api.github.com/users/windpiger/gists{/gist_id},https://api.github.com/users/windpiger/starred{/owner}{/repo},https://api.github.com/users/windpiger/subscriptions,https://api.github.com/users/windpiger/orgs,https://api.github.com/users/windpiger/repos,https://api.github.com/users/windpiger/events{/privacy},https://api.github.com/users/windpiger/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25095,https://github.com/apache/spark/pull/25095,https://github.com/apache/spark/pull/25095.diff,https://github.com/apache/spark/pull/25095.patch
241,https://api.github.com/repos/apache/spark/issues/25085,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25085/labels{/name},https://api.github.com/repos/apache/spark/issues/25085/comments,https://api.github.com/repos/apache/spark/issues/25085/events,https://github.com/apache/spark/pull/25085,465772594,MDExOlB1bGxSZXF1ZXN0Mjk1NzQ3NTA1,25085,[SPARK-28313][SQL] Spark sql null type incompatible with hive void type,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2019-07-09T13:11:38Z,2019-10-09T01:34:04Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

 [SPARK-20680](https://issues.apache.org/jira/browse/SPARK-20680), [PR](https://github.com/apache/spark/pull/17953), but actually this Jira was not solved. 

Spark is incompatible with hive void type. When table schema contains void type, spark throw exception in ddl option, like desc, show create table..

Also, spark catalog.createTable can create NullType Table that is not allowed.

Goal:

1. Support hive void column type for `desc`,`show create table` option
2. Add rule to throw exception when catalog.create NullType StructField, see [25198](https://github.com/apache/spark/pull/25198)

## How was this patch tested?

UT
",spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars0.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25085,https://github.com/apache/spark/pull/25085,https://github.com/apache/spark/pull/25085.diff,https://github.com/apache/spark/pull/25085.patch
242,https://api.github.com/repos/apache/spark/issues/25084,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25084/labels{/name},https://api.github.com/repos/apache/spark/issues/25084/comments,https://api.github.com/repos/apache/spark/issues/25084/events,https://github.com/apache/spark/pull/25084,465657734,MDExOlB1bGxSZXF1ZXN0Mjk1NjUzOTEw,25084,[SPARK-28314][SQL] Use the same MemoryManager when building HashedRelation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-07-09T08:58:38Z,2019-09-16T18:09:53Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Broadcasting some tables with a large number of rows may cause infinite recursion loop in ```TaskMemoryManager#allocatePage```.
This is because ```HashedRelation``` uses ```Long.MaxValue``` to construct ```MemoryManager``` instead of memory configured by driver.
```MemoryAllocator#allocate``` throws OOM,```TaskMemoryManager#allocatePage``` captures OOM and continues to call ```allocatePage```.

## How was this patch tested?
add UT
",spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars0.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25084,https://github.com/apache/spark/pull/25084,https://github.com/apache/spark/pull/25084.diff,https://github.com/apache/spark/pull/25084.patch
243,https://api.github.com/repos/apache/spark/issues/25078,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25078/labels{/name},https://api.github.com/repos/apache/spark/issues/25078/comments,https://api.github.com/repos/apache/spark/issues/25078/events,https://github.com/apache/spark/pull/25078,465341347,MDExOlB1bGxSZXF1ZXN0Mjk1NDAxOTk1,25078,[SPARK-28305][YARN] Request GetExecutorLossReason to use a smaller timeout parameter,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-07-08T16:03:36Z,2019-09-16T18:11:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Request GetExecutorLossReason to use a smaller timeout parameter.

In some cases, such as NM machine crashes or shuts down,driver ask ```GetExecutorLossReason```,
AM ```getCompletedContainersStatuses``` can't get the failure information of container.

Because the yarn NM detection timeout is 10 minutes, it is controlled by the parameter yarn.resourcemanager.rm.container-allocation.expiry-interval-ms.
So AM has to wait for 10 minutes to get the cause of the container failure.

Although the driver's ask fails, it will call recover.
However, due to the 2-minute timeout (spark.network.timeout) configured by ```IdleStateHandler```, the connection between driver and am is closed, AM exits, app finish, driver exits, causing the job to fail.

## How was this patch tested?
",spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars0.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25078,https://github.com/apache/spark/pull/25078,https://github.com/apache/spark/pull/25078.diff,https://github.com/apache/spark/pull/25078.patch
244,https://api.github.com/repos/apache/spark/issues/25064,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25064/labels{/name},https://api.github.com/repos/apache/spark/issues/25064/comments,https://api.github.com/repos/apache/spark/issues/25064/events,https://github.com/apache/spark/pull/25064,464928780,MDExOlB1bGxSZXF1ZXN0Mjk1MDg3OTY3,25064,[SPARK-28268][SQL] Rewrite non-correlated Semi/Anti join as Filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-07-07T08:34:33Z,2019-09-16T18:11:03Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When semi/anti join has a non-correlated join condition, we can convert it to a Filter with a non-correlated Exists subquery. As the Exists subquery is non-correlated, we can use a physical plan for it to avoid join.

Actually, this optimization is mainly for the non-correlated subqueries (Exists/In). We currently rewrite Exists/InSubquery as semi/anti/existential join, whether it is correlated or not. And they are mostly executed using a BroadcastNestedLoopJoin which is really not a good choice.

Here are some examples:
1. 
``` SQL
SELECT t1a
FROM    t1  
SEMI JOIN t2
ON t2a > 10 OR t2b = 'a'
```
=>
```SQL
SELECT t1a
FROM t1
WHERE EXISTS(SELECT 1 
             FROM t2 
             WHERE t2a > 10 OR t2b = 'a')
```
2.
```SQL
SELECT t1a
FROM  t1
ANTI JOIN t2
ON t1b > 10 AND t2b = 'b'
```
=>
```SQL
SELECT t1a
FROM t1
WHERE NOT(t1b > 10 
          AND EXISTS(SELECT 1
                     FROM  t2
                     WHERE t2b = 'b'))
```

This PR adds a new optimize rule : `ReplaceLeftSemiAntiJoinWithFilter`.
This rule replaces non-correlated `LeftSemi/LeftAnti Join` with `Filter`. When the `condition` of a semi/anti join can be split by `And` into expressions where each expression only refers to attributes from one side, we can turn it into a `Filter` with a non-correlated `Exists` subquery.

Besides, this PR adds a physical plan for non-correlated Exists.

## How was this patch tested?
ut
",spark,apache,francis0407,25431723,MDQ6VXNlcjI1NDMxNzIz,https://avatars1.githubusercontent.com/u/25431723?v=4,,https://api.github.com/users/francis0407,https://github.com/francis0407,https://api.github.com/users/francis0407/followers,https://api.github.com/users/francis0407/following{/other_user},https://api.github.com/users/francis0407/gists{/gist_id},https://api.github.com/users/francis0407/starred{/owner}{/repo},https://api.github.com/users/francis0407/subscriptions,https://api.github.com/users/francis0407/orgs,https://api.github.com/users/francis0407/repos,https://api.github.com/users/francis0407/events{/privacy},https://api.github.com/users/francis0407/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25064,https://github.com/apache/spark/pull/25064,https://github.com/apache/spark/pull/25064.diff,https://github.com/apache/spark/pull/25064.patch
245,https://api.github.com/repos/apache/spark/issues/25056,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25056/labels{/name},https://api.github.com/repos/apache/spark/issues/25056/comments,https://api.github.com/repos/apache/spark/issues/25056/events,https://github.com/apache/spark/pull/25056,464478250,MDExOlB1bGxSZXF1ZXN0Mjk0NzQ3NDg4,25056,[SPARK-28256][SS] Failed to initialize FileContextBasedCheckpointFileManager with uri without authority,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-07-05T06:13:07Z,2019-09-16T18:55:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

reproduce code:

```
CREATE TABLE `user_click_count` (`userId` STRING, `click` BIGINT)
USING org.apache.spark.sql.json
OPTIONS (path 'hdfs:///tmp/test');

INSERT INTO user_click_count SELECT xxx from ${streaming_source_table};
```

error:

```
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:136)
	at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:165)
	at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:250)
	at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:342)
	at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:339)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:456)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:297)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$.create(CheckpointFileManager.scala:189)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:63)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.<init>(CompactibleFileStreamLog.scala:46)
	at org.apache.spark.sql.execution.streaming.FileStreamSinkLog.<init>(FileStreamSinkLog.scala:85)
	at org.apache.spark.sql.execution.streaming.FileStreamSink.<init>(FileStreamSink.scala:98)
	at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:297)
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:379)

...

Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:134)
	... 67 more
Caused by: org.apache.hadoop.HadoopIllegalArgumentException: Uri without authority: hdfs:/tmp/test/_spark_metadata
	at org.apache.hadoop.fs.AbstractFileSystem.getUri(AbstractFileSystem.java:313)
	at org.apache.hadoop.fs.AbstractFileSystem.<init>(AbstractFileSystem.java:266)
	at org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:80)
	... 72 more
```

## How was this patch tested?

N/A",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25056,https://github.com/apache/spark/pull/25056,https://github.com/apache/spark/pull/25056.diff,https://github.com/apache/spark/pull/25056.patch
246,https://api.github.com/repos/apache/spark/issues/25053,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25053/labels{/name},https://api.github.com/repos/apache/spark/issues/25053/comments,https://api.github.com/repos/apache/spark/issues/25053/events,https://github.com/apache/spark/pull/25053,464302962,MDExOlB1bGxSZXF1ZXN0Mjk0NjExNDkx,25053,[SPARK-28252][SQL] local/global temp views should not accept duplicate column names,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-07-04T14:44:04Z,2019-09-16T18:55:08Z,,MEMBER,"## What changes were proposed in this pull request?

We disabled create table with duplicate column names:
https://github.com/apache/spark/blob/f9837d3bf6c15600f926917a253bf1bdddb4d1b4/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala#L247-L250

But we can create local/global temporary views with duplicate column names. This pr disabled create local/global temporary views with duplicate column names. Otherwise:
```shell
scala> spark.sql(""create temp view v1 as select 1 as col1, 2 as col1"")
res0: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""select col1 from v1"").show
19/07/04 22:27:19 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
org.apache.spark.sql.AnalysisException: Reference 'col1' is ambiguous, could be: v1.col1, v1.col1.; line 1 pos 7
  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)
```



## How was this patch tested?

unit tests
",spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25053,https://github.com/apache/spark/pull/25053,https://github.com/apache/spark/pull/25053.diff,https://github.com/apache/spark/pull/25053.patch
247,https://api.github.com/repos/apache/spark/issues/25035,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25035/labels{/name},https://api.github.com/repos/apache/spark/issues/25035/comments,https://api.github.com/repos/apache/spark/issues/25035/events,https://github.com/apache/spark/pull/25035,463407887,MDExOlB1bGxSZXF1ZXN0MjkzODkzMjQ5,25035,[SPARK-28235][SQL] Sum of decimals should return a decimal with MAX_PRECISION,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-07-02T19:54:23Z,2019-09-16T18:56:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Spark's decimal operations implementation follows what SQLServer does. This is not true for the `Sum` operation. In that case, SQLServer returns `DECIMAL(38, s)` where `s` is the scale of the input of the sum operator. Spark instead, uses as precision of the result type `p + 10` where `p` is the precision of the input type. This can cause overflows, which can be avoided with a higher precision: it happens in particular with sums of many decimals with a small precision.

## How was this patch tested?

changed UTs",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25035,https://github.com/apache/spark/pull/25035,https://github.com/apache/spark/pull/25035.diff,https://github.com/apache/spark/pull/25035.patch
248,https://api.github.com/repos/apache/spark/issues/25024,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25024/labels{/name},https://api.github.com/repos/apache/spark/issues/25024/comments,https://api.github.com/repos/apache/spark/issues/25024/events,https://github.com/apache/spark/pull/25024,462975777,MDExOlB1bGxSZXF1ZXN0MjkzNTQ4ODY2,25024,[SPARK-27296][SQL] User Defined Aggregators that do not ser/de on each input row,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,44,2019-07-02T01:20:34Z,2019-12-24T02:35:01Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Defines a new subclass of UDF: `UserDefinedAggregator`. Also allows `Aggregator` to be registered as a udf.  Under the hood, the implementation is based on the internal `TypedImperativeAggregate` class that spark's predefined aggregators make use of. The effect is that custom user defined aggregators are now serialized only on partition boundaries instead of being serialized and deserialized at each input row.

The two new modes of using `Aggregator` are as follows:
```scala
val agg: Aggregator[IN, BUF, OUT] = // typed aggregator
val udaf1 = UserDefinedAggregator(agg)
val udaf2 = spark.udf.register(""agg"", agg)
```

## How was this patch tested?
Unit testing has been added that corresponds to the testing suites for `UserDefinedAggregateFunction`. Additionally, unit tests explicitly count the number of aggregator ser/de cycles to ensure that it is governed only by the number of data partitions.

To evaluate the performance impact, I did two comparisons.
The code and REPL results are recorded on [this gist](https://gist.github.com/erikerlandson/b0e106a4dbaf7f80b4f4f3a21f05f892)
To characterize its behavior I benchmarked both a relatively simple aggregator and then an aggregator with a complex structure (a t-digest).

### performance
The following compares the new `Aggregator` based aggregation against UDAF. In this scenario, the new aggregation is about 100x faster. The difference in performance impact depends on the complexity of the aggregator. For very simple aggregators (e.g. implementing 'sum', etc), the performance impact is more like 25-30%.

```scala
scala> import scala.util.Random._, org.apache.spark.sql.Row, org.apache.spark.tdigest._
import scala.util.Random._
import org.apache.spark.sql.Row
import org.apache.spark.tdigest._

scala> val data = sc.parallelize(Vector.fill(50000){(nextInt(2), nextGaussian, nextGaussian.toFloat)}, 5).toDF(""cat"", ""x1"", ""x2"")
data: org.apache.spark.sql.DataFrame = [cat: int, x1: double ... 1 more field]

scala> val udaf = TDigestUDAF(0.5, 0)
udaf: org.apache.spark.tdigest.TDigestUDAF = TDigestUDAF(0.5,0)

scala> val bs = Benchmark.sample(10) { data.agg(udaf($""x1""), udaf($""x2"")).first }
bs: Array[(Double, org.apache.spark.sql.Row)] = Array((16.523,[TDigestSQL(TDigest(0.5,0,130,TDigestMap(-4.9171836327285225 -> (1.0, 1.0), -3.9615949140987685 -> (1.0, 2.0), -3.792874086327091 -> (0.7500781537109753, 2.7500781537109753), -3.720534874164185 -> (1.796754196108008, 4.546832349818983), -3.702105588052377 -> (0.4531676501810167, 5.0), -3.665883591332569 -> (2.3434687534153142, 7.343468753415314), -3.649982231368131 -> (0.6565312465846858, 8.0), -3.5914188829817744 -> (4.0, 12.0), -3.530472305581248 -> (4.0, 16.0), -3.4060489584449467 -> (2.9372251939818383, 18.93722519398184), -3.3000694035428486 -> (8.12412890252889, 27.061354096510726), -3.2250016655261877 -> (8.30564453211017, 35.3669986286209), -3.180537395623448 -> (6.001782561137285, 41.3687811...

scala> bs.map(_._1)
res0: Array[Double] = Array(16.523, 17.138, 17.863, 17.801, 17.769, 17.786, 17.744, 17.8, 17.939, 17.854)

scala> val agg = TDigestAggregator(0.5, 0)
agg: org.apache.spark.tdigest.TDigestAggregator = TDigestAggregator(0.5,0)

scala> val udaa = spark.udf.register(""tdigest"", agg)
udaa: org.apache.spark.sql.expressions.UserDefinedAggregator[Double,org.apache.spark.tdigest.TDigestSQL,org.apache.spark.tdigest.TDigestSQL] = UserDefinedAggregator(TDigestAggregator(0.5,0),None,true,true)

scala> val bs = Benchmark.sample(10) { data.agg(udaa($""x1""), udaa($""x2"")).first }
bs: Array[(Double, org.apache.spark.sql.Row)] = Array((0.313,[TDigestSQL(TDigest(0.5,0,130,TDigestMap(-4.9171836327285225 -> (1.0, 1.0), -3.9615949140987685 -> (1.0, 2.0), -3.792874086327091 -> (0.7500781537109753, 2.7500781537109753), -3.720534874164185 -> (1.796754196108008, 4.546832349818983), -3.702105588052377 -> (0.4531676501810167, 5.0), -3.665883591332569 -> (2.3434687534153142, 7.343468753415314), -3.649982231368131 -> (0.6565312465846858, 8.0), -3.5914188829817744 -> (4.0, 12.0), -3.530472305581248 -> (4.0, 16.0), -3.4060489584449467 -> (2.9372251939818383, 18.93722519398184), -3.3000694035428486 -> (8.12412890252889, 27.061354096510726), -3.2250016655261877 -> (8.30564453211017, 35.3669986286209), -3.180537395623448 -> (6.001782561137285, 41.36878118...

scala> bs.map(_._1)
res1: Array[Double] = Array(0.313, 0.193, 0.175, 0.185, 0.174, 0.176, 0.16, 0.186, 0.171, 0.179)

scala> 
```
",spark,apache,erikerlandson,259898,MDQ6VXNlcjI1OTg5OA==,https://avatars0.githubusercontent.com/u/259898?v=4,,https://api.github.com/users/erikerlandson,https://github.com/erikerlandson,https://api.github.com/users/erikerlandson/followers,https://api.github.com/users/erikerlandson/following{/other_user},https://api.github.com/users/erikerlandson/gists{/gist_id},https://api.github.com/users/erikerlandson/starred{/owner}{/repo},https://api.github.com/users/erikerlandson/subscriptions,https://api.github.com/users/erikerlandson/orgs,https://api.github.com/users/erikerlandson/repos,https://api.github.com/users/erikerlandson/events{/privacy},https://api.github.com/users/erikerlandson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25024,https://github.com/apache/spark/pull/25024,https://github.com/apache/spark/pull/25024.diff,https://github.com/apache/spark/pull/25024.patch
249,https://api.github.com/repos/apache/spark/issues/25022,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25022/labels{/name},https://api.github.com/repos/apache/spark/issues/25022/comments,https://api.github.com/repos/apache/spark/issues/25022/events,https://github.com/apache/spark/pull/25022,462616498,MDExOlB1bGxSZXF1ZXN0MjkzMjYxNDEz,25022,[SPARK-24695][SQL] Move `CalendarInterval` to org.apache.spark.sql.types package,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,61,2019-07-01T09:43:40Z,2019-10-24T10:18:50Z,,NONE,"## What changes were proposed in this pull request?

This change adds capability to return Calender interval type from udf by exposing `CalendarIntervalType` and `CalendarInterval`.

Earlier, the udf of Type `(String => CalendarInterval)` was throwing Exception stating:

```
Schema for type org.apache.spark.unsafe.types.CalendarInterval is not supported
java.lang.UnsupportedOperationException: Schema for type org.apache.spark.unsafe.types.CalendarInterval is not supported
at org.apache.spark.sql.catalyst.ScalaReflection391anonfun.apply(ScalaReflection.scala:781)
...
```

## How was this patch tested?

Added test case in `ScalaReflectionSuite.scala` and `ExpressionEncoderSuite`
Also, tested by creating an udf that returns Calendar interval.

jira entry for detail: https://issues.apache.org/jira/browse/SPARK-24695",spark,apache,priyankagargnitk,4745302,MDQ6VXNlcjQ3NDUzMDI=,https://avatars1.githubusercontent.com/u/4745302?v=4,,https://api.github.com/users/priyankagargnitk,https://github.com/priyankagargnitk,https://api.github.com/users/priyankagargnitk/followers,https://api.github.com/users/priyankagargnitk/following{/other_user},https://api.github.com/users/priyankagargnitk/gists{/gist_id},https://api.github.com/users/priyankagargnitk/starred{/owner}{/repo},https://api.github.com/users/priyankagargnitk/subscriptions,https://api.github.com/users/priyankagargnitk/orgs,https://api.github.com/users/priyankagargnitk/repos,https://api.github.com/users/priyankagargnitk/events{/privacy},https://api.github.com/users/priyankagargnitk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25022,https://github.com/apache/spark/pull/25022,https://github.com/apache/spark/pull/25022.diff,https://github.com/apache/spark/pull/25022.patch
250,https://api.github.com/repos/apache/spark/issues/25015,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/25015/labels{/name},https://api.github.com/repos/apache/spark/issues/25015/comments,https://api.github.com/repos/apache/spark/issues/25015/events,https://github.com/apache/spark/pull/25015,462339027,MDExOlB1bGxSZXF1ZXN0MjkzMDYwNzc2,25015,[SPARK-28217][SQL] Allow a pluggable statistics plan visitor for a logical plan.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-06-29T20:18:53Z,2019-09-16T18:56:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Spark currently has two built-in statistics plan visitor: `SizeInBytesOnlyStatsPlanVisitor` and `BasicStatsPlanVisitor`. However, this is a bit limited since there is no way to plug in a custom plan visitor from which a query optimizer can benefit.

This PR allowers the user to specify a custom stat plan visitor via a Spark conf to override the built-in one:

```Scala
// First create your custom stat plan visitor.
class MyStatsPlanVisitor extends LogicalPlanVisitor[Statistics] {
  // Implement LogicalPlanVisitor[Statistics] trait
}

// Set the visitor via Spark conf.
spark.conf.set(""spark.sql.catalyst.statsPlanVisitorClass"", ""MyStatsPlanVisitor"")

// Now, stat() on a LogicalPlan object will use MyStatsPlanVisitor as the stat plan visitor.
```

## How was this patch tested?
Existing and new unit tests.

cc: @rapoth",spark,apache,imback82,12103644,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/25015,https://github.com/apache/spark/pull/25015,https://github.com/apache/spark/pull/25015.diff,https://github.com/apache/spark/pull/25015.patch
251,https://api.github.com/repos/apache/spark/issues/24990,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24990/labels{/name},https://api.github.com/repos/apache/spark/issues/24990/comments,https://api.github.com/repos/apache/spark/issues/24990/events,https://github.com/apache/spark/pull/24990,461766145,MDExOlB1bGxSZXF1ZXN0MjkyNjE3MTc0,24990,[SPARK-28191][SS] New data source - state - reader part,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-06-27T21:53:59Z,2019-10-26T06:12:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Please refer [SPARK-28190](https://issues.apache.org/jira/browse/SPARK-28190) to refer rationalization of introducing new state data source.

This patch proposes introducing a new data source ""state"" on streaming query, and enable users' batch query to read state in checkpoint. The new data source is located in `sql-core` module - I didn't create a new module in external since state is not an external storage.

Given state itself has no schema information ([SPARK-27237](https://issues.apache.org/jira/browse/SPARK-27237) is addressing this), this patch includes some tool  (`StateSchemaExtractor`) to extract the schema of state from streaming query. It would be ideal to adopt SPARK-27237 and get rid of this tool.

State data source leverages existing state store APIs which would be compatible with any state store providers. That said, the data source is generic one, but could be target to specific state store provider to gain optimal performance. (on demand)

## How was this patch tested?

New UTs.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24990,https://github.com/apache/spark/pull/24990,https://github.com/apache/spark/pull/24990.diff,https://github.com/apache/spark/pull/24990.patch
252,https://api.github.com/repos/apache/spark/issues/24984,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24984/labels{/name},https://api.github.com/repos/apache/spark/issues/24984/comments,https://api.github.com/repos/apache/spark/issues/24984/events,https://github.com/apache/spark/pull/24984,461494180,MDExOlB1bGxSZXF1ZXN0MjkyNDA2MjEy,24984,[SPARK-28183][CORE][UI] Add a task status filter for taskList in REST API,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-06-27T12:15:07Z,2019-09-16T18:12:04Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

We have a scenario that our application needs to query failed tasks by REST API `/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList` when Spark job is running. In a large Stage, it may filter out dozens of failed tasks from hundred thousands total tasks. It consumes much unnecessary memory and time both in Spark and App side.

<del>This work splits to two PRs, the previous is #24982 (even though I commit them together here) since filter should be handled before pagination. (Assume 100 items per page, filtering after pagination may get four pages but each of page only contains several items. Actually one page could contains all the filtered item).</del>

## How was this patch tested?

Add UT.
And manually test:
(In local mode, there is only one running task by default.)
Before:
> http://localhost:4040/api/v1/applications/local-1562040123322/stages/3/0/taskList?offset=0&length=10

Return 1 running task and 9 success tasks or 10 success tasks.
After:
> http://localhost:4040/api/v1/applications/local-1562040123322/stages/3/0/taskList?offset=0&length=10&sortBy=status&status=RUNNING

Only return 1 running task or empty if all tasks completed.",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24984,https://github.com/apache/spark/pull/24984,https://github.com/apache/spark/pull/24984.diff,https://github.com/apache/spark/pull/24984.patch
253,https://api.github.com/repos/apache/spark/issues/24983,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24983/labels{/name},https://api.github.com/repos/apache/spark/issues/24983/comments,https://api.github.com/repos/apache/spark/issues/24983/events,https://github.com/apache/spark/pull/24983,461487464,MDExOlB1bGxSZXF1ZXN0MjkyNDAwNjY4,24983,[SPARK-27714][SQL][CBO] Support Genetic Algorithm based join reorder,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-06-27T11:59:18Z,2019-09-25T13:56:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Now the join reorder logic is based on dynamic planning which can find the most optimized plan theoretically, but the searching cost grows rapidly with the # of joined tables grows. This work introduce Genetic Algorithm (GA) based join reordering implementation in CBO.

## How was this patch tested?

New UTs is added.
",spark,apache,xianyinxin,15028683,MDQ6VXNlcjE1MDI4Njgz,https://avatars1.githubusercontent.com/u/15028683?v=4,,https://api.github.com/users/xianyinxin,https://github.com/xianyinxin,https://api.github.com/users/xianyinxin/followers,https://api.github.com/users/xianyinxin/following{/other_user},https://api.github.com/users/xianyinxin/gists{/gist_id},https://api.github.com/users/xianyinxin/starred{/owner}{/repo},https://api.github.com/users/xianyinxin/subscriptions,https://api.github.com/users/xianyinxin/orgs,https://api.github.com/users/xianyinxin/repos,https://api.github.com/users/xianyinxin/events{/privacy},https://api.github.com/users/xianyinxin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24983,https://github.com/apache/spark/pull/24983,https://github.com/apache/spark/pull/24983.diff,https://github.com/apache/spark/pull/24983.patch
254,https://api.github.com/repos/apache/spark/issues/24973,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24973/labels{/name},https://api.github.com/repos/apache/spark/issues/24973/comments,https://api.github.com/repos/apache/spark/issues/24973/events,https://github.com/apache/spark/pull/24973,460852193,MDExOlB1bGxSZXF1ZXN0MjkxODkyODA1,24973,"[SPARK-28169][SQL] Fix Partition table partition  PushDown failed by ""OR"" expression ","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-06-26T09:11:21Z,2019-09-16T18:10:58Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Spark can't push down filter condition of **Or**:

Such as if I have a table default.test, his partition col is ""dt"",

If we use query : 

`select * from default.test `
` where dt=20190625 or (dt = 20190626 and id in (1,2,3) )`

In this case, Spark will resolve **Or** condition as one expression, and since this expr has reference of ""id"", then it can't been push down.

In my PR ,  for SQL like 
`select * from default.test`
 `where  dt = 20190626  or  (dt = 20190626  and xxx="""")   `

For this **Or**  condition 
`or (dt = 20190626  or  (dt = 20190626  and xxx=""""   )`

All expression about partition keys will be  extracted as an expression only contains partition expression  
 and  retain the original logical relationship of **And** & **Or**  like below :
`dt = 20190626  or  dt = 20190626`

Then this condition will Passed to HiveTableScanExec. Such predicate expressions can be pushed down  as expected .

For this PR, it will extract deep relation of OR expression and push down this condition 
## How was this patch tested?
Exist unit test 
",spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24973,https://github.com/apache/spark/pull/24973,https://github.com/apache/spark/pull/24973.diff,https://github.com/apache/spark/pull/24973.patch
255,https://api.github.com/repos/apache/spark/issues/24939,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24939/labels{/name},https://api.github.com/repos/apache/spark/issues/24939/comments,https://api.github.com/repos/apache/spark/issues/24939/events,https://github.com/apache/spark/pull/24939,459569077,MDExOlB1bGxSZXF1ZXN0MjkwODg0ODE3,24939,"[SPARK-18569][ML][R] Support RFormula arithmetic, I() and spark functions","[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406606336, 'node_id': 'MDU6TGFiZWwxNDA2NjA2MzM2', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARKR', 'name': 'SPARKR', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2019-06-23T12:13:59Z,2019-11-27T14:38:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Added support for arithmetic expressions, use of spark functions and registered udf's inside formula so that expressions like this work as intended in `RFormula`;

`log(y) ~ a + pow(b, 2)`  ->  `log()` and `pow()` spark functions are used on `y` and `b`
`I(y +b) ~ a + x` -> The label term is sum of `y` and `b`

Udf's can also be used once they're registered:
```scala
  val registeredUdf = spark.udf.register(""plusTwo"", (x: Int) => (x + 2))
  val formula = new RFormula()
    .setFormula(""plusTwo(y) ~ a + plusTwo(b)"")

  val df = Seq((1, 4, 4), (2, 5, 6)).toDF(""y"", ""a"", ""b"")
  val model = formula.fit(df)
  model.transform(df).show()

+---+---+---+---------+-----+
|  y|  a|  b| features|label|
+---+---+---+---------+-----+
|  1|  4|  4|[4.0,6.0]|  3.0|
|  2|  5|  6|[5.0,8.0]|  4.0|
+---+---+---+---------+-----+

```
Summary of changes:
- Added `EvalExprParser` trait for parsing arithmetic expressions inside formula
- Added `ExprSelector` transformer which adds columns to a dataframe using `expr` spark function
- Add `ExprSelector` and `ColumnPruner` stages for parsed arithmetic expressions.

The criteria for parsing a term as arithmetic expression is that it's inside 'I(' expression ')', or it's a function (function criteria is ascii chars + alphanumeric chars + '(' args ')' ). From parsed arithmetic expressions, features are generated using `expr` spark function, so anything that can be used with `expr` function should be valid. Due to many ways `expr` function can be used, whether the expression has a valid syntax or whether the function is defined or not is not checked, only parsing rule is balanced parentheses.

## How was this patch tested?

Unit tests to RFormulaParserSuite and RFormulaSuite

@srowen @felixcheung ",spark,apache,ozancicek,11628913,MDQ6VXNlcjExNjI4OTEz,https://avatars1.githubusercontent.com/u/11628913?v=4,,https://api.github.com/users/ozancicek,https://github.com/ozancicek,https://api.github.com/users/ozancicek/followers,https://api.github.com/users/ozancicek/following{/other_user},https://api.github.com/users/ozancicek/gists{/gist_id},https://api.github.com/users/ozancicek/starred{/owner}{/repo},https://api.github.com/users/ozancicek/subscriptions,https://api.github.com/users/ozancicek/orgs,https://api.github.com/users/ozancicek/repos,https://api.github.com/users/ozancicek/events{/privacy},https://api.github.com/users/ozancicek/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24939,https://github.com/apache/spark/pull/24939,https://github.com/apache/spark/pull/24939.diff,https://github.com/apache/spark/pull/24939.patch
256,https://api.github.com/repos/apache/spark/issues/24938,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24938/labels{/name},https://api.github.com/repos/apache/spark/issues/24938/comments,https://api.github.com/repos/apache/spark/issues/24938/events,https://github.com/apache/spark/pull/24938,459550914,MDExOlB1bGxSZXF1ZXN0MjkwODczMzY1,24938,"[SPARK-27946][SQL] Hive DDL to Spark DDL conversion USING ""show create table""","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2019-06-23T08:35:36Z,2019-11-17T04:58:06Z,,MEMBER,"## What changes were proposed in this pull request?

This patch adds a DDL command `SHOW CREATE TABLE AS SPARK`. It is used to generate Spark DDL for a Hive table.

For Hive serde to data source conversion, this uses the existing mapping inside `HiveSerDe`. If can't find a mapping there, throws an analysis exception on unsupported serde configuration.

It is arguably that some Hive fileformat + row serde might be mapped to Spark data source, e.g., CSV. It is not included in this PR. To be conservative, it may not be supported.

For Hive serde properties, for now this doesn't save it to Spark DDL because it may not useful to keep Hive serde properties in Spark table.

## How was this patch tested?

Added test.",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24938,https://github.com/apache/spark/pull/24938,https://github.com/apache/spark/pull/24938.diff,https://github.com/apache/spark/pull/24938.patch
257,https://api.github.com/repos/apache/spark/issues/24936,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24936/labels{/name},https://api.github.com/repos/apache/spark/issues/24936/comments,https://api.github.com/repos/apache/spark/issues/24936/events,https://github.com/apache/spark/pull/24936,459253004,MDExOlB1bGxSZXF1ZXN0MjkwNjUyMzMx,24936,[SPARK-24634][SS] Add a new metric regarding number of rows later than watermark plus allowed delay,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,41,2019-06-21T15:26:51Z,2019-12-07T11:38:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Please refer https://issues.apache.org/jira/browse/SPARK-24634 to see rationalization of the issue.

This patch adds a new metric to count the number of rows arrived later than watermark plus allowed delay. To count the number of rows correctly, this patch adds a new physical node before stateful operator node(s) which counts the number of input rows later than watermark plus allowed delay. 

Note that it doesn't mean these rows will be always discarded, since there're some cases we don't discard late input rows, e.g. non-window streaming aggregation.

The metric will be exposed to two places:

* streaming query listener - numLateInputRows in stateOperators
* SQL tab in UI - `number of input rows later than watermark plus allowed delay` in CountLateRowsExec

This is a revised version of #21617.

## How was this patch tested?

Modified UT, and ran manual test reproducing [SPARK-28094](https://issues.apache.org/jira/browse/SPARK-28094).

I've picked the specific case on ""B outer C outer D"" which is enough to represent the ""intermediate late row"" issue due to global watermark.

https://gist.github.com/jammann/b58bfbe0f4374b89ecea63c1e32c8f17

Spark logs warning message on the query which means [SPARK-28074](https://issues.apache.org/jira/browse/SPARK-28074) is working correctly,

```
19/10/01 15:38:53 WARN UnsupportedOperationChecker: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are ""late rows"" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details.;
Project [B_ID#58 AS key#166, to_json(named_struct(B_ID, B_ID#58, B_LAST_MOD, B_LAST_MOD#59-T30000ms, C_FK, C_FK#60, D_FK, D_FK#61, C_ID, C_ID#91, C_LAST_MOD, C_LAST_MOD#92-T30000ms, D_ID, D_ID#120, D_LAST_MOD, D_LAST_MOD#121-T30000ms), Some(UTC)) AS value#167]
+- Join LeftOuter, ((D_FK#61 = D_ID#120) AND (B_LAST_MOD#59-T30000ms = D_LAST_MOD#121-T30000ms))
   :- Join LeftOuter, ((C_FK#60 = C_ID#91) AND (B_LAST_MOD#59-T30000ms = C_LAST_MOD#92-T30000ms))
   :  :- EventTimeWatermark B_LAST_MOD#59: timestamp, interval 30 seconds
   :  :  +- Project [v#56.B_ID AS B_ID#58, v#56.B_LAST_MOD AS B_LAST_MOD#59, v#56.C_FK AS C_FK#60, v#56.D_FK AS D_FK#61]
   :  :     +- Project [from_json(StructField(B_ID,StringType,false), StructField(B_LAST_MOD,TimestampType,false), StructField(C_FK,StringType,true), StructField(D_FK,StringType,true), value#54, Some(UTC)) AS v#56]
   :  :        +- Project [cast(value#41 as string) AS value#54]
   :  :           +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@5b3848b3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@63c5a407, org.apache.spark.sql.util.CaseInsensitiveStringMap@f2c4a44e, [key#40, value#41, topic#42, partition#43, offset#44L, timestamp#45, timestampType#46], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@69a2d6bd,kafka,List(),None,List(),None,Map(inferSchema -> true, startingOffsets -> latest, subscribe -> B, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#33, value#34, topic#35, partition#36, offset#37L, timestamp#38, timestampType#39]
   :  +- EventTimeWatermark C_LAST_MOD#92: timestamp, interval 30 seconds
   :     +- Project [v#89.C_ID AS C_ID#91, v#89.C_LAST_MOD AS C_LAST_MOD#92]
   :        +- Project [from_json(StructField(C_ID,StringType,false), StructField(C_LAST_MOD,TimestampType,false), value#87, Some(UTC)) AS v#89]
   :           +- Project [cast(value#74 as string) AS value#87]
   :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@79573aa5, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6bf1c0c, org.apache.spark.sql.util.CaseInsensitiveStringMap@f2c4a44f, [key#73, value#74, topic#75, partition#76, offset#77L, timestamp#78, timestampType#79], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@69a2d6bd,kafka,List(),None,List(),None,Map(inferSchema -> true, startingOffsets -> latest, subscribe -> C, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#66, value#67, topic#68, partition#69, offset#70L, timestamp#71, timestampType#72]
   +- EventTimeWatermark D_LAST_MOD#121: timestamp, interval 30 seconds
      +- Project [v#118.D_ID AS D_ID#120, v#118.D_LAST_MOD AS D_LAST_MOD#121]
         +- Project [from_json(StructField(D_ID,StringType,false), StructField(D_LAST_MOD,TimestampType,false), value#116, Some(UTC)) AS v#118]
            +- Project [cast(value#103 as string) AS value#116]
               +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6fa59acf, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3aa208b7, org.apache.spark.sql.util.CaseInsensitiveStringMap@f2c4a454, [key#102, value#103, topic#104, partition#105, offset#106L, timestamp#107, timestampType#108], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@69a2d6bd,kafka,List(),None,List(),None,Map(inferSchema -> true, startingOffsets -> latest, subscribe -> D, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#95, value#96, topic#97, partition#98, offset#99L, timestamp#100, timestampType#101]
```

and query plan represents CountLateRows with the number of late rows. Empty batch runs for batch 4 as follows:

![Screen Shot 2019-10-01 at 4 47 21 PM](https://user-images.githubusercontent.com/1317309/65944035-cd639580-e46b-11e9-83e0-bf8574cc6b8b.png)

The boxed node in the detail page for batch 4 represents there're 4 rows emitted from left side of join (B inner C) which rows are all later than watermark + allowed delay.

![Screen Shot 2019-10-01 at 16 50 06](https://user-images.githubusercontent.com/1317309/65944094-ec622780-e46b-11e9-9248-c821342722af.png)

The number of late rows are also reported as streaming query listener as follow (check `numLateInputRows`):

```
{
  ""id"" : ""7c5d10b5-1ae0-4a25-8b2b-f8b8eb4a4b6b"",
  ""runId"" : ""1f7375af-63b0-4272-9b87-4e59928d04d8"",
  ""name"" : ""B_outer_C_outer_D"",
  ""timestamp"" : ""2019-10-01T07:43:40.001Z"",
  ""batchId"" : 4,
  ""numInputRows"" : 0,
  ""inputRowsPerSecond"" : 0.0,
  ""processedRowsPerSecond"" : 0.0,
  ""durationMs"" : {
    ""addBatch"" : 1034,
    ""getBatch"" : 0,
    ""latestOffset"" : 14,
    ""queryPlanning"" : 459,
    ""triggerExecution"" : 1714,
    ""walCommit"" : 113
  },
  ""eventTime"" : {
    ""watermark"" : ""2019-06-01T18:36:58.769Z""
  },
  ""stateOperators"" : [ {
    ""numRowsTotal"" : 2,
    ""numRowsUpdated"" : 0,
    ""numLateInputRows"" : 4,
    ""memoryUsedBytes"" : 3710,
    ""customMetrics"" : {
      ""loadedMapCacheHitCount"" : 8,
      ""loadedMapCacheMissCount"" : 0,
      ""stateOnCurrentVersionSizeBytes"" : 702
    }
  }, {
    ""numRowsTotal"" : 5,
    ""numRowsUpdated"" : 0,
    ""numLateInputRows"" : 0,
    ""memoryUsedBytes"" : 7453,
    ""customMetrics"" : {
      ""loadedMapCacheHitCount"" : 8,
      ""loadedMapCacheMissCount"" : 0,
      ""stateOnCurrentVersionSizeBytes"" : 1485
    }
  } ],
  ""sources"" : [ {
    ""description"" : ""KafkaV2[Subscribe[B]]"",
    ""startOffset"" : {
      ""B"" : {
        ""2"" : 10,
        ""4"" : 10,
        ""1"" : 10,
        ""3"" : 10,
        ""0"" : 10
      }
    },
    ""endOffset"" : {
      ""B"" : {
        ""2"" : 10,
        ""4"" : 10,
        ""1"" : 10,
        ""3"" : 10,
        ""0"" : 10
      }
    },
    ""numInputRows"" : 0,
    ""inputRowsPerSecond"" : 0.0,
    ""processedRowsPerSecond"" : 0.0
  }, {
    ""description"" : ""KafkaV2[Subscribe[C]]"",
    ""startOffset"" : {
      ""C"" : {
        ""2"" : 3,
        ""4"" : 3,
        ""1"" : 2,
        ""3"" : 3,
        ""0"" : 4
      }
    },
    ""endOffset"" : {
      ""C"" : {
        ""2"" : 3,
        ""4"" : 3,
        ""1"" : 2,
        ""3"" : 3,
        ""0"" : 4
      }
    },
    ""numInputRows"" : 0,
    ""inputRowsPerSecond"" : 0.0,
    ""processedRowsPerSecond"" : 0.0
  }, {
    ""description"" : ""KafkaV2[Subscribe[D]]"",
    ""startOffset"" : {
      ""D"" : {
        ""2"" : 0,
        ""4"" : 2,
        ""1"" : 2,
        ""3"" : 1,
        ""0"" : 0
      }
    },
    ""endOffset"" : {
      ""D"" : {
        ""2"" : 0,
        ""4"" : 2,
        ""1"" : 2,
        ""3"" : 1,
        ""0"" : 0
      }
    },
    ""numInputRows"" : 0,
    ""inputRowsPerSecond"" : 0.0,
    ""processedRowsPerSecond"" : 0.0
  } ],
  ""sink"" : {
    ""description"" : ""org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@11edae5f"",
    ""numOutputRows"" : 2
  }
}
```",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24936,https://github.com/apache/spark/pull/24936,https://github.com/apache/spark/pull/24936.diff,https://github.com/apache/spark/pull/24936.patch
258,https://api.github.com/repos/apache/spark/issues/24896,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24896/labels{/name},https://api.github.com/repos/apache/spark/issues/24896/comments,https://api.github.com/repos/apache/spark/issues/24896/events,https://github.com/apache/spark/pull/24896,457174667,MDExOlB1bGxSZXF1ZXN0Mjg5MDQxNjk1,24896,[WIP][SPARK-28006] User-defined grouped transform pandas_udf for window operations,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-06-17T22:22:01Z,2019-10-29T07:27:45Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, pandas_udf supports ""grouped aggregate"" type that can be used with unbounded and unbounded windows. There is another set of use cases that can benefit from a ""grouped transform"" type pandas_udf.

Grouped transform is defined as a N -> N mapping over a group. For example, ""compute zscore for values in the group using the grouped mean and grouped stdev"", or ""rank the values in the group"".

Currently, in order to do this, user needs to use ""grouped apply"", for example:
```
@pandas_udf(schema, GROUPED_MAP)
def subtract_mean(pdf)
    v = pdf['v']
    pdf['v'] = v - v.mean()
    return pdf

df.groupby('id').apply(subtract_mean)
# +---+----+
# | id|   v|
# +---+----+
# |  1|-0.5|
# |  1| 0.5|
# |  2|-3.0|
# |  2|-1.0|
# |  2| 4.0|
# +---+----+
```
This approach has a few downside:

Specifying the full return schema is complicated for the user although the function only changes one column.
* The column name 'v' inside as part of the udf, makes the udf less reusable.
* The entire dataframe is serialized to pass to Python although only one column is needed.
* Here we propose a new type of pandas_udf to work with these types of use cases:

Here we propose a new type of pandas_udf to work with these types of use cases:
```
df = spark.createDataFrame(
    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],
    (""id"", ""v""))

@pandas_udf('double', GROUPED_XFORM)
def subtract_mean(v):
    return v - v.mean()

w = Window.partitionBy('id')

df = df.withColumn('v', subtract_mean(df['v']).over(w))
# +---+----+
# | id|   v|
# +---+----+
# |  1|-0.5|
# |  1| 0.5|
# |  2|-3.0|
# |  2|-1.0|
# |  2| 4.0|
# +---+----+
```
Which addresses the above downsides.

* The user only needs to specify the output type of a single column.
* The column being zscored is decoupled from the udf implementation
* We only need to send one column to Python worker and concat the result with the original dataframe (this is what grouped aggregate is doing already)

This is similar to groupby transform in pandas, hence the name ""grouped transform""

```
>>> df = pd.DataFrame({'id': [1, 1, 2, 2, 2], 'value': [1., 2., 3., 5., 10.]})

>>> df
   id  value

0   1    1.0
1   1    2.0
2   2    3.0
3   2    5.0
4   2   10.0

>>> df['value_demean'] = df.groupby('id')['value'].transform(lambda x: x - x.mean())
>>> df

   id  value  value_demean
0   1    1.0          -0.5
1   1    2.0           0.5
2   2    3.0          -3.0
3   2    5.0          -1.0
4   2   10.0           4.0
```

## How was this patch tested?

Add new tests in test_pandas_udf_window",spark,apache,icexelloss,793516,MDQ6VXNlcjc5MzUxNg==,https://avatars0.githubusercontent.com/u/793516?v=4,,https://api.github.com/users/icexelloss,https://github.com/icexelloss,https://api.github.com/users/icexelloss/followers,https://api.github.com/users/icexelloss/following{/other_user},https://api.github.com/users/icexelloss/gists{/gist_id},https://api.github.com/users/icexelloss/starred{/owner}{/repo},https://api.github.com/users/icexelloss/subscriptions,https://api.github.com/users/icexelloss/orgs,https://api.github.com/users/icexelloss/repos,https://api.github.com/users/icexelloss/events{/privacy},https://api.github.com/users/icexelloss/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24896,https://github.com/apache/spark/pull/24896,https://github.com/apache/spark/pull/24896.diff,https://github.com/apache/spark/pull/24896.patch
259,https://api.github.com/repos/apache/spark/issues/24888,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24888/labels{/name},https://api.github.com/repos/apache/spark/issues/24888/comments,https://api.github.com/repos/apache/spark/issues/24888/events,https://github.com/apache/spark/pull/24888,456733357,MDExOlB1bGxSZXF1ZXN0Mjg4Njg4Mzg3,24888,[SPARK-28040][SPARK-28070][R] Write type object s3,"[{'id': 1406606336, 'node_id': 'MDU6TGFiZWwxNDA2NjA2MzM2', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARKR', 'name': 'SPARKR', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-06-17T04:10:39Z,2019-09-16T19:30:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Follow-on to https://github.com/apache/spark/pull/24885 -- implement `writeType` and `writeObject` as S3 methods

## How was this patch tested?

Untested; please advise

Please review https://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,MichaelChirico,7606389,MDQ6VXNlcjc2MDYzODk=,https://avatars1.githubusercontent.com/u/7606389?v=4,,https://api.github.com/users/MichaelChirico,https://github.com/MichaelChirico,https://api.github.com/users/MichaelChirico/followers,https://api.github.com/users/MichaelChirico/following{/other_user},https://api.github.com/users/MichaelChirico/gists{/gist_id},https://api.github.com/users/MichaelChirico/starred{/owner}{/repo},https://api.github.com/users/MichaelChirico/subscriptions,https://api.github.com/users/MichaelChirico/orgs,https://api.github.com/users/MichaelChirico/repos,https://api.github.com/users/MichaelChirico/events{/privacy},https://api.github.com/users/MichaelChirico/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24888,https://github.com/apache/spark/pull/24888,https://github.com/apache/spark/pull/24888.diff,https://github.com/apache/spark/pull/24888.patch
260,https://api.github.com/repos/apache/spark/issues/24885,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24885/labels{/name},https://api.github.com/repos/apache/spark/issues/24885/comments,https://api.github.com/repos/apache/spark/issues/24885/events,https://github.com/apache/spark/pull/24885,456647530,MDExOlB1bGxSZXF1ZXN0Mjg4NjI4NTgz,24885,[SPARK-28040][R] Add serialization for glue type,"[{'id': 1406606336, 'node_id': 'MDU6TGFiZWwxNDA2NjA2MzM2', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARKR', 'name': 'SPARKR', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2019-06-16T14:31:44Z,2019-09-18T11:51:47Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

`glue`-class string can be properly used in `sql` (it's really just a `character`)

## How was this patch tested?

`glue` package creates decorated `character` objects:

```
# from ?glue::glue
name <- ""Fred""
age <- 50
anniversary <- as.Date(""1991-10-12"")
glue_obj = glue('My name is {name},',
  'my age next year is {age + 1},',
  'my anniversary is {format(anniversary, ""%A, %B %d, %Y"")}.')
dput(glue_obj)
# structure(""My name is Fred,my age next year is 51,my anniversary is Saturday, October 12, 1991."", class = c(""glue"", ""character""))
```

i.e., it's just `character` with an extra `class`.

`writeObject` and `getSerdeType` both rely on the first class of the input: `type <- class(object)[[1L]]`. Hence for `glue` objects, `type` will be `'glue'`.

Then the `switch` statement will e.g. work like:

```
switch('glue',
         NULL = writeVoid(con),
         integer = writeInt(con, object),
         glue = , # nolint
         character = 'matched to character')
# [1] ""matched to character""
```

WIP PR at #24888 will provide a much more general fix to the `glue`-specific problem solved in this PR, but as noted in the comments, `glue` is a pretty common package so this is likely to serve most related issues with a much simpler fix.",spark,apache,MichaelChirico,7606389,MDQ6VXNlcjc2MDYzODk=,https://avatars1.githubusercontent.com/u/7606389?v=4,,https://api.github.com/users/MichaelChirico,https://github.com/MichaelChirico,https://api.github.com/users/MichaelChirico/followers,https://api.github.com/users/MichaelChirico/following{/other_user},https://api.github.com/users/MichaelChirico/gists{/gist_id},https://api.github.com/users/MichaelChirico/starred{/owner}{/repo},https://api.github.com/users/MichaelChirico/subscriptions,https://api.github.com/users/MichaelChirico/orgs,https://api.github.com/users/MichaelChirico/repos,https://api.github.com/users/MichaelChirico/events{/privacy},https://api.github.com/users/MichaelChirico/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24885,https://github.com/apache/spark/pull/24885,https://github.com/apache/spark/pull/24885.diff,https://github.com/apache/spark/pull/24885.patch
261,https://api.github.com/repos/apache/spark/issues/24848,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24848/labels{/name},https://api.github.com/repos/apache/spark/issues/24848/comments,https://api.github.com/repos/apache/spark/issues/24848/events,https://github.com/apache/spark/pull/24848,455064901,MDExOlB1bGxSZXF1ZXN0Mjg3Mzg4MTQx,24848,[SPARK-28014][core] All waiting apps will be changed to the wrong state of Running after master changed.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-06-12T07:56:52Z,2019-09-16T18:13:01Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

These waiting apps which granted 0 cores will be changed to Running state after master changed, that is a little weird.

currently:
![image](https://user-images.githubusercontent.com/24823338/59333239-828e0880-8d2a-11e9-84ae-478fe118c5ee.png)

fixed:
![image](https://user-images.githubusercontent.com/24823338/59333330-ad785c80-8d2a-11e9-9587-45b8f63691fc.png)




 

## How was this patch tested?

update tests
",spark,apache,zuotingbing,24823338,MDQ6VXNlcjI0ODIzMzM4,https://avatars2.githubusercontent.com/u/24823338?v=4,,https://api.github.com/users/zuotingbing,https://github.com/zuotingbing,https://api.github.com/users/zuotingbing/followers,https://api.github.com/users/zuotingbing/following{/other_user},https://api.github.com/users/zuotingbing/gists{/gist_id},https://api.github.com/users/zuotingbing/starred{/owner}{/repo},https://api.github.com/users/zuotingbing/subscriptions,https://api.github.com/users/zuotingbing/orgs,https://api.github.com/users/zuotingbing/repos,https://api.github.com/users/zuotingbing/events{/privacy},https://api.github.com/users/zuotingbing/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24848,https://github.com/apache/spark/pull/24848,https://github.com/apache/spark/pull/24848.diff,https://github.com/apache/spark/pull/24848.patch
262,https://api.github.com/repos/apache/spark/issues/24840,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24840/labels{/name},https://api.github.com/repos/apache/spark/issues/24840/comments,https://api.github.com/repos/apache/spark/issues/24840/events,https://github.com/apache/spark/pull/24840,454604158,MDExOlB1bGxSZXF1ZXN0Mjg3MDE5Njc0,24840,[SPARK-27998][SQL] Column alias should support quote string,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-06-11T10:19:11Z,2019-09-17T00:01:50Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

According to the ANSI SQL standard, column alias can be double quote string but SparkSQL only support backquote string.
```sql 
SELECT au_fname AS ""First name"", 
au_lname AS 'Last name', 
city AS City, 
state, 
zip 'Postal code' FROM authors;
 ```

However, SparkSQL's syntax is different from others DB engines.

- MySQL support Backquote(``), Single quote (''), Double quote("""").
- SQL Server support Single quote (''), Double quote("""").
- Teradata, Oracle, PostgreSQL only support  Double quote("""").


## How was this patch tested?

Pass the Jenkins with the updated test cases.",spark,apache,lipzhu,698621,MDQ6VXNlcjY5ODYyMQ==,https://avatars3.githubusercontent.com/u/698621?v=4,,https://api.github.com/users/lipzhu,https://github.com/lipzhu,https://api.github.com/users/lipzhu/followers,https://api.github.com/users/lipzhu/following{/other_user},https://api.github.com/users/lipzhu/gists{/gist_id},https://api.github.com/users/lipzhu/starred{/owner}{/repo},https://api.github.com/users/lipzhu/subscriptions,https://api.github.com/users/lipzhu/orgs,https://api.github.com/users/lipzhu/repos,https://api.github.com/users/lipzhu/events{/privacy},https://api.github.com/users/lipzhu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24840,https://github.com/apache/spark/pull/24840,https://github.com/apache/spark/pull/24840.diff,https://github.com/apache/spark/pull/24840.patch
263,https://api.github.com/repos/apache/spark/issues/24839,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24839/labels{/name},https://api.github.com/repos/apache/spark/issues/24839/comments,https://api.github.com/repos/apache/spark/issues/24839/events,https://github.com/apache/spark/pull/24839,454519382,MDExOlB1bGxSZXF1ZXN0Mjg2OTUyMjQ1,24839,[SPARK-27258][K8S]Deal with the k8s resource names that don't match their own regular expression,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-06-11T07:18:36Z,2019-09-27T10:47:33Z,,NONE,"
## What changes were proposed in this pull request?
The regex of Pod is ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*.
The regex of Servive is ^[a-z]([-a-z0-9]*[a-z0-9])?.
The regex of Secret is ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*.

If --name=-min, the first character is '-' ,which does not satisfy the regex for Pod ,Service,Secret.
if --name=1min, the first character is digit, which does not satisfy the regex for Service.

[Previous Conflict PR](https://github.com/apache/spark/pull/24219)

",spark,apache,hehuiyuan,18002496,MDQ6VXNlcjE4MDAyNDk2,https://avatars0.githubusercontent.com/u/18002496?v=4,,https://api.github.com/users/hehuiyuan,https://github.com/hehuiyuan,https://api.github.com/users/hehuiyuan/followers,https://api.github.com/users/hehuiyuan/following{/other_user},https://api.github.com/users/hehuiyuan/gists{/gist_id},https://api.github.com/users/hehuiyuan/starred{/owner}{/repo},https://api.github.com/users/hehuiyuan/subscriptions,https://api.github.com/users/hehuiyuan/orgs,https://api.github.com/users/hehuiyuan/repos,https://api.github.com/users/hehuiyuan/events{/privacy},https://api.github.com/users/hehuiyuan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24839,https://github.com/apache/spark/pull/24839,https://github.com/apache/spark/pull/24839.diff,https://github.com/apache/spark/pull/24839.patch
264,https://api.github.com/repos/apache/spark/issues/24807,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24807/labels{/name},https://api.github.com/repos/apache/spark/issues/24807/comments,https://api.github.com/repos/apache/spark/issues/24807/events,https://github.com/apache/spark/pull/24807,452586107,MDExOlB1bGxSZXF1ZXN0Mjg1NDQ0MDc3,24807,[SPARK-27958][SQL] Stopping a SparkSession should not always stop Spark Context,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-06-05T16:02:06Z,2019-09-16T18:12:03Z,,NONE,"## What changes were proposed in this pull request?

This change changes the current behavior where stopping a `SparkSession` stops the underlying `SparkContext` to only stopping the `SparkContext` if there are no remaining `SparkSession`s. 

## How was this patch tested?

* Unit test included as a part of the PR 

Please review https://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,vinooganesh,2461070,MDQ6VXNlcjI0NjEwNzA=,https://avatars0.githubusercontent.com/u/2461070?v=4,,https://api.github.com/users/vinooganesh,https://github.com/vinooganesh,https://api.github.com/users/vinooganesh/followers,https://api.github.com/users/vinooganesh/following{/other_user},https://api.github.com/users/vinooganesh/gists{/gist_id},https://api.github.com/users/vinooganesh/starred{/owner}{/repo},https://api.github.com/users/vinooganesh/subscriptions,https://api.github.com/users/vinooganesh/orgs,https://api.github.com/users/vinooganesh/repos,https://api.github.com/users/vinooganesh/events{/privacy},https://api.github.com/users/vinooganesh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24807,https://github.com/apache/spark/pull/24807,https://github.com/apache/spark/pull/24807.diff,https://github.com/apache/spark/pull/24807.patch
265,https://api.github.com/repos/apache/spark/issues/24801,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24801/labels{/name},https://api.github.com/repos/apache/spark/issues/24801/comments,https://api.github.com/repos/apache/spark/issues/24801/events,https://github.com/apache/spark/pull/24801,452276184,MDExOlB1bGxSZXF1ZXN0Mjg1MTk4Mjky,24801,[SPARK-27950][DSTREAMS][Kinesis] dynamoDBEndpointUrl and cloudWatchMetricsLevel for Kinesis,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-06-05T01:30:50Z,2019-09-16T18:12:09Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

I was researching getting Spark‚Äôs Kinesis integration running locally against `localstack`. We found this issue, and it creates a complication: https://github.com/localstack/localstack/issues/677

Effectively, we need to be able to redirect calls for Kinesis, DynamoDB and Cloudwatch in order for the KCL to properly use the `localstack` infrastructure. We have successfully done this with the KCL (both 1.x and 2.x), but with Spark‚Äôs integration we are unable to configure DynamoDB and Cloudwatch‚Äôs endpoints:

https://github.com/apache/spark/blob/master/external/kinesis-asl/src/main/scala/org/apache/spark/streaming/kinesis/KinesisReceiver.scala#L162

This PR adds optional configuration values to the interfaces for dynamoDBEndpointUrl and cloudWatchMetricsLevel. 

Why cloudWatchMetricsLevel instead of cloudWatchEndpointUrl? Because the 1.x version of the KCL does not expose a means of configuring the cloudWatchEndpointUrl. Localstack users can instead disable metrics entirely by setting the cloudWatchMetricsLevel to Some(MetricsLevel.NONE)

## How was this patch tested?

Existing unit tests were expanded to check that these values were set.

Please review https://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,etspaceman,630953,MDQ6VXNlcjYzMDk1Mw==,https://avatars1.githubusercontent.com/u/630953?v=4,,https://api.github.com/users/etspaceman,https://github.com/etspaceman,https://api.github.com/users/etspaceman/followers,https://api.github.com/users/etspaceman/following{/other_user},https://api.github.com/users/etspaceman/gists{/gist_id},https://api.github.com/users/etspaceman/starred{/owner}{/repo},https://api.github.com/users/etspaceman/subscriptions,https://api.github.com/users/etspaceman/orgs,https://api.github.com/users/etspaceman/repos,https://api.github.com/users/etspaceman/events{/privacy},https://api.github.com/users/etspaceman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24801,https://github.com/apache/spark/pull/24801,https://github.com/apache/spark/pull/24801.diff,https://github.com/apache/spark/pull/24801.patch
266,https://api.github.com/repos/apache/spark/issues/24796,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24796/labels{/name},https://api.github.com/repos/apache/spark/issues/24796/comments,https://api.github.com/repos/apache/spark/issues/24796/events,https://github.com/apache/spark/pull/24796,452203361,MDExOlB1bGxSZXF1ZXN0Mjg1MTQwNTIy,24796,[SPARK-27900][CORE] Add uncaught exception handler to the driver,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2019-06-04T20:54:17Z,2019-09-16T19:00:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
In case of jvm errors the driver will not exit properly as there is no UnCaughtException handler. This causes issues when Spark is run in a container, as jvm will not exit due to still running threads, errors  are not propagated to K8s runtime and pods will run forever. This also makes impossible for the Spark Operator to report back a failed job.
As described in the related jira jvm errors may cause deadlocks and we cannot assume a healthy jvm
to do a proper shutdown, so we avoid the registred shutdown hooks, it is the equivalent of setting `-XX:+ExitOnOutOfMemoryError`. For example the DAG event loop thread is a daemon thread and in the scenario described in the jira becomes unresponsive while the main thread also is stuck in the `runJob()` method waiting forever to make a job submission. However, this PR does not change the logic for the handler for the master, workers in standalone mode and the Spark executors. It only adds a special behavior for the driver where we exit immediately. 

## How was this patch tested?

Manually by running a Spark Job.
This will make the job example in the jira fail and the container will exit with:
```
State:          Terminated
  Reason:       Error
  Exit Code:    52
```",spark,apache,skonto,7945591,MDQ6VXNlcjc5NDU1OTE=,https://avatars1.githubusercontent.com/u/7945591?v=4,,https://api.github.com/users/skonto,https://github.com/skonto,https://api.github.com/users/skonto/followers,https://api.github.com/users/skonto/following{/other_user},https://api.github.com/users/skonto/gists{/gist_id},https://api.github.com/users/skonto/starred{/owner}{/repo},https://api.github.com/users/skonto/subscriptions,https://api.github.com/users/skonto/orgs,https://api.github.com/users/skonto/repos,https://api.github.com/users/skonto/events{/privacy},https://api.github.com/users/skonto/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24796,https://github.com/apache/spark/pull/24796,https://github.com/apache/spark/pull/24796.diff,https://github.com/apache/spark/pull/24796.patch
267,https://api.github.com/repos/apache/spark/issues/24792,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24792/labels{/name},https://api.github.com/repos/apache/spark/issues/24792/comments,https://api.github.com/repos/apache/spark/issues/24792/events,https://github.com/apache/spark/pull/24792,451918886,MDExOlB1bGxSZXF1ZXN0Mjg0OTEwNjA2,24792,[SPARK-27953][SQL] Save default constraint with Column into table properties when create Hive table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2019-06-04T10:30:01Z,2019-12-03T12:50:18Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
**Background**
Default constraint with column is ANSI standard.
Hive 3.0+ has supported default constraint ref:https://issues.apache.org/jira/browse/HIVE-18726
But Spark SQL implement this feature not yet.

**Design**
Hive is widely used in production environments and is the standard in the field of big data in fact. 
But Hive exists many version used in production and the feature between each version are different.

Spark SQL need to implement default constraint, but there are three points to pay attention to in design:
First, Spark SQL should reduce coupling with Hive. 
Second, default constraint could compatible with different versions of Hive.
Thrid, Which expression of default constraint should Spark SQL support? I think should support `literal`, `current_date()`, `current_timestamp()`. Maybe other expression should also supported, like `Cast(1 as float)`, `1 + 2` and so on.

We want to save the metadata of default constraint into properties of Hive table, and then we restore metadata from the properties after client gets newest metadata.The implement is the same as other metadata (e.g. partition,bucket,statistics).

Because default constraint is part of column, so I think could reuse the metadata of StructField. The default constraint will cached by metadata of StructField.

**Detail of this PR**
This is a sub task to implement default constraint.
This PR will solve the issue that save default constraint into properties of Hive table or data source table.

There exists some issue in this PR:
First, how to check a number specified by somebody compliance with the accuracy and scope of the data type, like float, double.
Second, some code looks not very elegant, I hope to improve it with your suggestions.

**Brother PR**
This PR is related to [https://github.com/apache/spark/pull/24372](24372). If this PR finish, unselected target column can be inserted into the default value, while running `insert into`.
After this PR,  I will continue open other PR about default constraint, like alter table, desc table.

## How was this patch tested?

UT
",spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24792,https://github.com/apache/spark/pull/24792,https://github.com/apache/spark/pull/24792.diff,https://github.com/apache/spark/pull/24792.patch
268,https://api.github.com/repos/apache/spark/issues/24765,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24765/labels{/name},https://api.github.com/repos/apache/spark/issues/24765/comments,https://api.github.com/repos/apache/spark/issues/24765/events,https://github.com/apache/spark/pull/24765,451110144,MDExOlB1bGxSZXF1ZXN0Mjg0Mjg4MDc2,24765,[SPARK-27915][SQL][WIP] Update logical Filter's output nullability based on IsNotNull conditions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-06-01T18:04:56Z,2019-09-16T19:00:10Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR changes the logical `Filter` operator to update its outputs' nullability when filter conditions imply that outputs cannot be null. 

In addition, I refined similar existing logic in the physical `FilterExec` (changing the existing code to be more precise / less conservative in its non-nullability inference) and improved propagation of inferred nullability information in `Project`.

This is useful because of how it composes with other optimizations: Spark has several logical and physical optimizations which leverage non-nullability, so improving nullability inference increases the value of those existing optimizations.

:warning: **Disclaimers** :warning:

 - This is a work-in-progress / skunkworks side project; I'm not working on this full time.
 - Nullability has been a major source of bugs in the past: this PR requires careful review.
 - I haven't run analyzer / optimizer performance benchmarks, so there's a decent chance that this WIP changeset regresses query planning performance.
 - DataFrames / Datasets / queries' `.schema` may change as a result of this optimization: this may have consequences in case nullability information is used by downstream systems (e.g. for `CREATE TABLE` DDL).
 - The schemas of analyzed and optimized logical plans may now differ in terms of field nullability (because optimization might infer additional constraints which allow us to prove that fields are non-null).
 
### Examples

Consider the query

```
SELECT key
FROM t
WHERE key IS NOT NULL
```

where `t.key` is nullable.

Because of the `key IS NOT NULL` filter condition, `key` will always be non-null. Prior to this patch, this query's result schema was overly-conservative, continuing to mark  `key` as nullable. However, if we take advantage of the `key IS NOT NULL` condition we can set `nullable = false` for `key`.

This was a somewhat trivial example, so let's look at some more complicated cases:

Consider 

```
SELECT A.key, A.value
FROM A, B
WHERE
    A.key = B.key AND
    (A.num + B.num) > 0
```

where all columns of `A` and `B` are nullable. Because of the equality join condition, we know that `key` must be non-null in both tables. In addition, the condition `(A.num + B.num) > 0` can only hold if both `num` values are not null: addition is a _null-intolerant_ operator, meaning that it returns `null` if any of its operands is null.

Leveraging this, we should be able to mark both `key` and `value` as non-null in the join result's schema (even though both values are nullable in the underlying input relation).

Finally, let's look at an example of a **non** null-intolerant operator: `coalesce(a, b) IS NOT NULL` could still mean that `a` or `b` is null, so in

```
SELECT key, foo, COALESCE(foo, bar) as qux
FROM A
WHERE COALESCE(foo, bar) > 0
```

we can infer that `qux` is not null but cannot make any claims about `foo` or `bar`'s nullability.


### Description of changes

- Introduce `PredicateHelper.getImpliedNotNullExprIds(IsNotNull)` helper, which takes an `IsNotNull` expression and returns the `ExprId`s of expressions which cannot be null. This handles simple cases like `IsNotNull(columnFromTable)`, as well as more complex cases involving expression trees (properly accounting for null-(in)tolerance).
  - There was similar existing logic in `FilterExec`, but I think it was overly conservative: given `IsNotNull(x)`, it would claim that `x` _and all of its descendants_ were not null if and only if every ancestor of `x` was `NullIntolerant`. However, even if `x` is null-tolerant we can still make claims about `x`'s non-nullability even if we can't make further claims about its children.
- Update `logical.Filter` to leverage this new function to update output nullability.
- Modify `FilterExec` to re-use this logic. This part is a bit tricky because the `FilterExec` code looks at `IsNotNull` expressions both for optimizing the order of expression evaluation _and_ for refining nullability to elide null checks in downstream operators.
- Modify `logical.Project` so that inferred non-nullability information from child operators is preserved.
 
### Background on related historical changes / bugs

While developing this patch, I found the following historical PRs to be useful references (note: many of these original PRs contained correctness bugs which were subsequently fixed in later PRs):

- #10844 first introduced constraint inference and propagation in Spark SQL (including basic extraction of `IsNotNull` conditions from expressions).
- #11585 modified `FilterExec` to update output nullability based on `IsNotNull` conditions.
- #11792 modified `FilterExec` to add special handling for `IsNotNull` expression codegen, altering evaluation order to allow for better short-circuiting.
- #11809 introduced the `NullIntolerant` trait to generalize the `IsNotNull` extraction logic.
- #11810 updates `FilterExec`'s `IsNotNull`-handling path to use `NullIntolerant`
- #15523 fixed a bug in `FilterExec`'s logic: it did not properly account for null-tolerant operators which were ancestors of `IsNotNull` expressions.
- #16067 fixed a bug related to negation and `IsNotNull`.



## How was this patch tested?

Added new tests for the added `PredicateHelper.getImpliedNotNullExprIds`.

**TODO**: add new end-to-end tests reflecting the examples listed above (in order to properly test the integration of this new logic into `logical.Filter` and `logical.Project`).",spark,apache,JoshRosen,50748,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24765,https://github.com/apache/spark/pull/24765,https://github.com/apache/spark/pull/24765.diff,https://github.com/apache/spark/pull/24765.patch
269,https://api.github.com/repos/apache/spark/issues/24742,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24742/labels{/name},https://api.github.com/repos/apache/spark/issues/24742/comments,https://api.github.com/repos/apache/spark/issues/24742/events,https://github.com/apache/spark/pull/24742,450308578,MDExOlB1bGxSZXF1ZXN0MjgzNjU0OTQx,24742,[SPARK-27881][SQL] Support CAST (... FORMAT <pattern>) expression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-05-30T13:29:22Z,2019-09-16T19:00:09Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR aims to add SQL:2016 standard compatible `CAST (... FORMAT <pattern>)`  functions:
```
CAST(<datetime> AS <char string type> [FORMAT <template>])
CAST(<char string> AS <datetime type> [FORMAT <template>])
```

## How was this patch tested?

Added new SQL UTs.
",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24742,https://github.com/apache/spark/pull/24742,https://github.com/apache/spark/pull/24742.diff,https://github.com/apache/spark/pull/24742.patch
270,https://api.github.com/repos/apache/spark/issues/24726,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24726/labels{/name},https://api.github.com/repos/apache/spark/issues/24726/comments,https://api.github.com/repos/apache/spark/issues/24726/events,https://github.com/apache/spark/pull/24726,449146479,MDExOlB1bGxSZXF1ZXN0MjgyNzMxMTYx,24726,[SPARK-27865][SQL] Support 1:N sort merge bucket join without shuffle,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-05-28T08:57:53Z,2019-09-16T18:14:14Z,,NONE,"## Support 1:N sort merge bucket join without shuffle


## Test
Here is the code for verification
```scala
val spark = SparkSession.builder()
    .master(""local[*]"")
    .appName(""TestBucketJoin"")
    .config(""spark.sql.autoBroadcastJoinThreshold"", 1)
    .getOrCreate()

spark.sql(
    """"""create table tbl1(a int, b int)
      |using csv 
      |clustered by (a) 
      |sorted by (a) 
      |into 4 buckets
      |"""""".stripMargin)
  spark.sql(
    """"""create table tbl2(a int, b int)
      |using csv 
      |clustered by (a) 
      |sorted by (a) 
      |into 4 buckets
      |"""""".stripMargin)
  spark.sql(
    """"""create table tbl3(a int, b int)
      |using csv 
      |clustered by (a) 
      |sorted by (a) 
      |into 12 buckets
      |"""""".stripMargin)

  import spark.implicits._
  val data = spark.sparkContext.parallelize(0 until 12, 1)
  spark.createDataset(data).createOrReplaceTempView(""data"")

  spark.sql(""insert overwrite table tbl1 select value, value from data"")
  spark.sql(""insert overwrite table tbl2 select value, value from data"")
  spark.sql(""insert overwrite table tbl3 select value, value from data"")
  
  spark.sql(""select * from tbl1 join tbl3 on tbl1.a = tbl3.a"").show()
```

For the join in the last line, this feature make sure that the sort merge bucket join is used to join the two tables which has 4 and 12 buckets respectively.

With this feature, map-only sort merge bucket join is used as shown below
![1 N bucket join DAG](https://user-images.githubusercontent.com/3096874/58465000-5b9bd800-8169-11e9-9d0c-6031b7dc20d0.png)

Without this feature, shuffle based sort merge join is used
![1 N bucket join shuffle DAG](https://user-images.githubusercontent.com/3096874/58466960-53de3280-816d-11e9-914e-23603b4cddb5.png)


",spark,apache,habren,3096874,MDQ6VXNlcjMwOTY4NzQ=,https://avatars1.githubusercontent.com/u/3096874?v=4,,https://api.github.com/users/habren,https://github.com/habren,https://api.github.com/users/habren/followers,https://api.github.com/users/habren/following{/other_user},https://api.github.com/users/habren/gists{/gist_id},https://api.github.com/users/habren/starred{/owner}{/repo},https://api.github.com/users/habren/subscriptions,https://api.github.com/users/habren/orgs,https://api.github.com/users/habren/repos,https://api.github.com/users/habren/events{/privacy},https://api.github.com/users/habren/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24726,https://github.com/apache/spark/pull/24726,https://github.com/apache/spark/pull/24726.diff,https://github.com/apache/spark/pull/24726.patch
271,https://api.github.com/repos/apache/spark/issues/24709,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24709/labels{/name},https://api.github.com/repos/apache/spark/issues/24709/comments,https://api.github.com/repos/apache/spark/issues/24709/events,https://github.com/apache/spark/pull/24709,448518329,MDExOlB1bGxSZXF1ZXN0MjgyMjY1MDA2,24709,[SPARK-27841][SQL] Improve UTF8String to/fromString()/numBytesForFirstByte() performance,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-05-26T01:36:41Z,2019-11-16T02:58:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

UTF8String's `fromString()`, `toString()`, and `numBytesForFirstByte()` methods are performance hotspots.

This patch improves the performance of those methods by implementing special-case optimizations for strings which consist entirely of ASCII characters. Early microbenchmarks show a ~2x speedup for `numChars()` (which uses `numBytesForFirstByte()`), a ~20% speedup of `toString()` for short ASCII strings, and a 1.3x-1.9x speedup of `fromString()` for short ASCII strings (with somewhat negligible impact for non-ASCII strings).

This assumes that it's cheap to check whether a string is ASCII: my changes may harm performance in case non-ASCII strings cannot be quickly identified (e.g. the first non-ASCII character in a string appears at the very end of a huge string). To allow this optimization to be disabled, we may be able to add a static system property feature flag (we must use a static property instead of runtime conf. because the overhead of checking the configuration must be minimized: the configuration must be effectively constant for the lifetime of the JVM).

I'll now describe each optimization in turn:

### numBytesForFirstByte()

UTF8 uses between 1 and 4 bytes to encode characters. If we want to quickly determine the character length of a string (numChars()) or iterate character-by-character (e.g. to support substring / slice operations) then we need a fast method for determining how many bytes are used to store a codepoint. Spark's `numBytesForFirstByte()` method takes the first byte of a codepoint and returns the number of bytes in the codepoint (for ASCII characters, this is 1 byte).

Spark's current implementation converts the codepoint's first (signed) byte into an unsigned integer (e.g. 0 to 255) and uses it to index into a 256-element lookup table where each position records the number of bytes in the codepoint. For ASCII characters, we end up indexing into an essentially random location in one half of this array and this has a performance penalty. 

As an optimization, we can simply check whether the signed byte is positive (e.g. highest-order bit is 0): if so, we can short-circuit and return `1` because we know that the character is ASCII.

With this optimization I saw a ~2x speedup in `numChars()` and I expect it will also benefit other methoids, including substring(), indexOf(), upper(), lower(), and trim().

### fromString()

UTF8String.fromString(String) was originally implemented as 

```
str.getBytes(StandardCharsets.UTF_8)
```

but this is inefficient for ASCII characters because it ends up over-allocating a byte array and then needs to copy it to trim it. To see this:

```scala
// Helper function for measuring total allocations on current thread:
def measureAllocations(f: => Unit) {
  import java.lang.management.ManagementFactory
  import com.sun.management.ThreadMXBean
  val threadMxBean = ManagementFactory.getThreadMXBean.asInstanceOf[ThreadMXBean]
  def getAlloc() = threadMxBean.getThreadAllocatedBytes(Thread.currentThread.getId)
  System.gc()
  val allocStart = getAlloc()
  f
  System.gc()
  val allocEnd = getAlloc()
  println(s""${(allocEnd - allocStart) / (1024 * 1024)} MB"")
}

measureAllocations {
  val s = ""12345678""
  var i = 0
  while (i < 1000 * 1000) {
    // Use string charset name instead of StandardCharsets.UTF_8
    // so we re-use a cached thread local StringEncoder.
    // See https://psy-lob-saw.blogspot.com/2012/12/encode-utf-8-string-to-bytebuffer-faster.html 
    s.getBytes(""UTF-8"")
    i += 1
  }
}
```

This 8-character ASCII string can be stored in only 8 bytes for the array slots (plus 16 bytes of object + array header + alignment overhead), so each array should take 24 bytes of space and we'd expect converting 1 million of them to perform around ~24 MB of allocations.

However, the code above allocates ~3x more memory than expected (~61 MB)! The reason for this is that it allocates an overly-large byte array based on an estimated bytes per character (which is > 1) and then needs to trim it.

However, if we know that a string is ASCII then we can allocate a right-sized byte array ourselves and then can copy each character by casting it to a byte.

### toString()

If we know that our bytes are ASCII then we can call the deprecated `String(bytes[], int)` constructor to invoke a path which simply casts each byte into a char, completely skipping the `StringDecoder` (and any associated allocation / GC (or thread-local lookups if passing a charset name instead of a `StandardCharsets` constant)).

We could perform a `byte[]` to `char[]` conversion ourselves but that forces us to pay for an additional defensive copy of the `char[]`, whereas the deprecated constructor lets us skip this. I checked and this deprecated constructor is still present in newer versions of Java (and, in fact, has been optimized to support the JDK9 CompactString features).

## How was this patch tested?

Correctness should be covered by existing tests.

To measure performance I added a set of microbenchmarks for `UTF8String`.

 **Benchmarking help, especially on real workloads, would be very appreciated** üòÉ .",spark,apache,JoshRosen,50748,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24709,https://github.com/apache/spark/pull/24709,https://github.com/apache/spark/pull/24709.diff,https://github.com/apache/spark/pull/24709.patch
272,https://api.github.com/repos/apache/spark/issues/24659,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24659/labels{/name},https://api.github.com/repos/apache/spark/issues/24659/comments,https://api.github.com/repos/apache/spark/issues/24659/events,https://github.com/apache/spark/pull/24659,446486456,MDExOlB1bGxSZXF1ZXN0MjgwNjc2NjA1,24659,[SPARK-27788][SQL] Session aware event log for Spark thrift server,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-05-21T08:33:49Z,2019-09-17T00:04:40Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Spark Thrift server is a long running service. If we enabled Spark event log, the log won't close util Spark Thrift server exits. This event log is always too big to be replayed by history server. So this feature of session aware event log writes a standalone event log for each session.

This PR doesn't change the original Spark Event log at all. It imports a new one.
To enable it, set `spark.sql.hive.thriftServer.eventLog.enabled` to `true`.

## How was this patch tested?

Manually test.

Two connections from beeline will generate two event logs like below:

> $ ls -l
total 528
-rwxrwx---  1 lajin  wheel  78499 May 21 16:13 local-1558423605808_4e223a3d-0c3a-48ab-a210-3a928e29712d
-rwxrwx---  1 lajin  wheel  74873 May 21 16:13 local-1558423605808_6594a337-02dc-4398-8df2-b738eedb45c3
",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24659,https://github.com/apache/spark/pull/24659,https://github.com/apache/spark/pull/24659.diff,https://github.com/apache/spark/pull/24659.patch
273,https://api.github.com/repos/apache/spark/issues/24625,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24625/labels{/name},https://api.github.com/repos/apache/spark/issues/24625/comments,https://api.github.com/repos/apache/spark/issues/24625/events,https://github.com/apache/spark/pull/24625,444930149,MDExOlB1bGxSZXF1ZXN0Mjc5NDg1ODE0,24625,[SPARK-27744][SQL] preserve spark properties on async subquery tasks,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-05-16T12:34:58Z,2019-09-16T18:12:48Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Add a new future helper method that copies over spark properties before submitting to a new thread, and sets the original properties back on exit. SubqueryExec now uses this method to create futures. Any task that is submitted to its thread pool will now use the spark properties of the submitting thread, instead of the spark properties that were on the thread pool worker thread

## How was this patch tested?

Added a test, which fails with current master but passes with this patch
",spark,apache,onursatici,5051569,MDQ6VXNlcjUwNTE1Njk=,https://avatars3.githubusercontent.com/u/5051569?v=4,,https://api.github.com/users/onursatici,https://github.com/onursatici,https://api.github.com/users/onursatici/followers,https://api.github.com/users/onursatici/following{/other_user},https://api.github.com/users/onursatici/gists{/gist_id},https://api.github.com/users/onursatici/starred{/owner}{/repo},https://api.github.com/users/onursatici/subscriptions,https://api.github.com/users/onursatici/orgs,https://api.github.com/users/onursatici/repos,https://api.github.com/users/onursatici/events{/privacy},https://api.github.com/users/onursatici/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24625,https://github.com/apache/spark/pull/24625,https://github.com/apache/spark/pull/24625.diff,https://github.com/apache/spark/pull/24625.patch
274,https://api.github.com/repos/apache/spark/issues/24618,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24618/labels{/name},https://api.github.com/repos/apache/spark/issues/24618/comments,https://api.github.com/repos/apache/spark/issues/24618/events,https://github.com/apache/spark/pull/24618,444667023,MDExOlB1bGxSZXF1ZXN0Mjc5MjgxMTk1,24618,[SPARK-27734][CORE][SQL][WIP] Add memory based thresholds for shuffle spill,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-05-15T22:07:54Z,2019-09-16T18:13:56Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When running large shuffles (700TB input data, 200k map tasks, 50k reducers on a 300 nodes cluster) the job is regularly OOMing in map and reduce phase.

IIUC ShuffleExternalSorter (map side) and ExternalAppendOnlyMap and ExternalSorter (reduce side) are trying to max out the available execution memory. This in turn doesn't play nice with the Garbage Collector and executors are failing with OutOfMemoryError when the memory allocation from these in-memory structure is maxing out the available heap size (in our case we are running with 9 cores/executor, 32G per executor)

To mitigate this, one can set `spark.shuffle.spill.numElementsForceSpillThreshold` to force the spill on disk. While this config works, it is not flexible enough as it's expressed in number of elements, and in our case we run multiple shuffles in a single job and element size is different from one stage to another.


This patch extends the spill threshold behaviour and adds two new parameters to control the spill based on memory usage:

- `spark.shuffle.spill.map.maxRecordsSizeForSpillThreshold`
- `spark.shuffle.spill.reduce.maxRecordsSizeForSpillThreshold`


## How was this patch tested?

- internal testing
- TODO: extend existing unit-tests

",spark,apache,amuraru,728853,MDQ6VXNlcjcyODg1Mw==,https://avatars1.githubusercontent.com/u/728853?v=4,,https://api.github.com/users/amuraru,https://github.com/amuraru,https://api.github.com/users/amuraru/followers,https://api.github.com/users/amuraru/following{/other_user},https://api.github.com/users/amuraru/gists{/gist_id},https://api.github.com/users/amuraru/starred{/owner}{/repo},https://api.github.com/users/amuraru/subscriptions,https://api.github.com/users/amuraru/orgs,https://api.github.com/users/amuraru/repos,https://api.github.com/users/amuraru/events{/privacy},https://api.github.com/users/amuraru/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24618,https://github.com/apache/spark/pull/24618,https://github.com/apache/spark/pull/24618.diff,https://github.com/apache/spark/pull/24618.patch
275,https://api.github.com/repos/apache/spark/issues/24611,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24611/labels{/name},https://api.github.com/repos/apache/spark/issues/24611/comments,https://api.github.com/repos/apache/spark/issues/24611/events,https://github.com/apache/spark/pull/24611,444261323,MDExOlB1bGxSZXF1ZXN0Mjc4OTU2MTgy,24611,[SPARK-27717][SS] support UNION in continuous processing,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-05-15T06:37:18Z,2019-09-16T19:00:11Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, the `EpochCoordinator` can only support one stream. This PR enables `EpochCoordinator` to manage multiple streams and support stream `UNION`.

## How was this patch tested?

update ut and add new ut
",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24611,https://github.com/apache/spark/pull/24611,https://github.com/apache/spark/pull/24611.diff,https://github.com/apache/spark/pull/24611.patch
276,https://api.github.com/repos/apache/spark/issues/24601,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24601/labels{/name},https://api.github.com/repos/apache/spark/issues/24601/comments,https://api.github.com/repos/apache/spark/issues/24601/events,https://github.com/apache/spark/pull/24601,443844549,MDExOlB1bGxSZXF1ZXN0Mjc4NjI1NTU1,24601,[SPARK-27702][K8S] Allow using some alternatives for service accounts,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2019-05-14T10:48:21Z,2019-09-16T18:13:02Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Allow using alternatives for service accounts

## How was this patch tested?
passed the certificates using `spark.kubernetes.authenticate.driver.mounted.caCertFile`
or token file by `spark.kubernetes.authenticate.driver.mounted.oauthTokenFile` if there is no default service account available.",spark,apache,Udbhav30,44489863,MDQ6VXNlcjQ0NDg5ODYz,https://avatars2.githubusercontent.com/u/44489863?v=4,,https://api.github.com/users/Udbhav30,https://github.com/Udbhav30,https://api.github.com/users/Udbhav30/followers,https://api.github.com/users/Udbhav30/following{/other_user},https://api.github.com/users/Udbhav30/gists{/gist_id},https://api.github.com/users/Udbhav30/starred{/owner}{/repo},https://api.github.com/users/Udbhav30/subscriptions,https://api.github.com/users/Udbhav30/orgs,https://api.github.com/users/Udbhav30/repos,https://api.github.com/users/Udbhav30/events{/privacy},https://api.github.com/users/Udbhav30/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24601,https://github.com/apache/spark/pull/24601,https://github.com/apache/spark/pull/24601.diff,https://github.com/apache/spark/pull/24601.patch
277,https://api.github.com/repos/apache/spark/issues/24593,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24593/labels{/name},https://api.github.com/repos/apache/spark/issues/24593/comments,https://api.github.com/repos/apache/spark/issues/24593/events,https://github.com/apache/spark/pull/24593,443551598,MDExOlB1bGxSZXF1ZXN0Mjc4Mzk0ODIw,24593,[SPARK-27692][SQL] Add new optimizer rule to evaluate the deterministic scala udf only once if all inputs are literals,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2019-05-13T18:58:47Z,2019-09-22T12:06:41Z,,CONTRIBUTOR,"Description: 
Deterministic UDF is a udf for which the following is true:  Given a specific input, the output of the udf will be the same no matter how many times you execute the udf.

When your inputs to the UDF are all literal and UDF is deterministic, we can optimize this to evaluate the udf once and use the output instead of evaluating the UDF each time for every row in the query. 

This is valid only if the UDF is deterministic and inputs are literal.  Otherwise we should not and cannot apply this optimization. 

Changes: 
- Add a new optimizer rule to evaluate the ScalaUDF once if it is deterministic and the inputs are literals.  

Testing: 
- Added new unit tests

Credits: 
Thanks to [Guy Khazma](https://github.com/guykhazma) from the IBM Haifa Research Team for the idea and the original implementation. ",spark,apache,skambha,16563220,MDQ6VXNlcjE2NTYzMjIw,https://avatars3.githubusercontent.com/u/16563220?v=4,,https://api.github.com/users/skambha,https://github.com/skambha,https://api.github.com/users/skambha/followers,https://api.github.com/users/skambha/following{/other_user},https://api.github.com/users/skambha/gists{/gist_id},https://api.github.com/users/skambha/starred{/owner}{/repo},https://api.github.com/users/skambha/subscriptions,https://api.github.com/users/skambha/orgs,https://api.github.com/users/skambha/repos,https://api.github.com/users/skambha/events{/privacy},https://api.github.com/users/skambha/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24593,https://github.com/apache/spark/pull/24593,https://github.com/apache/spark/pull/24593.diff,https://github.com/apache/spark/pull/24593.patch
278,https://api.github.com/repos/apache/spark/issues/24575,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24575/labels{/name},https://api.github.com/repos/apache/spark/issues/24575/comments,https://api.github.com/repos/apache/spark/issues/24575/events,https://github.com/apache/spark/pull/24575,442576398,MDExOlB1bGxSZXF1ZXN0Mjc3NjYwMTM2,24575,[SPARK-27670][SQL]Add HA for HiveThriftServer2 based on HiveServer2.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-05-10T07:26:02Z,2019-09-18T15:06:09Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

`HiveThriftServer2` is a multi-session managed service based on `HiveServer2`, which provides centralized management services for Hive.

Since `HiveThriftServer2` itself inherits from `HiveServer2`, `HiveServer2`'s own HA solution can also support `HiveThriftServer2`.

`HiveThriftServer2` does not support **HA**. Why is this?

The main method of `HiveServer2` is as follows:
```
  public static void main(String[] args) {
    HiveConf.setLoadHiveServer2Config(true);
    try {
      ServerOptionsProcessor oproc = new ServerOptionsProcessor(""hiveserver2"");
      ServerOptionsProcessorResponse oprocResponse = oproc.parse(args);
 
      // Omit irrelevant code
 
      // Call the executor which will execute the appropriate command based on the parsed options
      oprocResponse.getServerOptionsExecutor().execute();
    } catch (LogInitializationException e) {
      LOG.error(""Error initializing log: "" + e.getMessage(), e);
      System.exit(-1);
    }
  }
```
The above code first creates the `ServerOptionsProcessor` object and parses the parameters. The `parse` method parses the parameters and returns the `oprocResponse` object (type is `ServerOptionsProcessorResponse`). Then the object obtained by calling the `getServerOptionsExecutor` method of `oprocResponse` is actually `StartOptionExecutor`. Finally, the execute method of `StartOptionExecutor` is called. The implementation of StartOptionExecutor is as follows:
```
  static class StartOptionExecutor implements ServerOptionsExecutor {
    @Override
    public void execute() {
      try {
        startHiveServer2();
      } catch (Throwable t) {
        LOG.fatal(""Error starting HiveServer2"", t);
        System.exit(-1);
      }
    }
  }
```
It can be seen that the `execute` method of `StartOptionExecutor` actually calls the `startHiveServer2` method, and the code related to HA in the `startHiveServer2` method is as follows:
```
        if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {
          server.addServerInstanceToZooKeeper(hiveConf);
        }
```
You can see that the `addServerInstanceToZooKeeper` method of `HiveServer2` is called. The role of the `addServerInstanceToZooKeeper` is to create a persistent parent node as the HA namespace on the specified `ZooKeeper` cluster, and create a persistent node to save the `HiveServer2` instance information to the node.

But `HiveThriftServer2` can not call the `execute` method of `StartOptionExecutor` as Spark has its own logic. So I added some reflection code as to call the `addServerInstanceToZooKeeper` method of `HiveServer2`.

**Notice**:This PR can support Hive version as follows:
0.14.0 through 2.3.4
3.1.0 through 3.1.1

## How was this patch tested?

Exists UT.
",spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24575,https://github.com/apache/spark/pull/24575,https://github.com/apache/spark/pull/24575.diff,https://github.com/apache/spark/pull/24575.patch
279,https://api.github.com/repos/apache/spark/issues/24566,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24566/labels{/name},https://api.github.com/repos/apache/spark/issues/24566/comments,https://api.github.com/repos/apache/spark/issues/24566/events,https://github.com/apache/spark/pull/24566,442211580,MDExOlB1bGxSZXF1ZXN0Mjc3MzcyMTc4,24566,[SPARK-27667][SQL] support display of current Database in the spark-sql CLI,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2019-05-09T12:30:08Z,2019-12-09T15:40:59Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Get the current database from the spark catalog instead of `CliSessionState`.Spark does not set the current database to `CliSessionState`.So it will always return empty.

## How was this patch tested?

Manually and added also with UT

**Before Fix**
![before](https://user-images.githubusercontent.com/35216143/57453478-2267fa80-7284-11e9-8302-87ffcdd56a40.png)

**After Fix**
![After](https://user-images.githubusercontent.com/35216143/57453510-36136100-7284-11e9-991c-c50508d1b0b3.png)
",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24566,https://github.com/apache/spark/pull/24566,https://github.com/apache/spark/pull/24566.diff,https://github.com/apache/spark/pull/24566.patch
280,https://api.github.com/repos/apache/spark/issues/24563,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24563/labels{/name},https://api.github.com/repos/apache/spark/issues/24563/comments,https://api.github.com/repos/apache/spark/issues/24563/events,https://github.com/apache/spark/pull/24563,442003216,MDExOlB1bGxSZXF1ZXN0Mjc3MjExMTYx,24563,[SPARK-27359] [OPTIMIZER] [SQL] Rewrite ArraysOverlap Join,"[{'id': 1406605297, 'node_id': 'MDU6TGFiZWwxNDA2NjA1Mjk3', 'url': 'https://api.github.com/repos/apache/spark/labels/OPTIMIZER', 'name': 'OPTIMIZER', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-05-09T00:55:11Z,2019-09-16T19:03:10Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

An  optimization for joins on a condition of `arrays_overlap`. I believe this worthwhile to integrate into Spark due to the recent release of several new array functions in Spark 2.4. This optimization will allow
users to make better use of the arrays overlap function. The technique proposed in the patch can also be trivially extended to joins with a condition involving `array_contains`.

The following code will produce a cartesian product in the physical plans.
```scala
import spark.implicits._
import org.apache.spark.sql.functions._

val a = Seq((Seq(1, 2, 3), ""one"")).toDF(""num"", ""name"")
val b = Seq((Seq(1, 5), ""two"")).toDF(""num"", ""name"")
val j = a.join(b, arrays_overlap(b(""num""), a(""num"")))
j.explain(true)
```
```
== Parsed Logical Plan ==
Join Inner, arrays_overlap(num#158, num#149)
:- Project [_1#146 AS num#149, _2#147 AS name#150]
:  +- LocalRelation [_1#146, _2#147]
+- Project [_1#155 AS num#158, _2#156 AS name#159]
   +- LocalRelation [_1#155, _2#156]

== Analyzed Logical Plan ==
num: array<int>, name: string, num: array<int>, name: string
Join Inner, arrays_overlap(num#158, num#149)
:- Project [_1#146 AS num#149, _2#147 AS name#150]
:  +- LocalRelation [_1#146, _2#147]
+- Project [_1#155 AS num#158, _2#156 AS name#159]
   +- LocalRelation [_1#155, _2#156]

== Optimized Logical Plan ==
Join Inner, arrays_overlap(num#158, num#149)
:- LocalRelation [num#149, name#150]
+- LocalRelation [num#158, name#159]

== Physical Plan ==
CartesianProduct arrays_overlap(num#158, num#149)
:- LocalTableScan [num#149, name#150]
+- LocalTableScan [num#158, name#159]
```

This is unacceptable for joins on large datasets.
The query can be written into an equivalent equijoin by:
1. exploding the arrays
2. joining on the exploded columns
3. dropping the exploded columns on the joined data
4. removing duplicates from the result of 3)

Doing so will bring a query that might otherwise never complete, down to a reasonable time.
```
== Parsed Logical Plan ==
Join Inner, arrays_overlap(num#158, num#149)
:- Project [_1#146 AS num#149, _2#147 AS name#150]
:  +- LocalRelation [_1#146, _2#147]
+- Project [_1#155 AS num#158, _2#156 AS name#159]
   +- LocalRelation [_1#155, _2#156]

== Analyzed Logical Plan ==
num: array<int>, name: string, num: array<int>, name: string
Join Inner, arrays_overlap(num#158, num#149)
:- Project [_1#146 AS num#149, _2#147 AS name#150]
:  +- LocalRelation [_1#146, _2#147]
+- Project [_1#155 AS num#158, _2#156 AS name#159]
   +- LocalRelation [_1#155, _2#156]

== Optimized Logical Plan ==
Aggregate [1], [first(num#149, false) AS num#149, first(name#150, false) AS name#150, first(num#158, false) AS num#158, first(name#159, false) AS name#159]
+- Project [num#149, name#150, num#158, name#159]
   +- Join Inner, (explode_larr#178 = explode_rarr#180)
      :- Project [num#149, name#150, explode_larr#178]
      :  +- Generate explode(num#149), false, [explode_larr#178]
      :     +- LocalRelation [num#149, name#150]
      +- Project [num#158, name#159, explode_rarr#180]
         +- Generate explode(num#158), false, [explode_rarr#180]
            +- LocalRelation [num#158, name#159]

== Physical Plan ==
SortAggregate(key=[1#185], functions=[finalmerge_first(merge first#188, valueSet#189) AS first(num#149)()#181, finalmerge_first(merge first#192, valueSet#193) AS first(name#150)()#182, finalmerge_first(merge first#196, valueSet#197) AS first(num#158)()#183, finalmerge_first(merge first#200, valueSet#201) AS first(name#159)()#184], output=[num#149, name#150, num#158, name#159])
+- Sort [1#185 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(1#185, 200)
      +- SortAggregate(key=[1 AS 1#185], functions=[partial_first(num#149, false) AS (first#188, valueSet#189), partial_first(name#150, false) AS (first#192, valueSet#193), partial_first(num#158, false) AS (first#196, valueSet#197), partial_first(name#159, false) AS (first#200, valueSet#201)], output=[1#185, first#188, valueSet#189, first#192, valueSet#193, first#196, valueSet#197, first#200, valueSet#201])
         +- *(3) Sort [1 AS 1#185 ASC NULLS FIRST], false, 0
            +- *(3) Project [num#149, name#150, num#158, name#159]
               +- *(3) SortMergeJoin [explode_larr#178], [explode_rarr#180], Inner
                  :- Sort [explode_larr#178 ASC NULLS FIRST], false, 0
                  :  +- Exchange hashpartitioning(explode_larr#178, 200)
                  :     +- *(1) Project [num#149, name#150, explode_larr#178]
                  :        +- *(1) Generate explode(num#149), [num#149, name#150], false, [explode_larr#178]
                  :           +- LocalTableScan [num#149, name#150]
                  +- Sort [explode_rarr#180 ASC NULLS FIRST], false, 0
                     +- Exchange hashpartitioning(explode_rarr#180, 200)
                        +- *(2) Project [num#158, name#159, explode_rarr#180]
                           +- *(2) Generate explode(num#158), [num#158, name#159], false, [explode_rarr#180]
                              +- LocalTableScan [num#158, name#159]
```

## How was this patch tested?

This patch has only been tested via manual tests on a large dataset.
I've used the technique implemented by this patch to perform similar joins with ~300 million records on either side of the join. If you agree that this is a worthwhile optimization, I'll happily contribute some unit tests to ensure the robustness of the optimization.",spark,apache,nvander1,11447895,MDQ6VXNlcjExNDQ3ODk1,https://avatars3.githubusercontent.com/u/11447895?v=4,,https://api.github.com/users/nvander1,https://github.com/nvander1,https://api.github.com/users/nvander1/followers,https://api.github.com/users/nvander1/following{/other_user},https://api.github.com/users/nvander1/gists{/gist_id},https://api.github.com/users/nvander1/starred{/owner}{/repo},https://api.github.com/users/nvander1/subscriptions,https://api.github.com/users/nvander1/orgs,https://api.github.com/users/nvander1/repos,https://api.github.com/users/nvander1/events{/privacy},https://api.github.com/users/nvander1/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24563,https://github.com/apache/spark/pull/24563,https://github.com/apache/spark/pull/24563.diff,https://github.com/apache/spark/pull/24563.patch
281,https://api.github.com/repos/apache/spark/issues/24561,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24561/labels{/name},https://api.github.com/repos/apache/spark/issues/24561/comments,https://api.github.com/repos/apache/spark/issues/24561/events,https://github.com/apache/spark/pull/24561,441900679,MDExOlB1bGxSZXF1ZXN0Mjc3MTI5Mjk4,24561,[SPARK-26130][UI] Change Event Timeline Display Functionality on the Stages Page to use either REST API or data from other tables,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-05-08T19:46:30Z,2019-09-16T18:13:21Z,,CONTRIBUTOR,"As per Jira https://issues.apache.org/jira/browse/SPARK-21809, Stages page will use datatables for performing Column sorting, searching, pagination etc. To support those datatables, we have changed the Stage page to use ajax calls to access the server API's. However, event timeline functionality on the stage page has not been updated to use the REST API or use data from the datatables dynamically to reconstruct the graphs at the Client end.

## What changes were proposed in this pull request?

Added AJAX Call to the Event Timeline Graph form to dynamically load data without needing page refresh. After this change, almost the whole Stages page uses AJAX to perform dynamic data loading and display.

## How was this patch tested?

Manual Tests
",spark,apache,pgandhi999,22228190,MDQ6VXNlcjIyMjI4MTkw,https://avatars0.githubusercontent.com/u/22228190?v=4,,https://api.github.com/users/pgandhi999,https://github.com/pgandhi999,https://api.github.com/users/pgandhi999/followers,https://api.github.com/users/pgandhi999/following{/other_user},https://api.github.com/users/pgandhi999/gists{/gist_id},https://api.github.com/users/pgandhi999/starred{/owner}{/repo},https://api.github.com/users/pgandhi999/subscriptions,https://api.github.com/users/pgandhi999/orgs,https://api.github.com/users/pgandhi999/repos,https://api.github.com/users/pgandhi999/events{/privacy},https://api.github.com/users/pgandhi999/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24561,https://github.com/apache/spark/pull/24561,https://github.com/apache/spark/pull/24561.diff,https://github.com/apache/spark/pull/24561.patch
282,https://api.github.com/repos/apache/spark/issues/24559,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24559/labels{/name},https://api.github.com/repos/apache/spark/issues/24559/comments,https://api.github.com/repos/apache/spark/issues/24559/events,https://github.com/apache/spark/pull/24559,441850105,MDExOlB1bGxSZXF1ZXN0Mjc3MDg4OTI0,24559,[SPARK-27658][SQL] Add FunctionCatalog API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-05-08T17:40:46Z,2019-09-29T09:59:38Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This adds a new API for catalog plugins that exposes functions to Spark. The API can list and load functions. This does not include create, delete, or alter operations.

There are 3 types of functions defined:
* A `ScalarFunction` that produces a value for every call
* An `AggregateFunction` that produces a value after updates for a group of rows
* An `AssociativeAggregateFunction` that is an aggregate that can be run in parallel and can merge intermediate results

Functions loaded from the catalog by name as `UnboundFunction`. Once input arguments are determined `bind` is called on the unbound function to get a `BoundFunction` implementation that is one of the 3 types above. Binding can fail if the function doesn't support the input type. `BoundFunction` returns the result type produced by the function.

## How was this patch tested?

This includes a test that demonstrates the new API.",spark,apache,rdblue,87915,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24559,https://github.com/apache/spark/pull/24559,https://github.com/apache/spark/pull/24559.diff,https://github.com/apache/spark/pull/24559.patch
283,https://api.github.com/repos/apache/spark/issues/24553,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24553/labels{/name},https://api.github.com/repos/apache/spark/issues/24553/comments,https://api.github.com/repos/apache/spark/issues/24553/events,https://github.com/apache/spark/pull/24553,441582152,MDExOlB1bGxSZXF1ZXN0Mjc2ODc3NTUz,24553,[SPARK-27604][SQL] Enhance constant propagation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2019-05-08T07:02:25Z,2019-09-16T19:03:09Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR improves `ConstantPropagation` rule by extending propagation from `attribute => constant` to `expression => constant` mapping and allows substituting deterministic expressions to constants in any expression. (Previously substitution of attributes only in `EqualTo` and `EqualNullSafe` was allowed).
```
SELECT * FROM table WHERE abs(i) = 5 AND j <= abs(i) + 3  =>  ... WHERE abs(i) = 5 AND j <= 8
```

## How was this patch tested?

Existing and new UTs.

",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24553,https://github.com/apache/spark/pull/24553,https://github.com/apache/spark/pull/24553.diff,https://github.com/apache/spark/pull/24553.patch
284,https://api.github.com/repos/apache/spark/issues/24537,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24537/labels{/name},https://api.github.com/repos/apache/spark/issues/24537/comments,https://api.github.com/repos/apache/spark/issues/24537/events,https://github.com/apache/spark/pull/24537,440709598,MDExOlB1bGxSZXF1ZXN0Mjc2MTk0NTY2,24537,[SPARK-23887][SS] continuous query progress reporting,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-05-06T13:49:47Z,2019-09-16T19:03:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Enable query progress reporting in continuous mode.

- add two new trait: `MicroBatchProgressReporter ` and `ContinuousProgressReporter `
- two stage duration in continuous query, including `queryPlanning` and `walCommit`. Especially, the `queryPlanning` is a static and wont change in each epoch.  It only happens once time at the beginning of query.

## How was this patch tested?

update existing uts",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24537,https://github.com/apache/spark/pull/24537,https://github.com/apache/spark/pull/24537.diff,https://github.com/apache/spark/pull/24537.patch
285,https://api.github.com/repos/apache/spark/issues/24530,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24530/labels{/name},https://api.github.com/repos/apache/spark/issues/24530/comments,https://api.github.com/repos/apache/spark/issues/24530/events,https://github.com/apache/spark/pull/24530,440461326,MDExOlB1bGxSZXF1ZXN0Mjc2MDA3NTQx,24530,[SPARK-27520][CORE][WIP] Introduce a global config system to replace hadoopConfiguration,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2019-05-05T16:08:34Z,2019-09-16T19:03:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

hadoopConf can be accessed via `SparkContext.hadoopConfiguration` from both user code and Spark internal. The configuration is mainly used to read files from hadoop-supported file system(eg. get URI/get FileSystem/add security credentials/get metastore connect url/etc.)
We shall keep a global config that users can set and use that to track the hadoop configurations.

This pr implements it with three main features showed below:

* using ThreadLocal to track Hadoop Configuration, so that concurrent jobs could use their own Hadoop Configurations

* provide set method by wrapping a Hadoop Configuration to allow user to modify Hadoop Configuration globally

* provide SparkContext.withHadoopConf(){} method to allow user to modify Hadoop Configuration temporary

## How was this patch tested?

TODO
",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24530,https://github.com/apache/spark/pull/24530,https://github.com/apache/spark/pull/24530.diff,https://github.com/apache/spark/pull/24530.patch
286,https://api.github.com/repos/apache/spark/issues/24526,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24526/labels{/name},https://api.github.com/repos/apache/spark/issues/24526/comments,https://api.github.com/repos/apache/spark/issues/24526/events,https://github.com/apache/spark/pull/24526,440417929,MDExOlB1bGxSZXF1ZXN0Mjc1OTc5Mjkx,24526,[SPARK-27603][CORE]Make the BlockTransferService for shuffle fetch pluggable,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-05-05T08:33:03Z,2019-09-16T18:14:51Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Shuffle manager is pluggable in Spark, however, some service closely related to the shuffle functionality is constrained to 1 or 2 implementations. One example is `NettyBlockTransferService`, it is used in BlockManager to fetch remote bytes, and to fetch shuffle data in non-external shuffle. The 2 functionalities are coupled together. Actually the latter functionality to fetch shuffle data should be pluggable/extensible.

A custom Spark shuffle manager may need the set of service, including the RPC servers, clients and context that `NettyBlockTransferService` has constructed(constructing a new set of connections between executors is redundant), but also a new `NettyBlockTransferService` with custom need. For example, a remote shuffle manager under disaggregated compute and storage architecture may only need the service to transfer index files from other executors(for cache purpose) through Netty, but read data files directly from the globally-accessible storage.

We propose to make this transfer service for shuffle pluggable, also make some fields in `NettyBlockTransferService` wider accessible for developers to extend.

## How was this patch tested?

Existing tests.",spark,apache,gczsjdy,7685352,MDQ6VXNlcjc2ODUzNTI=,https://avatars1.githubusercontent.com/u/7685352?v=4,,https://api.github.com/users/gczsjdy,https://github.com/gczsjdy,https://api.github.com/users/gczsjdy/followers,https://api.github.com/users/gczsjdy/following{/other_user},https://api.github.com/users/gczsjdy/gists{/gist_id},https://api.github.com/users/gczsjdy/starred{/owner}{/repo},https://api.github.com/users/gczsjdy/subscriptions,https://api.github.com/users/gczsjdy/orgs,https://api.github.com/users/gczsjdy/repos,https://api.github.com/users/gczsjdy/events{/privacy},https://api.github.com/users/gczsjdy/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24526,https://github.com/apache/spark/pull/24526,https://github.com/apache/spark/pull/24526.diff,https://github.com/apache/spark/pull/24526.patch
287,https://api.github.com/repos/apache/spark/issues/24525,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24525/labels{/name},https://api.github.com/repos/apache/spark/issues/24525/comments,https://api.github.com/repos/apache/spark/issues/24525/events,https://github.com/apache/spark/pull/24525,440400798,MDExOlB1bGxSZXF1ZXN0Mjc1OTY4NTc5,24525,[SPARK-27633][SQL] Remove redundant aliases in NestedColumnAliasing,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2019-05-05T04:54:46Z,2019-11-22T11:59:05Z,,MEMBER,"## What changes were proposed in this pull request?

In NestedColumnAliasing rule, we create aliases for nested field access in project list. We considered that top level parent field and nested fields under it were both accessed. In the case, we don't create the aliases because they are redundant.

There is another case, where a nested parent field and nested fields under it were both accessed, which we don't consider now. We don't need to create aliases in this case too.

## How was this patch tested?

Added test.
",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24525,https://github.com/apache/spark/pull/24525,https://github.com/apache/spark/pull/24525.diff,https://github.com/apache/spark/pull/24525.patch
288,https://api.github.com/repos/apache/spark/issues/24515,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24515/labels{/name},https://api.github.com/repos/apache/spark/issues/24515/comments,https://api.github.com/repos/apache/spark/issues/24515/events,https://github.com/apache/spark/pull/24515,439694929,MDExOlB1bGxSZXF1ZXN0Mjc1NDQ1NzM4,24515,[SPARK-14083][WIP] Basic bytecode analyzer to speed up Datasets,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2019-05-02T17:06:57Z,2019-09-16T19:03:12Z,,CONTRIBUTOR,"**Disclaimer!** The purpose of this PoC PR is to trigger a discussion on whether it is feasible to leverage bytecode analysis to speed up Datasets. This PR shows what can be achieved by interpreting stack-based bytecode directly. The current version does not handle all edge cases. If the community agrees that bytecode analysis can give substantial benefits, we will need to define the scope (i.e., which cases we are targeting) and decide which approach to take (e.g., stack-based bytecode interpretation, flow-control graphs, AST, Jimple). I would like to emphasize that this PR doesn't answer those questions. Instead, it shows what can be done with the simplest approach. Also, there is no intention to integrate this logic into Spark directly. It might be a separate package.

### Scope

Bytecode analysis can be used for several purposes:
- Provide information on what's going on inside closures so that Spark can perform additional optimizations while still relying on closures during execution (e.g., analyze which fields are accessed/modified and use that information to optimize the plan).
- Replace typed operations that rely on closures with equivalent untyped operations that rely on Catalyst expressions.

Rewriting closures is challenging but gives more benefits. So, this PR focuses on the second use case.

Right now, the code covers typed map and filter operations that involve primitive values, boxed values (not all are implemented), objects that are represented as structs (e.g., case classes). The same logic can be applied to UDFs/UDAFs and other operations.

### Algorithm

There are two ways to derive Catalyst expressions:
- Translate stack-based bytecode directly.
- Convert bytecode into an intermediate format that it is easier to work with (e.g., flow-control graphs, AST, Jimple).

Both approaches have their own tradeoffs, which are well-described in comments to [SPARK-14083](https://issues.apache.org/jira/browse/SPARK-14083). This PR translates stack-based bytecode directly and simulates what happens to the operand stack. The optimizer is supposed to simplify the derived expression. I think it is valid to start with the simplest approach and evaluate its limitations before considering a more advanced implementation.

Originally, we wanted to cover trivial cases. However, the current scope is much bigger, so we should consider an intermediate format and will be glad to hear more opinions. As it was mentioned before, the decision highly depends on our target use cases.

This PR adds a new optimizer rule `RewriteTypedOperations` that uses `TypedOperations` in order to convert typed operations into untyped. `TypedOperations` follows a trivial algorithm that is described below.

#### Step 1: Get closure bytecode

First of all, we need to get bytecode for the closure. Scala 2.12 migrated to Java lambdas and uses `LambdaMetafactory` (LMF) (seems like there are rare cases when Scala doesn't use LMF). This PR relies on the existing logic in Spark to obtain `SerializedLambda`, which has enough information to get bytecode for closures.

Scala uses ""adapted"" methods for LMF to encapsulate differences in boxing semantics (i.e., unboxing null in Scala gives 0 and NPE in Java). The current code will obtain bytecode of the non-adapted method whenever the args are primitives to avoid a round of unnecessary boxing/unboxing.

See `ClosureUtils$getMethod` for more information.

#### Step 2: Build a correct local variable array

Once we have bytecode for our closure, we need to build a local variable array that references correct Catalyst expressions. This can be achieved by translating the deserializer for a typed operation. Deserializers define how data is converted from the Spark internal format into Java objects. Frequently, deserializers contain `StaticInvoke`, `Invoke`, `NewInstance` expressions, which can be translated using the same algorithm.

See `TypedOperations$convertDeserializer` for more information.

#### Step 3: Create an operand stack

The next step is to create an operand stack for storing partial Catalyst expressions. The current code uses `mutable.ArrayStack[Expression]` for this purpose.

#### Step 4: Interpret instructions

Once we have bytecode, the array of local variables and the operand stack, we can follow our bytecode instructions one by one and simulate what happens to the operand stack.

#### Step 5: Assign the result back to the expected attributes

Once we have the result expression, we need to assign it to columns. At this point, the serializer is important as it contains information about the expected attributes and their data types.

### Trying things out

`DatasetBytecodeAnalysisBenchmark` and `BytecodeAnalysisSuite` show currently supported cases. The focus is on the conceptual approach and not on addressing every possible method/instruction.

### Open Questions

- We need to discuss the scope of this work. As mentioned before, we can either translate closures into Catalyst expressions or just derive additional information about the content of closures (if that's useful enough).
- We need to discuss the overall approach we want to take (e.g., bytecode interpretation, control-flow graphs, AST, Jimple).
- We need to discuss which library (if any) to use.
- We need to handle too complicated result expressions as they can slow down the computation. For example, we can introduce some thresholds. Apart from that, we can introduce more optimization rules to simplify expressions.
- We need to ensure the conversion is lightweight and doesn't slow down the job planning time.
- We need to handle cases when the conversion doesn't terminate (e.g., infinite recursion).
- We need to ensure that edge cases work properly (e.g., null handling, exceptions, arithmetic expressions).
- We need to decide how to properly handle flow control instructions (e.g., if statements). The current code handles them via recursion and jumps.",spark,apache,aokolnychyi,6235869,MDQ6VXNlcjYyMzU4Njk=,https://avatars3.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24515,https://github.com/apache/spark/pull/24515,https://github.com/apache/spark/pull/24515.diff,https://github.com/apache/spark/pull/24515.patch
289,https://api.github.com/repos/apache/spark/issues/24498,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24498/labels{/name},https://api.github.com/repos/apache/spark/issues/24498/comments,https://api.github.com/repos/apache/spark/issues/24498/events,https://github.com/apache/spark/pull/24498,438801623,MDExOlB1bGxSZXF1ZXN0Mjc0NzQ5Njgw,24498,"[SPARK-27605][UI] Add new column ""Partition ID"" to the tasks table in stages page","[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-04-30T14:15:12Z,2019-09-16T18:14:44Z,,CONTRIBUTOR,"If you have more than one stage attempt in a Spark job, the task index will not equal partition id. Thus, adding a column for partition id would be a useful feature to have.

## What changes were proposed in this pull request?

To get the data for partition id in UI, it has to be logged into the event files while the job is running. So, the code changes mainly focus on that. Also, backward compatibility with old history files has been ensured as well here.

## How was this patch tested?

<img width=""553"" alt=""Screen Shot 2019-04-30 at 9 13 58 AM"" src=""https://user-images.githubusercontent.com/22228190/56968248-72c6b600-6b28-11e9-96ab-245611dfd685.png"">

",spark,apache,pgandhi999,22228190,MDQ6VXNlcjIyMjI4MTkw,https://avatars0.githubusercontent.com/u/22228190?v=4,,https://api.github.com/users/pgandhi999,https://github.com/pgandhi999,https://api.github.com/users/pgandhi999/followers,https://api.github.com/users/pgandhi999/following{/other_user},https://api.github.com/users/pgandhi999/gists{/gist_id},https://api.github.com/users/pgandhi999/starred{/owner}{/repo},https://api.github.com/users/pgandhi999/subscriptions,https://api.github.com/users/pgandhi999/orgs,https://api.github.com/users/pgandhi999/repos,https://api.github.com/users/pgandhi999/events{/privacy},https://api.github.com/users/pgandhi999/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24498,https://github.com/apache/spark/pull/24498,https://github.com/apache/spark/pull/24498.diff,https://github.com/apache/spark/pull/24498.patch
290,https://api.github.com/repos/apache/spark/issues/24495,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24495/labels{/name},https://api.github.com/repos/apache/spark/issues/24495/comments,https://api.github.com/repos/apache/spark/issues/24495/events,https://github.com/apache/spark/pull/24495,438707730,MDExOlB1bGxSZXF1ZXN0Mjc0Njc2NDkx,24495,[SPARK-27604][SQL] Add filter reduction,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-04-30T10:30:23Z,2019-09-16T19:03:13Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR adds a new `FilterReduction` rule that can reduce `BinaryComparison` filters like these:
```
SELECT * FROM table WHERE i <= 5 AND i = 5        => ... WHERE i = 5
SELECT * FROM table WHERE i < j AND ... AND i > j => ... WHERE false
```

## How was this patch tested?

Existing and new UTs.",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24495,https://github.com/apache/spark/pull/24495,https://github.com/apache/spark/pull/24495.diff,https://github.com/apache/spark/pull/24495.patch
291,https://api.github.com/repos/apache/spark/issues/24462,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24462/labels{/name},https://api.github.com/repos/apache/spark/issues/24462/comments,https://api.github.com/repos/apache/spark/issues/24462/events,https://github.com/apache/spark/pull/24462,437430615,MDExOlB1bGxSZXF1ZXN0MjczNzA5NDA4,24462,[SPARK-26268][CORE] Do not resubmit tasks when executors are lost,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2019-04-25T22:55:30Z,2019-09-16T18:18:36Z,,NONE,"The DAGScheduler assumes that shuffle data is lost if an executor is lost and the Spark external shuffle service is not enabled. However, external shuffle managers other than the default external shuffle service can store shuffle data outside of the Spark cluster itself. In this case, completed map taks are not rerun even if the corresponding executors are lost.

The new `spark.shuffle.external.enabled` property allows this new external shuffle behavior to be toggled (independently off the built-in Spark external shuffle service).",spark,apache,bsidhom,1359900,MDQ6VXNlcjEzNTk5MDA=,https://avatars3.githubusercontent.com/u/1359900?v=4,,https://api.github.com/users/bsidhom,https://github.com/bsidhom,https://api.github.com/users/bsidhom/followers,https://api.github.com/users/bsidhom/following{/other_user},https://api.github.com/users/bsidhom/gists{/gist_id},https://api.github.com/users/bsidhom/starred{/owner}{/repo},https://api.github.com/users/bsidhom/subscriptions,https://api.github.com/users/bsidhom/orgs,https://api.github.com/users/bsidhom/repos,https://api.github.com/users/bsidhom/events{/privacy},https://api.github.com/users/bsidhom/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24462,https://github.com/apache/spark/pull/24462,https://github.com/apache/spark/pull/24462.diff,https://github.com/apache/spark/pull/24462.patch
292,https://api.github.com/repos/apache/spark/issues/24457,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24457/labels{/name},https://api.github.com/repos/apache/spark/issues/24457/comments,https://api.github.com/repos/apache/spark/issues/24457/events,https://github.com/apache/spark/pull/24457,437145276,MDExOlB1bGxSZXF1ZXN0MjczNDgyMjUz,24457,[SPARK-27340][SS] Alias on TimeWindow expression may cause watermark metadata lost,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-04-25T11:19:55Z,2019-09-16T18:14:07Z,,NONE,"## What changes were proposed in this pull request?

`window($""fooTime"", ""2 seconds"").alias(""fooWindow"")` can generate an expression tree  `Alias(fooWindow) <- TimeWindow`. The tree will become `Alias(fooWindow) <- Alias(window) <- Window(start, end)`  after analyzed by TimeWindowing rule. The `Alias(window)` got metadata of watermark when created:
```
val windowStruct = Alias(getWindow(0, 1), WINDOW_COL_NAME)(
exprId = windowAttr.exprId, explicitMetadata = Some(metadata))
``` 
but the `Alias(fooWindow)` is created before  TimeWindowing rule effected. Its code path is:
```
...
case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
...
```
before TimeWindowing rule effected, the `ne.metadata`  is  None and cause the watermark metadata lost

We make the `def name(alias: String)` return a `Alias` which  get metadata from its child automatically, when not specifying metadata explicitly.

Thank @LinhongLiu for helping analyzing this problem!

## How was this patch tested?
Add a UT and do the integration tests by run the example in jira successfully and do not throw org.apache.spark.sql.AnalysisException anymore

",spark,apache,LiangchangZ,34741089,MDQ6VXNlcjM0NzQxMDg5,https://avatars1.githubusercontent.com/u/34741089?v=4,,https://api.github.com/users/LiangchangZ,https://github.com/LiangchangZ,https://api.github.com/users/LiangchangZ/followers,https://api.github.com/users/LiangchangZ/following{/other_user},https://api.github.com/users/LiangchangZ/gists{/gist_id},https://api.github.com/users/LiangchangZ/starred{/owner}{/repo},https://api.github.com/users/LiangchangZ/subscriptions,https://api.github.com/users/LiangchangZ/orgs,https://api.github.com/users/LiangchangZ/repos,https://api.github.com/users/LiangchangZ/events{/privacy},https://api.github.com/users/LiangchangZ/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24457,https://github.com/apache/spark/pull/24457,https://github.com/apache/spark/pull/24457.diff,https://github.com/apache/spark/pull/24457.patch
293,https://api.github.com/repos/apache/spark/issues/24447,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24447/labels{/name},https://api.github.com/repos/apache/spark/issues/24447/comments,https://api.github.com/repos/apache/spark/issues/24447/events,https://github.com/apache/spark/pull/24447,436470145,MDExOlB1bGxSZXF1ZXN0MjcyOTU0OTQx,24447,[SPARK-27562][Shuffle]Complete the verification mechanism for shuffle transmitted data,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-04-24T02:39:38Z,2019-09-16T18:17:10Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

We've seen some shuffle data corruption during shuffle read phase. 

As described in SPARK-26089, spark only checks small  shuffle blocks before PR #23453, which is proposed by ankuriitg.

There are two changes/improvements that are made in PR #23453.

1. Large blocks are checked upto maxBytesInFlight/3 size in a similar way as smaller blocks, so if a
large block is corrupt in the starting, that block will be re-fetched and if that also fails,
FetchFailureException will be thrown.
2. If large blocks are corrupt after size maxBytesInFlight/3, then any IOException thrown while
reading the stream will be converted to FetchFailureException. This is slightly more aggressive
than was originally intended but since the consumer of the stream may have already read some records and processed them, we can't just re-fetch the block, we need to fail the whole task. Additionally, we also thought about maybe adding a new type of TaskEndReason, which would re-try the task couple of times before failing the previous stage, but given the complexity involved in that solution we decided to not proceed in that direction.

However, I think there still exists some problems with the current shuffle transmitted data verification mechanism:

- For a large block, it is checked upto  maxBytesInFlight/3 size when fetching shuffle data. So if a large block is corrupt after size maxBytesInFlight/3, it can not be detected in data fetch phase.  This has been described in the previous section.
- Only the compressed or wrapped blocks are checked, I think we should also check thease blocks which are not wrapped.

This pr complete the verification mechanism for shuffle transmitted data:

Firstly, crc32 is choosed for the checksum verification  of shuffle data.

Crc is also used for checksum verification in hadoop, it is simple and fast.

During shuffle write phase, after completing the partitionedFile, we compute 

the crc32 value for each partition and then write these digests with the indexs into shuffle index file.

For the sortShuffleWriter and unsafe shuffle writer, there is only one partitionedFile for a shuffleMapTask, so the compution of digests(compute the digests for each partition depend on the indexs of this partitionedFile) is  cheap.

For the bypassShuffleWriter, the reduce partitions is little than byPassMergeThreshold, so the cost of digests compution is acceptable.

During shuffle read phase, the digest value will be passed with the block data.

And we will recompute the digest of the data obtained to compare with the origin digest value.
When recomputing the digest of data obtained, it only need an additional buffer(2048Bytes) for computing crc32 value.
After recomputing, we will reset the obtained data inputStream, if it is markSupported we only need reset it, otherwise it is a fileSegmentManagerBuffer, we need recreate it.

So, I think this verification mechanism  proposed for shuffle transmitted data is efficient and complete.

## How was this patch tested?

Unit test.
",spark,apache,turboFei,6757692,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24447,https://github.com/apache/spark/pull/24447,https://github.com/apache/spark/pull/24447.diff,https://github.com/apache/spark/pull/24447.patch
294,https://api.github.com/repos/apache/spark/issues/24440,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24440/labels{/name},https://api.github.com/repos/apache/spark/issues/24440/comments,https://api.github.com/repos/apache/spark/issues/24440/events,https://github.com/apache/spark/pull/24440,436029512,MDExOlB1bGxSZXF1ZXN0MjcyNjA2Mjc1,24440,[SPARK-27545] [SQL] Uncache table needs to delete the temporary view ‚Ä¶,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-04-23T07:01:47Z,2019-09-17T04:25:06Z,,CONTRIBUTOR,"‚Ä¶created when the cache table is executed

## What changes were proposed in this pull request?

When executing the uncache table, delete the temporary table created when the cache table is executed.

## How was this patch tested?

 unit tests

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,httfighter,15611708,MDQ6VXNlcjE1NjExNzA4,https://avatars0.githubusercontent.com/u/15611708?v=4,,https://api.github.com/users/httfighter,https://github.com/httfighter,https://api.github.com/users/httfighter/followers,https://api.github.com/users/httfighter/following{/other_user},https://api.github.com/users/httfighter/gists{/gist_id},https://api.github.com/users/httfighter/starred{/owner}{/repo},https://api.github.com/users/httfighter/subscriptions,https://api.github.com/users/httfighter/orgs,https://api.github.com/users/httfighter/repos,https://api.github.com/users/httfighter/events{/privacy},https://api.github.com/users/httfighter/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24440,https://github.com/apache/spark/pull/24440,https://github.com/apache/spark/pull/24440.diff,https://github.com/apache/spark/pull/24440.patch
295,https://api.github.com/repos/apache/spark/issues/24438,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24438/labels{/name},https://api.github.com/repos/apache/spark/issues/24438/comments,https://api.github.com/repos/apache/spark/issues/24438/events,https://github.com/apache/spark/pull/24438,435985655,MDExOlB1bGxSZXF1ZXN0MjcyNTczNjc5,24438,[SPARK-23626][CORE] DAGScheduler blocked due to JobSubmitted event,"[{'id': 1406606875, 'node_id': 'MDU6TGFiZWwxNDA2NjA2ODc1', 'url': 'https://api.github.com/repos/apache/spark/labels/SCHEDULER', 'name': 'SCHEDULER', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-04-23T03:47:50Z,2019-09-16T18:18:30Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

DAGScheduler becomes a bottleneck in cluster when multiple JobSubmitted events has to be processed as DAGSchedulerEventProcessLoop is single threaded and it will block other tasks in queue like TaskCompletion.
The JobSubmitted event is time consuming depending on the nature of the job (Example: calculating parent stage dependencies, shuffle dependencies, partitions) and thus it blocks all the events to be processed.

Similarly in my cluster some jobs partition calculation is time consuming (Similar to stack at SPARK-2647) hence it slows down the spark DAGSchedulerEventProcessLoop which results in user jobs to slowdown, even if its tasks are finished within seconds, as TaskCompletion Events are processed at a slower rate due to blockage.

Refer: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Scheduler-Spark-DAGScheduler-scheduling-performance-hindered-on-JobSubmitted-Event-td23562.html 

I see multiple JIRA referring to this behavior
https://issues.apache.org/jira/browse/SPARK-2647
https://issues.apache.org/jira/browse/SPARK-4961

Forcing partition evaluation before submitting job can help in mitigation

## How was this patch tested?

1) Added UT  
2) Manual test to verify blockage before and after applying patch.
",spark,apache,ajithme,22072336,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24438,https://github.com/apache/spark/pull/24438,https://github.com/apache/spark/pull/24438.diff,https://github.com/apache/spark/pull/24438.patch
296,https://api.github.com/repos/apache/spark/issues/24421,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24421/labels{/name},https://api.github.com/repos/apache/spark/issues/24421/comments,https://api.github.com/repos/apache/spark/issues/24421/events,https://github.com/apache/spark/pull/24421,435346393,MDExOlB1bGxSZXF1ZXN0MjcyMTI1NDIw,24421,[SPARK-12312][SQL]Support Kerberos login in JDBC connector,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-04-20T02:19:23Z,2019-11-12T23:36:34Z,,NONE,"## What changes were proposed in this pull request?

Use the provided principal and keytab to do Kerberos login before setting up the JDBC connection.

## How was this patch tested?

Step 1: Configure Microsoft SQL Server

Step 2: Start spark shell 
/spark/bin/spark-shell --master yarn \
  --files hdfs:///spark/admin.keytab

Step 3: Run the following Scala code in spark shell
import java.util.Properties

val dataSrc = ""jdbc""
val hostname = ""master-0.azdata.local""
val port = 1433
val database = ""spark""
val url = s""jdbc:sqlserver://${hostname}:${port};database=${database};integratedSecurity=true;authenticationScheme=JavaKerberos""

val df = Seq(
  (8, ""bat""),
  (64, ""mouse""),
  (-27, ""horse"")
).toDF(""number"", ""word"")

import org.apache.spark.sql.SaveMode  

df.write.mode(SaveMode.Overwrite).format(""jdbc"").option(""principal"", ""admin@AZDATA.LOCAL"").option(""keytab"", ""hdfs:///spark/admin.keytab"").option(""url"", url).option(""dbtable"", ""spark"").save()

val outDf = spark.read.format(""jdbc"").option(""principal"", ""admin@AZDATA.LOCAL"").option(""keytab"", ""admin.keytab"").option(""url"", url).option(""dbtable"", ""spark"").load()

outDf.show()",spark,apache,xuzikun2003,25806935,MDQ6VXNlcjI1ODA2OTM1,https://avatars3.githubusercontent.com/u/25806935?v=4,,https://api.github.com/users/xuzikun2003,https://github.com/xuzikun2003,https://api.github.com/users/xuzikun2003/followers,https://api.github.com/users/xuzikun2003/following{/other_user},https://api.github.com/users/xuzikun2003/gists{/gist_id},https://api.github.com/users/xuzikun2003/starred{/owner}{/repo},https://api.github.com/users/xuzikun2003/subscriptions,https://api.github.com/users/xuzikun2003/orgs,https://api.github.com/users/xuzikun2003/repos,https://api.github.com/users/xuzikun2003/events{/privacy},https://api.github.com/users/xuzikun2003/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24421,https://github.com/apache/spark/pull/24421,https://github.com/apache/spark/pull/24421.diff,https://github.com/apache/spark/pull/24421.patch
297,https://api.github.com/repos/apache/spark/issues/24372,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24372/labels{/name},https://api.github.com/repos/apache/spark/issues/24372/comments,https://api.github.com/repos/apache/spark/issues/24372/events,https://github.com/apache/spark/pull/24372,433194829,MDExOlB1bGxSZXF1ZXN0MjcwNDQ3OTcy,24372,[SPARK-27462][SQL] Enhance insert into hive table that could choose some columns in target table flexibly.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2019-04-15T09:58:05Z,2019-09-24T09:23:29Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

ANSI 2003 standard contains syntax about `insert into` as follows:
```
insert into gja_test_spark select * from gja_test;
insert into gja_test_spark select key, value, other from gja_test;
insert into gja_test_spark(key, value, other) select key, value, other from gja_test;
insert into gja_test_spark(key, value, other) select * from gja_test;
insert into gja_test_spark(key, value) select value, other from gja_test;
insert into gja_test_spark(key, other) select value, other from gja_test;
insert into gja_test_spark(value, other) select value, other from gja_test;
```
Hive 1.2.0+ support the syntax about `insert into` as follows:
```
insert into gja_test_spark select * from gja_test;
insert into gja_test_spark select key, value, other from gja_test;
insert into gja_test_spark(key, value, other) select key, value, other from gja_test;
insert into gja_test_spark(key, value, other) select * from gja_test;
insert into gja_test_spark(key, value) select value, other from gja_test;
insert into gja_test_spark(key, other) select value, other from gja_test;
insert into gja_test_spark(value, other) select value, other from gja_test;
insert into table gja_test_partition partition (col2 = 1) select * from gja_test;
insert into table gja_test_partition(key, value, other) partition (col2 = 1) select key, value, other from gja_test;
insert into table gja_test_partition(key, other) partition (col2 = 1) select key, other from gja_test;
```
This means that Hive can support flexible selection of some columns in the target table as the target columns for data insertion. See Description of [HIVE-9481](https://issues.apache.org/jira/browse/HIVE-9481) .
But Spark SQL only support 
```
insert into gja_test_spark select * from gja_test;
insert into gja_test_spark select key, value, other from gja_test;
insert into table gja_test_partition partition (col2 = 1) select * from gja_test;
insert into table gja_test_partition partition (col2 = 1) select key, value, other from gja_test;
```

This PR will enhance the feature. Implemented steps as follows:

**First**
I checked the implement of `HiveOutputWriter`, it writes data according to the index(position) of columns which belongs target table. So I think should use this feature.

**Second**
I add the new syntax and pass `insertedCols` as target columns for data insertion.

**Thrid**
In order to fill in the columns that are not selected, construct empty expressions and fill them in empty index(position) in `Project.projectList`.

**Notice**
This PR only enhance `insert into` for Hive.

## How was this patch tested?

Exists UT and new added UT.",spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24372,https://github.com/apache/spark/pull/24372,https://github.com/apache/spark/pull/24372.diff,https://github.com/apache/spark/pull/24372.patch
298,https://api.github.com/repos/apache/spark/issues/24367,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24367/labels{/name},https://api.github.com/repos/apache/spark/issues/24367/comments,https://api.github.com/repos/apache/spark/issues/24367/events,https://github.com/apache/spark/pull/24367,432905400,MDExOlB1bGxSZXF1ZXN0MjcwMjU2OTEw,24367,[SPARK-27457][SQL] modify bean encoder to support avro objects,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-04-13T21:59:10Z,2019-09-16T18:18:11Z,,NONE,"## What changes were proposed in this pull request?
Currently we modified JavaTypeInference to be able to create encoders for Avro objects; we have now three solutions, the ones in the PR https://github.com/apache/spark/pull/24299, and https://github.com/apache/spark/pull/22878 and this PR (fewer code changes); which one is better? 

## How was this patch tested?

We added one test in ExpressionencoderSuite and used the following program to test it locally:

      implicit val avroExampleEncoder = Encoders.bean[AvroExample1](classOf[AvroExample1]).asInstanceOf[ExpressionEncoder[AvroExample1]]
      val input: AvroExample1 = AvroExample1.newBuilder()
        .setMyarray(List(""Foo"", ""Bar"").asJava)
        .setMyboolean(true)
        .setMybytes(java.nio.ByteBuffer.wrap(""MyBytes"".getBytes()))
        .setMydouble(2.5)
        .setMyfixed(new Magic(""magic"".getBytes))
        .setMyfloat(25.0F)
        .setMyint(100)
        .setMylong(10L)
        .setMystring(""hello"")
        .setMymap(Map(
          ""foo"" -> new java.lang.Integer(1),
          ""bar"" -> new java.lang.Integer(2)).asJava)
        .setMymoney(Money.newBuilder().setAmount(100.0F).setCurrency(Currency.EUR).build())
        .build()
  
      val row: InternalRow = avroExampleEncoder.toRow(input)
  
      val output: AvroExample1 = avroExampleEncoder.resolveAndBind().fromRow(row)
  
      val ds: Dataset[AvroExample1] = List(input).toDS()
  
      println(ds.schema)
      println(ds.collect().toList)
  
      ds.write.format(""avro"").save(""example1"")
  
      val fooDF = spark.read.format(""avro"").load(""example1"")
  
      val fooDS = fooDF.as[AvroExample1]
  
      println(fooDS.collect().toList)
",spark,apache,mazeboard,7288427,MDQ6VXNlcjcyODg0Mjc=,https://avatars0.githubusercontent.com/u/7288427?v=4,,https://api.github.com/users/mazeboard,https://github.com/mazeboard,https://api.github.com/users/mazeboard/followers,https://api.github.com/users/mazeboard/following{/other_user},https://api.github.com/users/mazeboard/gists{/gist_id},https://api.github.com/users/mazeboard/starred{/owner}{/repo},https://api.github.com/users/mazeboard/subscriptions,https://api.github.com/users/mazeboard/orgs,https://api.github.com/users/mazeboard/repos,https://api.github.com/users/mazeboard/events{/privacy},https://api.github.com/users/mazeboard/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24367,https://github.com/apache/spark/pull/24367,https://github.com/apache/spark/pull/24367.diff,https://github.com/apache/spark/pull/24367.patch
299,https://api.github.com/repos/apache/spark/issues/24350,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24350/labels{/name},https://api.github.com/repos/apache/spark/issues/24350/comments,https://api.github.com/repos/apache/spark/issues/24350/events,https://github.com/apache/spark/pull/24350,432113784,MDExOlB1bGxSZXF1ZXN0MjY5NjQ1NTMy,24350,[SPARK-27348][Core] HeartbeatReceiver should remove lost executors from CoarseGrainedSchedulerBackend,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-04-11T16:21:59Z,2019-12-21T00:26:04Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When a heartbeat timeout happens in HeartbeatReceiver, it doesn't remove lost executors from CoarseGrainedSchedulerBackend. When a connection of an executor is not gracefully shut down, CoarseGrainedSchedulerBackend may not receive a disconnect event. In this case, CoarseGrainedSchedulerBackend still thinks a lost executor is still alive. CoarseGrainedSchedulerBackend may ask TaskScheduler to run tasks on this lost executor. This task will never finish and the job will hang forever.

## How was this patch tested?

Add UT in HeartbeatReceiverSuite.
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24350,https://github.com/apache/spark/pull/24350,https://github.com/apache/spark/pull/24350.diff,https://github.com/apache/spark/pull/24350.patch
300,https://api.github.com/repos/apache/spark/issues/24323,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24323/labels{/name},https://api.github.com/repos/apache/spark/issues/24323/comments,https://api.github.com/repos/apache/spark/issues/24323/events,https://github.com/apache/spark/pull/24323,430896749,MDExOlB1bGxSZXF1ZXN0MjY4NjgyMTIw,24323,[SPARK-27413][SS] keep the same epoch pace between driver and executor.,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,27,2019-04-09T10:42:21Z,2019-09-16T19:03:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

The pace of epoch generation in driver and epoch pulling in executor is different. It will result in many empty epochs for partition if the epoch pulling interval is larger than epoch generation.

## How was this patch tested?

update existing unit tests.
",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24323,https://github.com/apache/spark/pull/24323,https://github.com/apache/spark/pull/24323.diff,https://github.com/apache/spark/pull/24323.patch
301,https://api.github.com/repos/apache/spark/issues/24299,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24299/labels{/name},https://api.github.com/repos/apache/spark/issues/24299/comments,https://api.github.com/repos/apache/spark/issues/24299/events,https://github.com/apache/spark/pull/24299,429473220,MDExOlB1bGxSZXF1ZXN0MjY3NjA0MDYz,24299,[SPARK-27388][SQL] encoder for objects defined by properties (ie. Avro),"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-04-04T20:40:01Z,2019-09-16T18:18:19Z,,NONE,"## What changes were proposed in this pull request?

This PR adds expression encoders for beans, java.util.List, java.util.Map and java enum. This addition make it possible to encode Avro objects.

The Beans are objects defined by properties; A property is defined by a setter and a getter functions where the getter return type is equal to the setter unique parameter type and the getter and setter functions have the same name; if the getter name is prefixed by ""get"" then the setter name must be prefixed by ""set""; see tests for bean examples.

Avro objects are beans and thus we can create an expression encoder for avro objects as follows:
```
implicit val exprEncoder = ExpressionEncoder[Foo]()
```
All avro types, including fixed types, and excluding complex union types, are suppported by this addition.

The avro fixed types are beans  with exactly one property: bytes.

Currently complex avro unions are not supported because a complex union is declared as Object and there is no expression encoder for Object type (need to use a custom serializer like kryo for example)

## How was this patch tested?

currently only 1 encodeDecodeTest was added to ExpressionEncoderSuite; the test uses an avro object with all accepted types (ie. array, map, fixed, bytes, enum, ...). 

```
    AvroExample1.newBuilder()
      .setMyarray(List(""Foo"", ""Bar"").asJava)
      .setMyboolean(true)
      .setMybytes(java.nio.ByteBuffer.wrap(""MyBytes"".getBytes()))
      .setMydouble(2.5)
      .setMyfixed(new Magic(""magic"".getBytes))
      .setMyfloat(25.0F)
      .setMyint(100)
      .setMylong(10L)
      .setMystring(""hello"")
      .setMymap(Map(
        ""foo"" -> new java.lang.Integer(1),
        ""bar"" -> new java.lang.Integer(2)).asJava)
      .setMymoney(Money.newBuilder().setAmount(100.0F).setCurrency(Currency.EUR).build())
      .build(),
    ""Avro encoder with map, array and fixed types"")(
    ExpressionEncoder[AvroExample1])
```",spark,apache,mazeboard,7288427,MDQ6VXNlcjcyODg0Mjc=,https://avatars0.githubusercontent.com/u/7288427?v=4,,https://api.github.com/users/mazeboard,https://github.com/mazeboard,https://api.github.com/users/mazeboard/followers,https://api.github.com/users/mazeboard/following{/other_user},https://api.github.com/users/mazeboard/gists{/gist_id},https://api.github.com/users/mazeboard/starred{/owner}{/repo},https://api.github.com/users/mazeboard/subscriptions,https://api.github.com/users/mazeboard/orgs,https://api.github.com/users/mazeboard/repos,https://api.github.com/users/mazeboard/events{/privacy},https://api.github.com/users/mazeboard/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24299,https://github.com/apache/spark/pull/24299,https://github.com/apache/spark/pull/24299.diff,https://github.com/apache/spark/pull/24299.patch
302,https://api.github.com/repos/apache/spark/issues/24283,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24283/labels{/name},https://api.github.com/repos/apache/spark/issues/24283/comments,https://api.github.com/repos/apache/spark/issues/24283/events,https://github.com/apache/spark/pull/24283,428774130,MDExOlB1bGxSZXF1ZXN0MjY3MDUyMjk1,24283,[SPARK-27355][SS] Make query execution more sensitive to epoch message late or lost,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-04-03T13:39:45Z,2019-09-16T19:03:13Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

In [SPARK-23503](https://github.com/apache/spark/pull/20936), we enforce sequencing of committed epochs for Continuous Execution. In case a message for epoch n is lost and epoch (n + 1) is ready for commit before epoch n is, epoch (n + 1) will wait for epoch n to be committed first. With extreme condition, we will wait for `epochBacklogQueueSize` (10000 in default) epochs and then failed. There is no need to wait for such a long time before query fail **if there maybe some message LATE/LOST**. In this PR, we make the condition more sensitive.

## How was this patch tested?

update existing unit tests.",spark,apache,uncleGen,7402327,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24283,https://github.com/apache/spark/pull/24283,https://github.com/apache/spark/pull/24283.diff,https://github.com/apache/spark/pull/24283.patch
303,https://api.github.com/repos/apache/spark/issues/24237,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24237/labels{/name},https://api.github.com/repos/apache/spark/issues/24237/comments,https://api.github.com/repos/apache/spark/issues/24237/events,https://github.com/apache/spark/pull/24237,426393937,MDExOlB1bGxSZXF1ZXN0MjY1MjU0NjMw,24237,[SPARK-27319][SQL] Filter out dir based on PathFilter before listing them,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2019-03-28T09:45:31Z,2019-09-16T18:14:40Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

In `InMemoryFileIndex`, we should filter out dir based on PathFilter before listing them. This is useful in the following case: Consider we have the following data:
```
hdfs://name:port/root-dir/timestamp=2019-01-01/***
hdfs://name:port/root-dir/timestamp=2019-01-02/***
....
hdfs://name:port/root-dir/timestamp=2019-03-04/***
```
We can set a PathFilter when the only part of directories' data is needed. Before this patch, we may need to trigger a Spark job or many local threads to listing all the directories. After patch, we can filter out the directories before listing them.

## How was this patch tested?

Added new UT tests.
",spark,apache,ConeyLiu,12733256,MDQ6VXNlcjEyNzMzMjU2,https://avatars3.githubusercontent.com/u/12733256?v=4,,https://api.github.com/users/ConeyLiu,https://github.com/ConeyLiu,https://api.github.com/users/ConeyLiu/followers,https://api.github.com/users/ConeyLiu/following{/other_user},https://api.github.com/users/ConeyLiu/gists{/gist_id},https://api.github.com/users/ConeyLiu/starred{/owner}{/repo},https://api.github.com/users/ConeyLiu/subscriptions,https://api.github.com/users/ConeyLiu/orgs,https://api.github.com/users/ConeyLiu/repos,https://api.github.com/users/ConeyLiu/events{/privacy},https://api.github.com/users/ConeyLiu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24237,https://github.com/apache/spark/pull/24237,https://github.com/apache/spark/pull/24237.diff,https://github.com/apache/spark/pull/24237.patch
304,https://api.github.com/repos/apache/spark/issues/24230,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24230/labels{/name},https://api.github.com/repos/apache/spark/issues/24230/comments,https://api.github.com/repos/apache/spark/issues/24230/events,https://github.com/apache/spark/pull/24230,426102157,MDExOlB1bGxSZXF1ZXN0MjY1MDM2Nzgy,24230,[SPARK-27295][GraphX] Provision to provide the initial scores for source nodes while running Personalized Page Rank,"[{'id': 1406607243, 'node_id': 'MDU6TGFiZWwxNDA2NjA3MjQz', 'url': 'https://api.github.com/repos/apache/spark/labels/GRAPHX', 'name': 'GRAPHX', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-03-27T17:49:49Z,2019-11-21T01:09:05Z,,NONE,"## What changes were proposed in this pull request?

The present implementation of parallel personalized page rank algorithm takes only node ids as the starting nodes for algorithm. And then it assigns initial value of 1.0 to all those source nodes.

But the user might also be interested in specifying the initial values for each node.",spark,apache,EshwarSR,10551366,MDQ6VXNlcjEwNTUxMzY2,https://avatars2.githubusercontent.com/u/10551366?v=4,,https://api.github.com/users/EshwarSR,https://github.com/EshwarSR,https://api.github.com/users/EshwarSR/followers,https://api.github.com/users/EshwarSR/following{/other_user},https://api.github.com/users/EshwarSR/gists{/gist_id},https://api.github.com/users/EshwarSR/starred{/owner}{/repo},https://api.github.com/users/EshwarSR/subscriptions,https://api.github.com/users/EshwarSR/orgs,https://api.github.com/users/EshwarSR/repos,https://api.github.com/users/EshwarSR/events{/privacy},https://api.github.com/users/EshwarSR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24230,https://github.com/apache/spark/pull/24230,https://github.com/apache/spark/pull/24230.diff,https://github.com/apache/spark/pull/24230.patch
305,https://api.github.com/repos/apache/spark/issues/24228,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24228/labels{/name},https://api.github.com/repos/apache/spark/issues/24228/comments,https://api.github.com/repos/apache/spark/issues/24228/events,https://github.com/apache/spark/pull/24228,425837974,MDExOlB1bGxSZXF1ZXN0MjY0ODMzMjU5,24228,[SPARK-27280][SQL]infer more filter from join or condition,"[{'id': 1406605297, 'node_id': 'MDU6TGFiZWwxNDA2NjA1Mjk3', 'url': 'https://api.github.com/repos/apache/spark/labels/OPTIMIZER', 'name': 'OPTIMIZER', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-03-27T09:05:07Z,2019-09-16T19:04:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
current spark can handle this query to infer fitler from join condition:
```
select * from t1 join t2 where ((t1.a = t2.d and t1.b = 1) or (t1.a = t2.d and t1.b = 5))

(t1.b =1 or t1.b =5) can be infered, and pushed to table t1.
```
but it can not handle following query:
```
select * from t1 join t2 where (t1.a = t2.d and t1.b = 1 and t2.e=3 ) or (t1.a = t2.d and t1.b = 5 and t2.e=7)

(t1.b=1 or t1.b=5) should be infered and push to t1
(t2.e=3 or t2.e=7) should be infered and push to t2

```

## How was this patch tested?
unit test added",spark,apache,windpiger,12979185,MDQ6VXNlcjEyOTc5MTg1,https://avatars3.githubusercontent.com/u/12979185?v=4,,https://api.github.com/users/windpiger,https://github.com/windpiger,https://api.github.com/users/windpiger/followers,https://api.github.com/users/windpiger/following{/other_user},https://api.github.com/users/windpiger/gists{/gist_id},https://api.github.com/users/windpiger/starred{/owner}{/repo},https://api.github.com/users/windpiger/subscriptions,https://api.github.com/users/windpiger/orgs,https://api.github.com/users/windpiger/repos,https://api.github.com/users/windpiger/events{/privacy},https://api.github.com/users/windpiger/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24228,https://github.com/apache/spark/pull/24228,https://github.com/apache/spark/pull/24228.diff,https://github.com/apache/spark/pull/24228.patch
306,https://api.github.com/repos/apache/spark/issues/24218,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24218/labels{/name},https://api.github.com/repos/apache/spark/issues/24218/comments,https://api.github.com/repos/apache/spark/issues/24218/events,https://github.com/apache/spark/pull/24218,425292809,MDExOlB1bGxSZXF1ZXN0MjY0NDIzNzkz,24218,[SPARK-27281][DStreams] Change the way latest kafka offsets are retrieved to consumer#endOffsets,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-03-26T08:52:23Z,2019-09-16T18:18:01Z,,NONE,"## What changes were proposed in this pull request?
Change the way latest kafka offsets are retrieved in `latestOffsets` methods from
```
consumer#seekToEnd(tp)
consumer#position(tp)
```
to
```
consumer#endOffsets(partitions)
```

This fixed the issue from corresponding jira issue.

With existing code from time to time I get an error
```
java.lang.IllegalArgumentException: requirement failed: numRecords must not be negative
at scala.Predef$.require(Predef.scala:224)
at org.apache.spark.streaming.scheduler.StreamInputInfo.<init>(InputInfoTracker.scala:38)
at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:250)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
at scala.Option.orElse(Option.scala:289)
at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
at scala.util.Try$.apply(Try.scala:192)
at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
19/01/29 13:10:00 ERROR apps.BusinessRuleEngine: Job failed. Stopping JVM
java.lang.IllegalArgumentException: requirement failed: numRecords must not be negative
at scala.Predef$.require(Predef.scala:224)
at org.apache.spark.streaming.scheduler.StreamInputInfo.<init>(InputInfoTracker.scala:38)
at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:250)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
at scala.Option.orElse(Option.scala:289)
at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
at scala.util.Try$.apply(Try.scala:192)
at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
java.lang.IllegalArgumentException: requirement failed: numRecords must not be negative
at scala.Predef$.require(Predef.scala:224)
at org.apache.spark.streaming.scheduler.StreamInputInfo.<init>(InputInfoTracker.scala:38)
at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:250)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
at scala.Option.orElse(Option.scala:289)
at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
at scala.util.Try$.apply(Try.scala:192)
at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
```
With 10+ jobs consuming 100+ partitions this happens only once a day - once a week. The code seekToEnd returns offsets that are lower that `currentOffsets`, while `consumer#endOffsets` returns correct end offsets


## How was this patch tested?

Existing tests
",spark,apache,vkrot,4815528,MDQ6VXNlcjQ4MTU1Mjg=,https://avatars3.githubusercontent.com/u/4815528?v=4,,https://api.github.com/users/vkrot,https://github.com/vkrot,https://api.github.com/users/vkrot/followers,https://api.github.com/users/vkrot/following{/other_user},https://api.github.com/users/vkrot/gists{/gist_id},https://api.github.com/users/vkrot/starred{/owner}{/repo},https://api.github.com/users/vkrot/subscriptions,https://api.github.com/users/vkrot/orgs,https://api.github.com/users/vkrot/repos,https://api.github.com/users/vkrot/events{/privacy},https://api.github.com/users/vkrot/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24218,https://github.com/apache/spark/pull/24218,https://github.com/apache/spark/pull/24218.diff,https://github.com/apache/spark/pull/24218.patch
307,https://api.github.com/repos/apache/spark/issues/24215,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24215/labels{/name},https://api.github.com/repos/apache/spark/issues/24215/comments,https://api.github.com/repos/apache/spark/issues/24215/events,https://github.com/apache/spark/pull/24215,425210563,MDExOlB1bGxSZXF1ZXN0MjY0MzYxMjAy,24215,[SPARK-27229][SQL] GroupBy Placement in Intersect Distinct,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2019-03-26T03:33:13Z,2019-09-16T19:04:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Intersect  operator will be replace by Left Semi Join in Optimizer.

for example:
```
SELECT a1, a2 FROM Tab1 INTERSECT SELECT b1, b2 FROM Tab2
 ==>  SELECT DISTINCT a1, a2 FROM Tab1 LEFT SEMI JOIN Tab2 ON a1<=>b1 AND a2<=>b2
```

if Tabe1 and Tab2 are too large, the join will be very slow, we can reduce the table data before
Join by place groupby operator under join, that is 
```
==>  
SELECT a1, a2 FROM 
   (SELECT a1,a2 FROM Tab1 GROUP BY a1,a2) X
   LEFT SEMI JOIN 
   (SELECT b1,b2 FROM Tab2 GROUP BY b1,b2) Y
ON X.a1<=>Y.b1 AND X.a2<=>Y.b2
```

then we can have smaller table data when execute join, because  group by has cut lots of 
 data.
 
## How was this patch tested?
uinit test added
",spark,apache,windpiger,12979185,MDQ6VXNlcjEyOTc5MTg1,https://avatars3.githubusercontent.com/u/12979185?v=4,,https://api.github.com/users/windpiger,https://github.com/windpiger,https://api.github.com/users/windpiger/followers,https://api.github.com/users/windpiger/following{/other_user},https://api.github.com/users/windpiger/gists{/gist_id},https://api.github.com/users/windpiger/starred{/owner}{/repo},https://api.github.com/users/windpiger/subscriptions,https://api.github.com/users/windpiger/orgs,https://api.github.com/users/windpiger/repos,https://api.github.com/users/windpiger/events{/privacy},https://api.github.com/users/windpiger/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24215,https://github.com/apache/spark/pull/24215,https://github.com/apache/spark/pull/24215.diff,https://github.com/apache/spark/pull/24215.patch
308,https://api.github.com/repos/apache/spark/issues/24208,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24208/labels{/name},https://api.github.com/repos/apache/spark/issues/24208/comments,https://api.github.com/repos/apache/spark/issues/24208/events,https://github.com/apache/spark/pull/24208,425048016,MDExOlB1bGxSZXF1ZXN0MjY0MjM0NzA4,24208,[SPARK-27272][CORE] Enable blacklisting of node/executor on fetch failures by default,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2019-03-25T18:17:03Z,2019-10-15T00:29:02Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

SPARK-20898 added a new configuration to blacklist a node/executor on fetch
failures. This config was deemed risky at the time and was disabled by default
until more data is collected.

This commit aims to enable that feature by default as we have seen couple of
instances where that feature was found to be useful. Additionally, the commit
changes the blacklist criteria slightly.

If external shuffle service is not enabled, the commit will blacklist the
executor immediately (on first fetch failure). This is consistent with the fact
that we delete all the shuffle outputs on that executor. So, I think it is
useful that we also blacklist that executor temporarily.

Otherwise, if external shuffle service is enabled, instead of blacklisting the
node immediately, it keeps track of all such fetch failures on that node. If
unique and active fetch failures on that node exceed the configured threshold
(it re-uses MAX_FAILED_EXEC_PER_NODE, but can be changed), then the node is also
blacklisted. This will ensure that persistent issues with a node do not lead to
job failures.

## How was this patch tested?

1. Added a unit test case to ensure that blacklisting works as configured",spark,apache,ankuriitg,1723600,MDQ6VXNlcjE3MjM2MDA=,https://avatars1.githubusercontent.com/u/1723600?v=4,,https://api.github.com/users/ankuriitg,https://github.com/ankuriitg,https://api.github.com/users/ankuriitg/followers,https://api.github.com/users/ankuriitg/following{/other_user},https://api.github.com/users/ankuriitg/gists{/gist_id},https://api.github.com/users/ankuriitg/starred{/owner}{/repo},https://api.github.com/users/ankuriitg/subscriptions,https://api.github.com/users/ankuriitg/orgs,https://api.github.com/users/ankuriitg/repos,https://api.github.com/users/ankuriitg/events{/privacy},https://api.github.com/users/ankuriitg/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24208,https://github.com/apache/spark/pull/24208,https://github.com/apache/spark/pull/24208.diff,https://github.com/apache/spark/pull/24208.patch
309,https://api.github.com/repos/apache/spark/issues/24173,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24173/labels{/name},https://api.github.com/repos/apache/spark/issues/24173/comments,https://api.github.com/repos/apache/spark/issues/24173/events,https://github.com/apache/spark/pull/24173,423960269,MDExOlB1bGxSZXF1ZXN0MjYzNDE5MDUz,24173,[SPARK-27237][SS] Introduce State schema validation among query restart,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2019-03-21T21:58:01Z,2019-12-06T12:45:19Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Please refer the description of [SPARK-27237](https://issues.apache.org/jira/browse/SPARK-27237) to see rationalization of this patch.

This patch proposes to introduce state schema validation, via storing key schema and value schema to `schema` file (for the first time) and verify new key schema and value schema for state are compatible with existing one. To be clear for definition of ""compatible"", state schema is ""compatible"" when number of fields are same and data type for each field is same - Spark has been allowing rename of field.

This patch will prevent query run which has incompatible state schema, which would reduce the chance to get indeterministic behavior (actually renaming of field is also the smell of semantically incompatible, but end users could just modify its name so we can't say) as well as providing more informative error message.

## How was this patch tested?

Added UTs.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24173,https://github.com/apache/spark/pull/24173,https://github.com/apache/spark/pull/24173.diff,https://github.com/apache/spark/pull/24173.patch
310,https://api.github.com/repos/apache/spark/issues/24151,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24151/labels{/name},https://api.github.com/repos/apache/spark/issues/24151/comments,https://api.github.com/repos/apache/spark/issues/24151/events,https://github.com/apache/spark/pull/24151,423079075,MDExOlB1bGxSZXF1ZXN0MjYyNzI4ODM1,24151,[SPARK-26739][SQL] Standardized Join Types for DataFrames,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-03-20T05:51:41Z,2019-09-17T00:09:04Z,,NONE,"## What changes were proposed in this pull request?
Tries the address the concern mentioned in [SPARK-26739](https://issues.apache.org/jira/browse/SPARK-26739)
To summarise, currently, in the join functions on DataFrames, the join types are defined via a string parameter called joinType. In order for a developer to know which joins are possible, they must look up the API call for join. While this works fine, it can cause the developer to make a typo resulting in improper joins and/or unexpected errors that aren't evident at compile time. The objective of this improvement would be to allow developers to use a common definition for join types (by enum or constants) called JoinTypes. This would contain the possible joins and remove the possibility of a typo. It would also allow Spark to alter the names of the joins in the future without impacting end-users.

## How was this patch tested?
Tested via Unit tests

",spark,apache,agrawalpooja,16685651,MDQ6VXNlcjE2Njg1NjUx,https://avatars2.githubusercontent.com/u/16685651?v=4,,https://api.github.com/users/agrawalpooja,https://github.com/agrawalpooja,https://api.github.com/users/agrawalpooja/followers,https://api.github.com/users/agrawalpooja/following{/other_user},https://api.github.com/users/agrawalpooja/gists{/gist_id},https://api.github.com/users/agrawalpooja/starred{/owner}{/repo},https://api.github.com/users/agrawalpooja/subscriptions,https://api.github.com/users/agrawalpooja/orgs,https://api.github.com/users/agrawalpooja/repos,https://api.github.com/users/agrawalpooja/events{/privacy},https://api.github.com/users/agrawalpooja/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24151,https://github.com/apache/spark/pull/24151,https://github.com/apache/spark/pull/24151.diff,https://github.com/apache/spark/pull/24151.patch
311,https://api.github.com/repos/apache/spark/issues/24128,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24128/labels{/name},https://api.github.com/repos/apache/spark/issues/24128/comments,https://api.github.com/repos/apache/spark/issues/24128/events,https://github.com/apache/spark/pull/24128,422137572,MDExOlB1bGxSZXF1ZXN0MjYxOTkzMzky,24128,[SPARK-27188][SS] FileStreamSink: provide a new option to have retention on output files,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2019-03-18T10:20:43Z,2019-12-19T01:59:38Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This patch proposes to provide a new option to specify time-to-live (TTL) for output file entries in FileStreamSink.

The metadata log greatly helps to easily achieve exactly-once but given the output path is open to arbitrary readers, there's no way to compact the metadata log, which ends up growing the metadata file as query runs for long time, especially for compacted batch.
(There're some reports from end users which include their workarounds: [SPARK-24295](https://issues.apache.org/jira/browse/SPARK-24295))

This patch will filter out outdated output files in metadata while compacting batches, which helps metadata to not grow linearly, as well as filtered out files will be ""eventually"" no longer seen in reader queries which leverage File(Stream)Source.

## How was this patch tested?

Added unit tests.",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24128,https://github.com/apache/spark/pull/24128,https://github.com/apache/spark/pull/24128.diff,https://github.com/apache/spark/pull/24128.patch
312,https://api.github.com/repos/apache/spark/issues/24118,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24118/labels{/name},https://api.github.com/repos/apache/spark/issues/24118/comments,https://api.github.com/repos/apache/spark/issues/24118/events,https://github.com/apache/spark/pull/24118,421889012,MDExOlB1bGxSZXF1ZXN0MjYxODIyNjg4,24118,[SPARK-26736][SQL] if filter condition `And` has non-determined sub function it does not do partition prunning,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2019-03-17T06:47:27Z,2019-09-16T18:15:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

If filter condition `And` includes non-determined sub expression partition prunning will not work. This patch will take out the determined sub expression in `And` to make the partition prunning work. 

Example:
A  partitioned table definition:
`create table test(id int) partitioned by (dt string);`
The following sql does not do partition prunning:
`select * from test where dt='20190101' and rand() < 0.5;`

This PR will fix this problem.

## How was this patch tested?

It will be tested in `PruningSuite` by adding test case `Partition pruning - with filter containing non-determined condition`

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,zhaorongsheng,4468567,MDQ6VXNlcjQ0Njg1Njc=,https://avatars2.githubusercontent.com/u/4468567?v=4,,https://api.github.com/users/zhaorongsheng,https://github.com/zhaorongsheng,https://api.github.com/users/zhaorongsheng/followers,https://api.github.com/users/zhaorongsheng/following{/other_user},https://api.github.com/users/zhaorongsheng/gists{/gist_id},https://api.github.com/users/zhaorongsheng/starred{/owner}{/repo},https://api.github.com/users/zhaorongsheng/subscriptions,https://api.github.com/users/zhaorongsheng/orgs,https://api.github.com/users/zhaorongsheng/repos,https://api.github.com/users/zhaorongsheng/events{/privacy},https://api.github.com/users/zhaorongsheng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24118,https://github.com/apache/spark/pull/24118,https://github.com/apache/spark/pull/24118.diff,https://github.com/apache/spark/pull/24118.patch
313,https://api.github.com/repos/apache/spark/issues/24102,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24102/labels{/name},https://api.github.com/repos/apache/spark/issues/24102/comments,https://api.github.com/repos/apache/spark/issues/24102/events,https://github.com/apache/spark/pull/24102,421376439,MDExOlB1bGxSZXF1ZXN0MjYxNDM5ODY5,24102,[SPARK-27171][SQL] Support Full-Partitons scan in limit for the first time,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2019-03-15T06:48:03Z,2019-09-16T18:17:43Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

SparkPlan#executeTake will pick element starting with one partition. Sometimes it will be slow for some queries. Although, Spark is better at batch query. It's not bad to add a switch to allow user control the ratio of partitons for the first time in limit.

## How was this patch tested?

Exist tests.
",spark,apache,deshanxiao,42019462,MDQ6VXNlcjQyMDE5NDYy,https://avatars0.githubusercontent.com/u/42019462?v=4,,https://api.github.com/users/deshanxiao,https://github.com/deshanxiao,https://api.github.com/users/deshanxiao/followers,https://api.github.com/users/deshanxiao/following{/other_user},https://api.github.com/users/deshanxiao/gists{/gist_id},https://api.github.com/users/deshanxiao/starred{/owner}{/repo},https://api.github.com/users/deshanxiao/subscriptions,https://api.github.com/users/deshanxiao/orgs,https://api.github.com/users/deshanxiao/repos,https://api.github.com/users/deshanxiao/events{/privacy},https://api.github.com/users/deshanxiao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24102,https://github.com/apache/spark/pull/24102,https://github.com/apache/spark/pull/24102.diff,https://github.com/apache/spark/pull/24102.patch
314,https://api.github.com/repos/apache/spark/issues/24076,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24076/labels{/name},https://api.github.com/repos/apache/spark/issues/24076/comments,https://api.github.com/repos/apache/spark/issues/24076/events,https://github.com/apache/spark/pull/24076,420306089,MDExOlB1bGxSZXF1ZXN0MjYwNjE0ODcz,24076,[SPARK-27142] Provide REST API for SQL level information,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2019-03-13T03:57:57Z,2019-09-16T18:17:31Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently for Monitoring Spark application SQL information is not available from REST but only via UI. REST provides only applications,jobs,stages,environment. This Jira is targeted to provide a REST API so that SQL level information can be found
A single SQL query can result into multiple jobs. So for end user who is using STS or spark-sql, the intended highest level of probe is the SQL which he has executed. This information can be seen from SQL tab. Attaching a sample. 
![image](https://user-images.githubusercontent.com/22072336/54298729-5524a800-45df-11e9-8e4d-b99a8b882031.png)
But same information he cannot access using the REST API exposed by spark and he always have to rely on jobs API which may be difficult. So i intend to expose the information seen in SQL tab in UI via REST API

Mainly:

executionId :  long
status : string - possible values COMPLETED/RUNNING/FAILED
description : string - executed SQL string
submissionTime : formatted time of SQL submission
duration : string - total run time
runningJobIds : Seq[Int] - sequence of running job ids
failedJobIds : Seq[Int] - sequence of failed job ids
successJobIds : Seq[Int] - sequence of success job ids

## How was this patch tested?

![image](https://user-images.githubusercontent.com/22072336/54282168-6d85ca00-45c1-11e9-8935-7586ccf0efff.png)

![image](https://user-images.githubusercontent.com/22072336/54282191-7b3b4f80-45c1-11e9-941c-f0ec37026192.png)


",spark,apache,ajithme,22072336,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24076,https://github.com/apache/spark/pull/24076,https://github.com/apache/spark/pull/24076.diff,https://github.com/apache/spark/pull/24076.patch
315,https://api.github.com/repos/apache/spark/issues/24010,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/24010/labels{/name},https://api.github.com/repos/apache/spark/issues/24010/comments,https://api.github.com/repos/apache/spark/issues/24010/events,https://github.com/apache/spark/pull/24010,418365564,MDExOlB1bGxSZXF1ZXN0MjU5MTU0NTQ1,24010,[SPARK-26439][CORE][WIP] Introduce WorkerOffer reservation mechanism for Barrier TaskSet,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-03-07T15:20:08Z,2019-09-16T19:05:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, Barrier TaskSet has a hard requirement that tasks can only be launched
in a single resourceOffers round with enough slots(or sufficient resources), but
can not be guaranteed even if with enough slots due to task locality delay scheduling.
So, it is very likely that Barrier TaskSet gets a chunk of sufficient resources after
all the trouble, but let it go easily just because one of pending tasks can not be
scheduled. Futhermore, it causes severe resource competition between TaskSets and jobs
and introduce unclear semantic for DynamicAllocation.

This pr trys to introduce WorkerOffer reservation mechanism for Barrier TaskSet, which
allows Barrier TaskSet to reserve WorkerOffer in each resourceOffers round, and launch
tasks at the same time once it accumulate the sufficient resource. In this way, we
relax the requirement of resources for the Barrier TaskSet.

Besides, we have two features along with WorkerOffer reservation mechanism:

To avoid the deadlock which may be introuduced by serveral Barrier TaskSets holding the reserved WorkerOffers for a long time, we'll ask Barrier TaskSets to force releasing part of reserved WorkerOffers
on demand. So, it is highly possible that each Barrier TaskSet would be launched in the end.

Barrier TaskSet could replace old high level locality reserved WorkerOffer with new low level locality WorkerOffer during the time it wating for sufficient resources, to perform better locality at the end.

To integrate with DynamicAllocation:

The possible effective way I can imagine is that adding new event, e.g.
ExecutorReservedEvent, ExecutorReleasedEvent, which behaved like busy executor with
running tasks or idle executor without running tasks. Thus, ExecutorAllocationManager
would not let the executor go if it reminds of there're some reserved resource on that
executor.

## How was this patch tested?

existed and added some, needs to add more.
",spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/24010,https://github.com/apache/spark/pull/24010,https://github.com/apache/spark/pull/24010.diff,https://github.com/apache/spark/pull/24010.patch
316,https://api.github.com/repos/apache/spark/issues/23988,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23988/labels{/name},https://api.github.com/repos/apache/spark/issues/23988/comments,https://api.github.com/repos/apache/spark/issues/23988/events,https://github.com/apache/spark/pull/23988,417797805,MDExOlB1bGxSZXF1ZXN0MjU4NzE0NzIw,23988,[SPARK-26509][SQL] Parquet DELTA_BYTE_ARRAY is not supported in Spark 2.x's Vectorized Reader,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2019-03-06T13:16:28Z,2019-10-31T02:22:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Implement Parquet delta encoding for vectorized interface, which is needed for V2 pages. The implementation simply delegates the decoding to the Parquet implementation.

## How was this patch tested?

Added new test case for delta encoding, ran unit tests",spark,apache,nandorKollar,12639187,MDQ6VXNlcjEyNjM5MTg3,https://avatars2.githubusercontent.com/u/12639187?v=4,,https://api.github.com/users/nandorKollar,https://github.com/nandorKollar,https://api.github.com/users/nandorKollar/followers,https://api.github.com/users/nandorKollar/following{/other_user},https://api.github.com/users/nandorKollar/gists{/gist_id},https://api.github.com/users/nandorKollar/starred{/owner}{/repo},https://api.github.com/users/nandorKollar/subscriptions,https://api.github.com/users/nandorKollar/orgs,https://api.github.com/users/nandorKollar/repos,https://api.github.com/users/nandorKollar/events{/privacy},https://api.github.com/users/nandorKollar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23988,https://github.com/apache/spark/pull/23988,https://github.com/apache/spark/pull/23988.diff,https://github.com/apache/spark/pull/23988.patch
317,https://api.github.com/repos/apache/spark/issues/23942,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23942/labels{/name},https://api.github.com/repos/apache/spark/issues/23942/comments,https://api.github.com/repos/apache/spark/issues/23942/events,https://github.com/apache/spark/pull/23942,416483210,MDExOlB1bGxSZXF1ZXN0MjU3NzE2NTg1,23942,[SPARK-27033][SQL]Add Optimize rule RewriteArithmeticFiltersOnIntegralColumn,"[{'id': 1406605297, 'node_id': 'MDU6TGFiZWwxNDA2NjA1Mjk3', 'url': 'https://api.github.com/repos/apache/spark/labels/OPTIMIZER', 'name': 'OPTIMIZER', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2019-03-03T06:54:04Z,2019-09-16T18:17:41Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, filters like `select * from table where a + 1 = 3` cannot be pushed down, this optimizer can convert it to `select * from table where a = 3 - 1`, and then optimized to `select * from table where a = 2` by other optimizer, so that it can be pushed down to parquet or other file format. 

The comparison supports =, !=. The operation supports `Add` and `Subtract`. It only supports integral-type (`Byte`, `Short`,`INT` and `LONG`), it doesn't support `FLOAT/DOUBLE` for precision issues. 

## How was this patch tested?

Unit test by RewriteArithmeticFiltersOnIntegralColumnSuite.
",spark,apache,WangGuangxin,1312321,MDQ6VXNlcjEzMTIzMjE=,https://avatars0.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23942,https://github.com/apache/spark/pull/23942,https://github.com/apache/spark/pull/23942.diff,https://github.com/apache/spark/pull/23942.patch
318,https://api.github.com/repos/apache/spark/issues/23921,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23921/labels{/name},https://api.github.com/repos/apache/spark/issues/23921/comments,https://api.github.com/repos/apache/spark/issues/23921/events,https://github.com/apache/spark/pull/23921,415659984,MDExOlB1bGxSZXF1ZXN0MjU3MDk4MzM2,23921,[SPARK-26560][SQL]:Repeat select on HiveUDF fails,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2019-02-28T14:48:32Z,2019-09-16T18:15:41Z,,NONE,"## What changes were proposed in this pull request?

The classloader of spark-shell is IMainsTranslatingClassLoader which is loaded from IMain (scala.tools.nsc.interpreter). But on the first select, it registers the function and loads the hiveUDF jar to sparkContext classloader which is NonClosableMutuableClassLoader. While selecting the function on the second time, function is already registered so it tries to fetch from IMainTranslatingClassLoader  which is giving analysis exception.


Changing of the classLoader of currentThread to NonClosableMutuableclassLoader will solve this issue.
",spark,apache,nivo091,8516175,MDQ6VXNlcjg1MTYxNzU=,https://avatars2.githubusercontent.com/u/8516175?v=4,,https://api.github.com/users/nivo091,https://github.com/nivo091,https://api.github.com/users/nivo091/followers,https://api.github.com/users/nivo091/following{/other_user},https://api.github.com/users/nivo091/gists{/gist_id},https://api.github.com/users/nivo091/starred{/owner}{/repo},https://api.github.com/users/nivo091/subscriptions,https://api.github.com/users/nivo091/orgs,https://api.github.com/users/nivo091/repos,https://api.github.com/users/nivo091/events{/privacy},https://api.github.com/users/nivo091/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23921,https://github.com/apache/spark/pull/23921,https://github.com/apache/spark/pull/23921.diff,https://github.com/apache/spark/pull/23921.patch
319,https://api.github.com/repos/apache/spark/issues/23912,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23912/labels{/name},https://api.github.com/repos/apache/spark/issues/23912/comments,https://api.github.com/repos/apache/spark/issues/23912/events,https://github.com/apache/spark/pull/23912,415325439,MDExOlB1bGxSZXF1ZXN0MjU2ODM4NTc1,23912,[SPARK-21029][SS] StreamingQuery should be stopped when the SparkSession is stopped,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2019-02-27T21:02:45Z,2019-09-17T03:12:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This patch proposes to stop the underlying streaming queries when a spark session is stopped.

Without this, the streaming query continues to run and fails with exception during the next trigger.

For E.g.
```
val lines = spark.readStream.format(""socket"").option(""host"", ""localhost"").option(""port"", ""9999"").load()
lines.writeStream.format(""console"").start
...
> send some input
...
spark.stop
...
> send more input
...
```
The job fails with exception

## How was this patch tested?

Tested manually. Would add unit tests once I receive feedback on the approach.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,arunmahadevan,6792890,MDQ6VXNlcjY3OTI4OTA=,https://avatars0.githubusercontent.com/u/6792890?v=4,,https://api.github.com/users/arunmahadevan,https://github.com/arunmahadevan,https://api.github.com/users/arunmahadevan/followers,https://api.github.com/users/arunmahadevan/following{/other_user},https://api.github.com/users/arunmahadevan/gists{/gist_id},https://api.github.com/users/arunmahadevan/starred{/owner}{/repo},https://api.github.com/users/arunmahadevan/subscriptions,https://api.github.com/users/arunmahadevan/orgs,https://api.github.com/users/arunmahadevan/repos,https://api.github.com/users/arunmahadevan/events{/privacy},https://api.github.com/users/arunmahadevan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23912,https://github.com/apache/spark/pull/23912,https://github.com/apache/spark/pull/23912.diff,https://github.com/apache/spark/pull/23912.patch
320,https://api.github.com/repos/apache/spark/issues/23865,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23865/labels{/name},https://api.github.com/repos/apache/spark/issues/23865/comments,https://api.github.com/repos/apache/spark/issues/23865/events,https://github.com/apache/spark/pull/23865,413159507,MDExOlB1bGxSZXF1ZXN0MjU1MjEwNjY5,23865,[SPARK-26957][SCHEDULER] Config properties for spark dynamic scheduler pools,"[{'id': 1406606875, 'node_id': 'MDU6TGFiZWwxNDA2NjA2ODc1', 'url': 'https://api.github.com/repos/apache/spark/labels/SCHEDULER', 'name': 'SCHEDULER', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2019-02-21T22:45:12Z,2019-12-17T17:26:13Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This fix addresses the issue raised in SPARK-26957.  If a spark.scheduler.pool is set to a name that was not a preconfigured pool, the pool was always created with the default properties.  This fix allows the default scheduler pool properties to be configured through configuration parameters.  Previously they were hardcoded.

The fix is fully backwards compatible because the configuration properties default to the existing hardcoded values.  This PR just allows those values to be updated.  The specific use case where we needed this was explained in SPARK-26957

## How was this patch tested?

A unit test was added to PoolSuite that sets the config values, generates a pool, and checks that the pool is using the configured values.  An existing test verifies that with no special configuration settings, the original defaults are used.

In addition to this unit testing, we are currently using a version of this patch in production on AWS EMR for Spark 2.4.0.

This contribution is my original work and that I license the work to the project under the project‚Äôs open source license.
",spark,apache,DaveDeCaprio,841146,MDQ6VXNlcjg0MTE0Ng==,https://avatars3.githubusercontent.com/u/841146?v=4,,https://api.github.com/users/DaveDeCaprio,https://github.com/DaveDeCaprio,https://api.github.com/users/DaveDeCaprio/followers,https://api.github.com/users/DaveDeCaprio/following{/other_user},https://api.github.com/users/DaveDeCaprio/gists{/gist_id},https://api.github.com/users/DaveDeCaprio/starred{/owner}{/repo},https://api.github.com/users/DaveDeCaprio/subscriptions,https://api.github.com/users/DaveDeCaprio/orgs,https://api.github.com/users/DaveDeCaprio/repos,https://api.github.com/users/DaveDeCaprio/events{/privacy},https://api.github.com/users/DaveDeCaprio/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23865,https://github.com/apache/spark/pull/23865,https://github.com/apache/spark/pull/23865.diff,https://github.com/apache/spark/pull/23865.patch
321,https://api.github.com/repos/apache/spark/issues/23791,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23791/labels{/name},https://api.github.com/repos/apache/spark/issues/23791/comments,https://api.github.com/repos/apache/spark/issues/23791/events,https://github.com/apache/spark/pull/23791,410344138,MDExOlB1bGxSZXF1ZXN0MjUzMTA2NTU3,23791,[SPARK-20597][SQL][SS][WIP] KafkaSourceProvider falls back on path as synonym for topic,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2019-02-14T15:07:06Z,2019-09-16T18:17:21Z,,NONE,"## What changes were proposed in this pull request?

**KafkaSourceProvider** supports 'topic' option that sets target Kafka topic.
**KafkaSourceProvider** can use 'topic' column to assign rows to Kafka topics for writing.

It is proposed to treat 'path' option in start(path: String) and save(path: String) as alternative to setting 'topic' option. Path would designate the target topic when 'topic' option is not specified and will override 'topic' column value for topic.

Currently if topic in 'path' option is specified it overrides topic column and is overridden by 'topic' option (if present). IMHO this relation seems more straightforward than that specified in the ticket ('path' is the least precedence option).

for streaming it will look as follows:
```
df.writeStream
  .format(""kafka"")
  .option(""kafka.bootstrap.servers"", ""host1:port1,host2:port2"")
  .start(""topic"")
```
for batch it will look as follows:
```
df.write
  .format(""kafka"")
  .option(""kafka.bootstrap.servers"", ""host1:port1,host2:port2"")
  .save(""topic"")
```

## How was this patch tested?
Added and extended unit tests, local build&test",spark,apache,Nimfadora,10544767,MDQ6VXNlcjEwNTQ0NzY3,https://avatars2.githubusercontent.com/u/10544767?v=4,,https://api.github.com/users/Nimfadora,https://github.com/Nimfadora,https://api.github.com/users/Nimfadora/followers,https://api.github.com/users/Nimfadora/following{/other_user},https://api.github.com/users/Nimfadora/gists{/gist_id},https://api.github.com/users/Nimfadora/starred{/owner}{/repo},https://api.github.com/users/Nimfadora/subscriptions,https://api.github.com/users/Nimfadora/orgs,https://api.github.com/users/Nimfadora/repos,https://api.github.com/users/Nimfadora/events{/privacy},https://api.github.com/users/Nimfadora/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23791,https://github.com/apache/spark/pull/23791,https://github.com/apache/spark/pull/23791.diff,https://github.com/apache/spark/pull/23791.patch
322,https://api.github.com/repos/apache/spark/issues/23783,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23783/labels{/name},https://api.github.com/repos/apache/spark/issues/23783/comments,https://api.github.com/repos/apache/spark/issues/23783/events,https://github.com/apache/spark/pull/23783,410095766,MDExOlB1bGxSZXF1ZXN0MjUyOTE3MDI0,23783,[SPARK-26854][SQL] Support ANY/SOME subquery,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,77,2019-02-14T02:27:23Z,2019-11-09T01:57:17Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR adds `ANY/SOME` subquery support in SQL.  
ANY/SOME syntax:
``` SQL
SELECT column(s)
FROM table
WHERE column(s) operator ANY 
(SELECT column(s) FROM table WHERE condition)
```
The implementation is based on `IN` subquery. `IN` subquery can be regarded as a special case of `ANY` subquery whose operator should be ""="".  As `IN` subquery will be rewritten as semi/anti join, the only difference between them is that `ANY` subquery should change the comparison operator.

Here are some details:

* There are 7 comparison operators can be used in `ANY` subquery: `=, <=>, >, >=, <, <=, !=`.
All of these operators will be directly used in the join condition. e.g.,
```sql
SELECT t1a FROM t1 WHERE t1b > ANY (SELECT t2b FROM t2)
```
will be rewritten as
```sql
SELECT t1a FROM t1 LEFT SEMI JOIN t2 ON t1.t1b > t2.t2b
 ```

* `a != ANY (select b from c)` is **not equal** to `NOT a = ANY (select b from c)` semantically.
`a != ANY (select b from c)` returns true if `a` is not equal to all rows in the subquery's results.
it will be rewritten as `left semi join c on a != b`. 
 e.g., `3 != ANY(1,3,3,2)` returns true, `3 != ANY(3,3,3)` returns false.
While `NOT a = ANY (select b from c)` returns true if `a` is not equal to every row in the subquery's results. It is a syntax sugar of `a NOT IN (select b from c)`. And it will be rewritten as `left anti join c on a = b`.

* Currently, ANY/SOME subquery only supports  'EqualTo' operation when multiple columns are specified in operands.





## How was this patch tested?
Added simple unit tests.",spark,apache,francis0407,25431723,MDQ6VXNlcjI1NDMxNzIz,https://avatars1.githubusercontent.com/u/25431723?v=4,,https://api.github.com/users/francis0407,https://github.com/francis0407,https://api.github.com/users/francis0407/followers,https://api.github.com/users/francis0407/following{/other_user},https://api.github.com/users/francis0407/gists{/gist_id},https://api.github.com/users/francis0407/starred{/owner}{/repo},https://api.github.com/users/francis0407/subscriptions,https://api.github.com/users/francis0407/orgs,https://api.github.com/users/francis0407/repos,https://api.github.com/users/francis0407/events{/privacy},https://api.github.com/users/francis0407/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23783,https://github.com/apache/spark/pull/23783,https://github.com/apache/spark/pull/23783.diff,https://github.com/apache/spark/pull/23783.patch
323,https://api.github.com/repos/apache/spark/issues/23782,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23782/labels{/name},https://api.github.com/repos/apache/spark/issues/23782/comments,https://api.github.com/repos/apache/spark/issues/23782/events,https://github.com/apache/spark/pull/23782,410084037,MDExOlB1bGxSZXF1ZXN0MjUyOTA4OTk5,23782,[SPARK-26875][SS] Add an option on FileStreamSource to include modified files,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-02-14T01:35:23Z,2019-11-20T00:30:15Z,,NONE,"## What changes were proposed in this pull request?

The current behavior only the check the filename to determine if a file should be processed or not. I propose to add an option to also test the file timestamp if is greater than last time it was processed, as an indication that it's modified and have different content. 

It is useful when the source producer eventually overrides files with new content.

## How was this patch tested?

Added unit tests.
",spark,apache,mikedias,472304,MDQ6VXNlcjQ3MjMwNA==,https://avatars0.githubusercontent.com/u/472304?v=4,,https://api.github.com/users/mikedias,https://github.com/mikedias,https://api.github.com/users/mikedias/followers,https://api.github.com/users/mikedias/following{/other_user},https://api.github.com/users/mikedias/gists{/gist_id},https://api.github.com/users/mikedias/starred{/owner}{/repo},https://api.github.com/users/mikedias/subscriptions,https://api.github.com/users/mikedias/orgs,https://api.github.com/users/mikedias/repos,https://api.github.com/users/mikedias/events{/privacy},https://api.github.com/users/mikedias/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23782,https://github.com/apache/spark/pull/23782,https://github.com/apache/spark/pull/23782.diff,https://github.com/apache/spark/pull/23782.patch
324,https://api.github.com/repos/apache/spark/issues/23762,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23762/labels{/name},https://api.github.com/repos/apache/spark/issues/23762/comments,https://api.github.com/repos/apache/spark/issues/23762/events,https://github.com/apache/spark/pull/23762,409201033,MDExOlB1bGxSZXF1ZXN0MjUyMjM4Nzcz,23762,[SPARK-21492][SQL][WIP] Memory leak in SortMergeJoin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-02-12T09:31:41Z,2019-09-16T18:17:10Z,,NONE,"## What changes were proposed in this pull request?

If SortMergeJoinScanner doesn't consume the iterator from
UnsafeExternalRowSorter entirely, the memory that
UnsafeExternalSorter acquired from TaskMemoryManager will not
be released. This leads to a memory leak, spills, and OOME. A
page will be held per partition of the unused iterator.
This patch will allow the SortMergeJoinScanner to explicitly close the iterators (for non-generated code)

## How was this patch tested?

Manual testing and profiling with scripts in SPARK-21492 comments.

",spark,apache,taosaildrone,42982007,MDQ6VXNlcjQyOTgyMDA3,https://avatars1.githubusercontent.com/u/42982007?v=4,,https://api.github.com/users/taosaildrone,https://github.com/taosaildrone,https://api.github.com/users/taosaildrone/followers,https://api.github.com/users/taosaildrone/following{/other_user},https://api.github.com/users/taosaildrone/gists{/gist_id},https://api.github.com/users/taosaildrone/starred{/owner}{/repo},https://api.github.com/users/taosaildrone/subscriptions,https://api.github.com/users/taosaildrone/orgs,https://api.github.com/users/taosaildrone/repos,https://api.github.com/users/taosaildrone/events{/privacy},https://api.github.com/users/taosaildrone/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23762,https://github.com/apache/spark/pull/23762,https://github.com/apache/spark/pull/23762.diff,https://github.com/apache/spark/pull/23762.patch
325,https://api.github.com/repos/apache/spark/issues/23758,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23758/labels{/name},https://api.github.com/repos/apache/spark/issues/23758/comments,https://api.github.com/repos/apache/spark/issues/23758/events,https://github.com/apache/spark/pull/23758,408544996,MDExOlB1bGxSZXF1ZXN0MjUxNzQ0ODU0,23758,[SPARK-17454][MESOS] Use Mesos disk resources for executors.,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2019-02-10T15:18:04Z,2019-09-16T18:15:55Z,,NONE,"## What changes were proposed in this pull request?

Before this change, there was no way to allocate a given amount of
disk when using Mesos scheduler. It's good enough when using default isolation
options but not when enabling the XFS isolator with hard limit in order to
properly isolate all containers. In that case, the executor is killed by Mesos
during the download of the Spark executor archive.

Therefore, this change introduces a configuration flag, specific to Mesos, to
declare the amount of disk required by the executors and therefore prevent
Mesos from killing the container because the XFS hard limit has been exceeded.

## How was this patch tested?

I added 3 unit tests and tested my built version of Spark against a real Mesos cluster.
",spark,apache,clems4ever,3193257,MDQ6VXNlcjMxOTMyNTc=,https://avatars2.githubusercontent.com/u/3193257?v=4,,https://api.github.com/users/clems4ever,https://github.com/clems4ever,https://api.github.com/users/clems4ever/followers,https://api.github.com/users/clems4ever/following{/other_user},https://api.github.com/users/clems4ever/gists{/gist_id},https://api.github.com/users/clems4ever/starred{/owner}{/repo},https://api.github.com/users/clems4ever/subscriptions,https://api.github.com/users/clems4ever/orgs,https://api.github.com/users/clems4ever/repos,https://api.github.com/users/clems4ever/events{/privacy},https://api.github.com/users/clems4ever/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23758,https://github.com/apache/spark/pull/23758,https://github.com/apache/spark/pull/23758.diff,https://github.com/apache/spark/pull/23758.patch
326,https://api.github.com/repos/apache/spark/issues/23749,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23749/labels{/name},https://api.github.com/repos/apache/spark/issues/23749/comments,https://api.github.com/repos/apache/spark/issues/23749/events,https://github.com/apache/spark/pull/23749,408196568,MDExOlB1bGxSZXF1ZXN0MjUxNDk5MzY3,23749,[SPARK-26841][SQL] Kafka timestamp pushdown,"[{'id': 1406605415, 'node_id': 'MDU6TGFiZWwxNDA2NjA1NDE1', 'url': 'https://api.github.com/repos/apache/spark/labels/INPUT/OUTPUT', 'name': 'INPUT/OUTPUT', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2019-02-08T15:19:02Z,2019-10-01T21:52:05Z,,NONE,"## What changes were proposed in this pull request?

This patch introduces timestamp pushdown to Kafka relation to significantly speed up queries filtered by timestamp.
Before this PR, queries filtered by timestamp were performed as full-topic scans and filtering was handled in spark memory. For production topics with few days of history such query couldn't complete in a reasonable time and clients had to rely on manual offset computation. 
Moreover offset filters have to be specified during dataframe initialization (or table creation when using thrift) as Datasource's startingOffsets/endingOffsets option, which makes offset changes inflexible.
Timestamp pushdown is a flexible solution which reuses existing datasources/tables and makes it possible to create dynamic queries or even live views.
As an example following SQL commands will create a **live view** displaying last 10 minutes of data stored in Kafka:

    CREATE TABLE kafka_source USING kafka OPTIONS (kafka.bootstrap.servers '$urls', subscribe '$topic');
    CREATE or replace VIEW kafka_5_min as 
        select value, timestamp from kafka_source 
        where timestamp > cast(from_unixtime(unix_timestamp() - 10 * 60, ""YYYY-MM-dd HH:mm:ss"") as TIMESTAMP);`

PR deals with SQL queries it doesn't handle streaming queries.

## Technical notes
Technically `KafkaRelation`'s parent was changed from `TableScan` to `PrunedFilteredScan` allowing to use filter conditions and required columns projection. 
Operating only on required columns should result in lower memory pressure and slightly better performance.
Filtering leverages `KafkaConsumer.offsetsForTimes()` method to compute offset ranges for filtered timestamps. Resulting offset ranges are merged with existing range filters specified as Datasource options.

There is another PR #23747 related to timestamp filter on Kafka table which handles timestamps as dataframe/table options during dataframe creation.
In practice 2 PRs supplement each other and they should be able to work together.

## Restrictions
There is one technical restriction when using equals operator. Equals will not find element if it has the latest timestamp in its partition.
Example with timestamp in milliseconds (ts):
```
Topic with partitions:
partition 0 : [ (k1, v1, ts: 1001), (k4, v4, ts: 1002), (k7, v7, ts: 1004) ]
partition 1 : [ (k2, v2, ts: 1001), (k5, v5, ts: 1002), (k8, v8, ts: 1004) ]
partition 2 : [ (k3, v3, ts: 1001), (k6, v6, ts: 1002), (k9, v9, ts: 1004) ]

-- OK, result [(k4, v4, ts: 1002), (k5, v5, ts: 1002), (k6, v6, ts: 1002))
select * from kafka_table where timestamp = 1002
-- Not OK, result is empty
select * from kafka_table where timestamp = 1004
-- OK, result [(k1, v1, ts: 1001), (k2, v2, ts: 1001), (k3, v3, ts: 1001))
select * from kafka_table where timestamp = 1001
```
This situation is explained in unit test `timestamp pushdown on unevenly distributed partitions`. 
In real world scenario with live traffic it is extremely unlikely to run into this scenario, however its still worth documenting.
**Queries with OR condition (`timestamp > a or timestamp < b`) will not use pushdown, however they will return correct results.**

## How was this patch tested?
* Manual tests with queries to thrift JDBC/ODBC server connected to Kafka 
* Unit tests in KafkaRelationSuite class",spark,apache,tomasbartalos,12713554,MDQ6VXNlcjEyNzEzNTU0,https://avatars3.githubusercontent.com/u/12713554?v=4,,https://api.github.com/users/tomasbartalos,https://github.com/tomasbartalos,https://api.github.com/users/tomasbartalos/followers,https://api.github.com/users/tomasbartalos/following{/other_user},https://api.github.com/users/tomasbartalos/gists{/gist_id},https://api.github.com/users/tomasbartalos/starred{/owner}{/repo},https://api.github.com/users/tomasbartalos/subscriptions,https://api.github.com/users/tomasbartalos/orgs,https://api.github.com/users/tomasbartalos/repos,https://api.github.com/users/tomasbartalos/events{/privacy},https://api.github.com/users/tomasbartalos/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23749,https://github.com/apache/spark/pull/23749,https://github.com/apache/spark/pull/23749.diff,https://github.com/apache/spark/pull/23749.patch
327,https://api.github.com/repos/apache/spark/issues/23745,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23745/labels{/name},https://api.github.com/repos/apache/spark/issues/23745/comments,https://api.github.com/repos/apache/spark/issues/23745/events,https://github.com/apache/spark/pull/23745,407836765,MDExOlB1bGxSZXF1ZXN0MjUxMjIwMzU4,23745,[SPARK-26707][SQL] Insertion of a single struct into a table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2019-02-07T18:29:42Z,2019-09-16T18:16:01Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
A commend like ```INSERT INTO tbl VALUES (struct(123))``` currently leads to an ```AnalysisException```.  The PR tries to fix the problem by introducing a new unresolved expression that plays a role of a collection for multiple named expressions.

## How was this patch tested?
- Added UT describing  the problem
- Run all existing SQL test ",spark,apache,mn-mikke,3674621,MDQ6VXNlcjM2NzQ2MjE=,https://avatars3.githubusercontent.com/u/3674621?v=4,,https://api.github.com/users/mn-mikke,https://github.com/mn-mikke,https://api.github.com/users/mn-mikke/followers,https://api.github.com/users/mn-mikke/following{/other_user},https://api.github.com/users/mn-mikke/gists{/gist_id},https://api.github.com/users/mn-mikke/starred{/owner}{/repo},https://api.github.com/users/mn-mikke/subscriptions,https://api.github.com/users/mn-mikke/orgs,https://api.github.com/users/mn-mikke/repos,https://api.github.com/users/mn-mikke/events{/privacy},https://api.github.com/users/mn-mikke/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23745,https://github.com/apache/spark/pull/23745,https://github.com/apache/spark/pull/23745.diff,https://github.com/apache/spark/pull/23745.patch
328,https://api.github.com/repos/apache/spark/issues/23735,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23735/labels{/name},https://api.github.com/repos/apache/spark/issues/23735/comments,https://api.github.com/repos/apache/spark/issues/23735/events,https://github.com/apache/spark/pull/23735,406499748,MDExOlB1bGxSZXF1ZXN0MjUwMTgzNTQx,23735,[SPARK-26801][SQL] Read avro types other than record,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2019-02-04T20:13:28Z,2019-09-30T13:40:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently spark reads avro files of type records only as this is internally mapped to the SQL StructType. Any other avro type is currently not supported reading at the top level. Primitive and Non-record types are supported within a record but not on their own. This change fixes the issue by wrapping the incompatible types under a record so these can be successfully read. Note, the issue is while reading only, as during writing, spark writes these types wrapped in a record itself.

## How was this patch tested?
Manually loaded avro primitive types and added unit tests. ",spark,apache,dhruve,7732317,MDQ6VXNlcjc3MzIzMTc=,https://avatars1.githubusercontent.com/u/7732317?v=4,,https://api.github.com/users/dhruve,https://github.com/dhruve,https://api.github.com/users/dhruve/followers,https://api.github.com/users/dhruve/following{/other_user},https://api.github.com/users/dhruve/gists{/gist_id},https://api.github.com/users/dhruve/starred{/owner}{/repo},https://api.github.com/users/dhruve/subscriptions,https://api.github.com/users/dhruve/orgs,https://api.github.com/users/dhruve/repos,https://api.github.com/users/dhruve/events{/privacy},https://api.github.com/users/dhruve/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23735,https://github.com/apache/spark/pull/23735,https://github.com/apache/spark/pull/23735.diff,https://github.com/apache/spark/pull/23735.patch
329,https://api.github.com/repos/apache/spark/issues/23721,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23721/labels{/name},https://api.github.com/repos/apache/spark/issues/23721/comments,https://api.github.com/repos/apache/spark/issues/23721/events,https://github.com/apache/spark/pull/23721,405628662,MDExOlB1bGxSZXF1ZXN0MjQ5NTQ1MjQw,23721,[SPARK-26797][SQL][WIP][test-maven] Start using the new logical types API of Parquet 1.11.0 instead of the deprecated one,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,56,2019-02-01T09:55:34Z,2019-11-13T15:02:14Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

A new, more flexible logical type API was introduced in parquet-mr 1.11.0 (based on the the Thrift field in parquet-format available for a while). This change migrates from the old (now deprecated) enum-based OriginalType API to this new logical type API.

In addition to replacing the deprecated API calls, this PR also introduces support for reading the new subtypes for different timestamp semantics.

Since parquet-mr 1.11.0 is not yet released, this is tested against a release candidate. Before merging, the additional repository should be deleted from pom.xml, which can only be done once parquet-mr 1.11.0 is released.

## How was this patch tested?

Unit tests were added to the PR.",spark,apache,nandorKollar,12639187,MDQ6VXNlcjEyNjM5MTg3,https://avatars2.githubusercontent.com/u/12639187?v=4,,https://api.github.com/users/nandorKollar,https://github.com/nandorKollar,https://api.github.com/users/nandorKollar/followers,https://api.github.com/users/nandorKollar/following{/other_user},https://api.github.com/users/nandorKollar/gists{/gist_id},https://api.github.com/users/nandorKollar/starred{/owner}{/repo},https://api.github.com/users/nandorKollar/subscriptions,https://api.github.com/users/nandorKollar/orgs,https://api.github.com/users/nandorKollar/repos,https://api.github.com/users/nandorKollar/events{/privacy},https://api.github.com/users/nandorKollar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23721,https://github.com/apache/spark/pull/23721,https://github.com/apache/spark/pull/23721.diff,https://github.com/apache/spark/pull/23721.patch
330,https://api.github.com/repos/apache/spark/issues/23701,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23701/labels{/name},https://api.github.com/repos/apache/spark/issues/23701/comments,https://api.github.com/repos/apache/spark/issues/23701/events,https://github.com/apache/spark/pull/23701,404868726,MDExOlB1bGxSZXF1ZXN0MjQ4OTUzODcx,23701,[SPARK-26741][SQL] Allow using aggregate expressions in ORDER BY clause,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2019-01-30T17:00:09Z,2019-09-16T19:05:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When an `AggergateExpression` is used in a `Sort` operator and the sort operator is not immediately after an `Aggregate` plan, the expression is left as is in the sort operator by the Analyzer. This generates a bad plan which fails at execution.

The PR rewrites `AggergateExpression` in `Sort`, also when the `Aggregate` is not a direct child.

## How was this patch tested?

added UT and re-enabled UT
",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23701,https://github.com/apache/spark/pull/23701,https://github.com/apache/spark/pull/23701.diff,https://github.com/apache/spark/pull/23701.patch
331,https://api.github.com/repos/apache/spark/issues/23695,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23695/labels{/name},https://api.github.com/repos/apache/spark/issues/23695/comments,https://api.github.com/repos/apache/spark/issues/23695/events,https://github.com/apache/spark/pull/23695,404640380,MDExOlB1bGxSZXF1ZXN0MjQ4Nzc0MjA3,23695,[SPARK-26780][CORE]Improve shuffle read using ReadAheadInputStream ,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2019-01-30T07:42:09Z,2019-09-16T19:05:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Using `ReadAheadInputStream`  to improve  shuffle read  performance.
 `ReadAheadInputStream` can save cpu utilization and almost no performance regression


## How was this patch tested?
1. Using unit tests for correctness testing.
1. TPCDS test: 
./spark-shell --executor-memory 4G --total-executor-cores 18 --conf ""spark.sql.shuffle.partitions=6"" --conf ""spark.shuffle.detectCorrupt=false"" --conf ""spark.shuffle.read.ahead.enabled=true"" --jars /home/spark-sql-perf-assembly-0.5.0-SNAPSHOT.jar

Test cases ran three times.

**Before this PRÔºö**
**One(Total time :1759.577170773999969s)**Ôºö
Running execution q4-v2.50 iteration: 1, StandardRun=true
Execution time: 402.58029997799997s
Running execution q14a-v2.50 iteration: 1, StandardRun=true
Execution time: 652.151175984s
Running execution q14b-v2.50 iteration: 1, StandardRun=true
Execution time: 547.148668606s
Running execution q25-v2.50 iteration: 1, StandardRun=true
Execution time: 150.53358054s
Running execution ss_max-v2.50 iteration: 1, StandardRun=true
Execution time: 7.163445665999999s

**Two(Total time :1734.5941864229999s)**Ôºö
Running execution q4-v2.50 iteration: 1, StandardRun=true
Execution time: 391.070943194s
Running execution q14a-v2.50 iteration: 1, StandardRun=true
Execution time: 654.8007626389999s
Running execution q14b-v2.50 iteration: 1, StandardRun=true
Execution time: 537.754249246s
Running execution q25-v2.50 iteration: 1, StandardRun=true
Execution time: 143.928162026s
Running execution ss_max-v2.50 iteration: 1, StandardRun=true
Execution time: 7.040069318s

**Three(Total time :1749.7357909459999995s)**Ôºö
Running execution q4-v2.50 iteration: 1, StandardRun=true
Execution time: 395.008734361s
Running execution q14a-v2.50 iteration: 1, StandardRun=true
Execution time: 651.273268132s
Running execution q14b-v2.50 iteration: 1, StandardRun=true
Execution time: 544.524801175s
Running execution q25-v2.50 iteration: 1, StandardRun=true
Execution time: 151.95291255s
Running execution ss_max-v2.50 iteration: 1, StandardRun=true
Execution time: 6.9760747279999995s

Average time of three timesÔºö(1759.577170773999969 +1734.5941864229999 +1749.7357909459999995 ) / 3 = **1747.969s**

**After this PRÔºö**
**One(Total time :1751.97129727800003s)**Ôºö
Running execution q4-v2.50 iteration: 1, StandardRun=true
Execution time: 395.34360907900003s
Running execution q14a-v2.50 iteration: 1, StandardRun=true
Execution time: 658.750893903s
Running execution q14b-v2.50 iteration: 1, StandardRun=true
Execution time: 539.719277862s
Running execution q25-v2.50 iteration: 1, StandardRun=true
Execution time: 150.859470327s
Running execution ss_max-v2.50 iteration: 1, StandardRun=true
Execution time: 7.298046107s

**Two(Total time :1706.564606477s)**Ôºö
Running execution q4-v2.50 iteration: 1, StandardRun=true
Execution time: 382.847373545s
Running execution q14a-v2.50 iteration: 1, StandardRun=true
Execution time: 649.87997278s
Running execution q14b-v2.50 iteration: 1, StandardRun=true
Execution time: 522.374645028s
Running execution q25-v2.50 iteration: 1, StandardRun=true
Execution time: 143.963793466s
Running execution ss_max-v2.50 iteration: 1, StandardRun=true
Execution time: 7.498821658s

**Three(Total time :1732.8683500150001s)**Ôºö
Running execution q4-v2.50 iteration: 1, StandardRun=true
Execution time: 393.625937426s
Running execution q14a-v2.50 iteration: 1, StandardRun=true
Execution time: 644.5871864950001s
Running execution q14b-v2.50 iteration: 1, StandardRun=true
Execution time: 542.764517975s
Running execution q25-v2.50 iteration: 1, StandardRun=true
Execution time: 144.518587871s
Running execution ss_max-v2.50 iteration: 1, StandardRun=true
Execution time: 7.372120248s

Average time of three timesÔºö(1751.97129727800003 +1706.564606477 +1732.8683500150001 ) / 3 = **1730.468s**",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23695,https://github.com/apache/spark/pull/23695,https://github.com/apache/spark/pull/23695.diff,https://github.com/apache/spark/pull/23695.patch
332,https://api.github.com/repos/apache/spark/issues/23679,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23679/labels{/name},https://api.github.com/repos/apache/spark/issues/23679/comments,https://api.github.com/repos/apache/spark/issues/23679/events,https://github.com/apache/spark/pull/23679,404090477,MDExOlB1bGxSZXF1ZXN0MjQ4MzUxNDIx,23679,[SPARK-23516][CORE] Avoid repeated release/acquire on storage memory,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2019-01-29T02:10:30Z,2019-09-16T19:05:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Now `StaticMemoryManager` has been removed.
And for `UnifiedMemoryManager`,  unroll memory is also storage memory, so I think it is unnecessary to release unroll memory really,  and then to get storage memory again.

## How was this patch tested?
Existing unit tests
",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23679,https://github.com/apache/spark/pull/23679,https://github.com/apache/spark/pull/23679.diff,https://github.com/apache/spark/pull/23679.patch
333,https://api.github.com/repos/apache/spark/issues/23584,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23584/labels{/name},https://api.github.com/repos/apache/spark/issues/23584/comments,https://api.github.com/repos/apache/spark/issues/23584/events,https://github.com/apache/spark/pull/23584,400638311,MDExOlB1bGxSZXF1ZXN0MjQ1Nzc3ODMz,23584,[SPARK-21213][SQL][FOLLOWUP] Improve partition statistics in AnalyzePartitionCommand,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2019-01-18T09:22:17Z,2019-09-16T18:17:10Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This pr proposes to improve partition statistics in `AnalyzePartitionCommand`:

1.  Restore Spark partition statistics in `HiveExternalCatalog.listPartitions` and `HiveExternalCatalog.listPartitionsByFilter`. 
2.  Compare with old partition stats instead of old table stats in `AnalyzePartitionCommand`.

Thus partitions listed in `AnalyzePartitionCommand` would contain Spark stats and would not update HiveMetaStore if stats not changed.
https://github.com/apache/spark/blob/6d9c54b62cee6fdf396f507caf7eb7f2e3f35b0a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala#L87
## How was this patch tested?
Modified existing tests.",spark,apache,wangshuo128,4003322,MDQ6VXNlcjQwMDMzMjI=,https://avatars0.githubusercontent.com/u/4003322?v=4,,https://api.github.com/users/wangshuo128,https://github.com/wangshuo128,https://api.github.com/users/wangshuo128/followers,https://api.github.com/users/wangshuo128/following{/other_user},https://api.github.com/users/wangshuo128/gists{/gist_id},https://api.github.com/users/wangshuo128/starred{/owner}{/repo},https://api.github.com/users/wangshuo128/subscriptions,https://api.github.com/users/wangshuo128/orgs,https://api.github.com/users/wangshuo128/repos,https://api.github.com/users/wangshuo128/events{/privacy},https://api.github.com/users/wangshuo128/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23584,https://github.com/apache/spark/pull/23584,https://github.com/apache/spark/pull/23584.diff,https://github.com/apache/spark/pull/23584.patch
334,https://api.github.com/repos/apache/spark/issues/23576,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23576/labels{/name},https://api.github.com/repos/apache/spark/issues/23576/comments,https://api.github.com/repos/apache/spark/issues/23576/events,https://github.com/apache/spark/pull/23576,400468557,MDExOlB1bGxSZXF1ZXN0MjQ1NjQ5NzE3,23576,[SPARK-26655] [SS] Support multiple aggregates in append mode,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,39,2019-01-17T21:22:08Z,2019-09-17T13:30:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This patch proposes to add support for multiple aggregates in append mode. In
append mode, the aggregates are emitted only after the watermark passes
the threshold (e.g. the window boundary) and the emitted value is not
affected by further late data. This allows to chain multiple aggregates
in 'Append' output mode without worrying about retractions etc.

However the current event time watermarks in structured streaming are
tracked at a global level and this does not work when aggregates are
chained. The downstream watermarks usually lags the ones before and the
global (min or max) watermarks will not let the stages make progress
independently.

The patch tracks the watermarks at each (stateful)
operator so that the aggregate outputs are generated when the watermark
passes the thresholds at the corresponding stateful operator. The values
are also saved into the commit/offset logs (similar to global watermark)

Each aggregate should have a corresponding watermark defined while
creating the query (E.g. via withWatermark) and this is used to
track the progress of event time corresponding to the stateful operator.


## How was this patch tested?

New and existing unit tests

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,arunmahadevan,6792890,MDQ6VXNlcjY3OTI4OTA=,https://avatars0.githubusercontent.com/u/6792890?v=4,,https://api.github.com/users/arunmahadevan,https://github.com/arunmahadevan,https://api.github.com/users/arunmahadevan/followers,https://api.github.com/users/arunmahadevan/following{/other_user},https://api.github.com/users/arunmahadevan/gists{/gist_id},https://api.github.com/users/arunmahadevan/starred{/owner}{/repo},https://api.github.com/users/arunmahadevan/subscriptions,https://api.github.com/users/arunmahadevan/orgs,https://api.github.com/users/arunmahadevan/repos,https://api.github.com/users/arunmahadevan/events{/privacy},https://api.github.com/users/arunmahadevan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23576,https://github.com/apache/spark/pull/23576,https://github.com/apache/spark/pull/23576.diff,https://github.com/apache/spark/pull/23576.patch
335,https://api.github.com/repos/apache/spark/issues/23531,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23531/labels{/name},https://api.github.com/repos/apache/spark/issues/23531/comments,https://api.github.com/repos/apache/spark/issues/23531/events,https://github.com/apache/spark/pull/23531,398650363,MDExOlB1bGxSZXF1ZXN0MjQ0Mjc0NTY1,23531,[SPARK-24497][SQL] Support recursive SQL query,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,68,2019-01-13T12:00:07Z,2019-12-05T17:07:58Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR adds recursion to Spark SQL.

A recursive query is defined using the `WITH RECURSIVE` keywords and referring the name of the common table expression within the query.
The implementation complies with SQL standard and follows similar rules to other relational databases:
- A query is made of an anchor followed by a recursive term.
- The anchor terms doesn't contain self reference and it is used to initialize the query.
- The recursive term contains a self reference and it is used to expand the current set of rows with new ones.
- The anchor and recursive terms must be joined with each other by `UNION` or `UNION ALL` operators.
- New rows can only be derived from the newly added rows of the previous iteration (or from the initial set of rows of anchor terms). This limitation implies that recursive references can't be used with some of the joins, aggregations or subqueries.

Please see `cte-recursive.sql` and `with.sql` for some examples.

Please note that this PR focuses on the minimal working implementation which means:
- SQL recursion is actually loop where the current iteration is computed based on the previous one's result and when an iteration returns no rows the loop is over. The final result is the union of all iteration results. This means that caching intermediate results could speed up the process, but caching was removed from this PR to reduce complexity and can be added back in a follow-up PR.
- A common way to stop SQL recursion is using the LIMIT operator to stop computing more than the required number of rows. LIMIT support was removed from this PR to reduce complexity and can be added back in a follow-up PR.
- Some relational databases are more relaxed in terms how many anchor and recursive terms can be in a recursion. This PR allows the most simple case and allows only 1-1 of them. A follow-up PR can target to relax this limitation.

## How was this patch tested?

Added new UTs and tests in `cte-recursion.sql` and `with.sql`. 
",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23531,https://github.com/apache/spark/pull/23531,https://github.com/apache/spark/pull/23531.diff,https://github.com/apache/spark/pull/23531.patch
336,https://api.github.com/repos/apache/spark/issues/23528,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23528/labels{/name},https://api.github.com/repos/apache/spark/issues/23528/comments,https://api.github.com/repos/apache/spark/issues/23528/events,https://github.com/apache/spark/pull/23528,398562234,MDExOlB1bGxSZXF1ZXN0MjQ0MjI0NjU1,23528,[SPARK-26225][SQL] Track decoding time for row-based data sources,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2019-01-12T15:13:26Z,2019-09-16T19:06:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Add record decoding time for RowDataSourceScanExec.
![image](https://user-images.githubusercontent.com/4833765/51074881-7a6c6700-16bf-11e9-9036-ae512fd702bb.png)

## How was this patch tested?
New UT in TableScanSuite.",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23528,https://github.com/apache/spark/pull/23528,https://github.com/apache/spark/pull/23528.diff,https://github.com/apache/spark/pull/23528.patch
337,https://api.github.com/repos/apache/spark/issues/23490,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23490/labels{/name},https://api.github.com/repos/apache/spark/issues/23490/comments,https://api.github.com/repos/apache/spark/issues/23490/events,https://github.com/apache/spark/pull/23490,396758088,MDExOlB1bGxSZXF1ZXN0MjQyODYyNjY5,23490,[SPARK-26543][SQL] Support the coordinator to determine post-shuffle partitions more reasonably,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2019-01-08T05:40:38Z,2019-09-17T12:30:37Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

For SparkSQL ,when we open AE by 'set spark.sql.adapative.enable=true'Ôºåthe ExchangeCoordinator will introduced to determine the number of post-shuffle partitions. But in some certain conditions,the coordinator performed not very well, there are always some tasks retained and they worked with Shuffle Read Size / Records 0.0B/0 ,We could increase the spark.sql.adaptive.shuffle.targetPostShuffleInputSize to solve this,but this action is unreasonable as targetPostShuffleInputSize Should not be set too large. As follow:
![image](https://user-images.githubusercontent.com/20614350/50747129-5519cc00-126d-11e9-8511-8cdb324366c9.png)

We could   reproduce this problem easily with the SQL:

`set spark.sql.adaptive.enabled=trueÔºõ`
`  spark.sql.shuffle.partitions 100Ôºõ`
`  spark.sql.adaptive.shuffle.targetPostShuffleInputSize¬† 33554432 Ôºõ`
`  SELECT a,COUNT(1) FROM TABLE  GROUP BY  a DISTRIBUTE BY cast(rand()* 10 as bigint)` 

before fixÔºö
![image](https://user-images.githubusercontent.com/20614350/50747540-577d2580-126f-11e9-80b0-1b36fc2fd692.png)
after fixÔºö
![image](https://user-images.githubusercontent.com/20614350/50747608-c65a7e80-126f-11e9-9b10-32494232f0f9.png)


## How was this patch tested?
manual and  unit tests 
",spark,apache,southernriver,20614350,MDQ6VXNlcjIwNjE0MzUw,https://avatars3.githubusercontent.com/u/20614350?v=4,,https://api.github.com/users/southernriver,https://github.com/southernriver,https://api.github.com/users/southernriver/followers,https://api.github.com/users/southernriver/following{/other_user},https://api.github.com/users/southernriver/gists{/gist_id},https://api.github.com/users/southernriver/starred{/owner}{/repo},https://api.github.com/users/southernriver/subscriptions,https://api.github.com/users/southernriver/orgs,https://api.github.com/users/southernriver/repos,https://api.github.com/users/southernriver/events{/privacy},https://api.github.com/users/southernriver/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23490,https://github.com/apache/spark/pull/23490,https://github.com/apache/spark/pull/23490.diff,https://github.com/apache/spark/pull/23490.patch
338,https://api.github.com/repos/apache/spark/issues/23381,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23381/labels{/name},https://api.github.com/repos/apache/spark/issues/23381/comments,https://api.github.com/repos/apache/spark/issues/23381/events,https://github.com/apache/spark/pull/23381,394071221,MDExOlB1bGxSZXF1ZXN0MjQwOTA3MTM0,23381,[SPARK-26434][SQL][Struct streaming]move adaptive_execution&CBO disabled warning message to StreamExecuti‚Ä¶,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-12-26T03:09:38Z,2019-09-16T18:17:09Z,,CONTRIBUTOR,"‚Ä¶on.scala

## What changes were proposed in this pull request?
There are warning messages in org.apache.spark.sql.streaming.StreamingQueryManager#createQuery for ""adaptiveExecutionEnabled"",
but actually disallow it in org.apache.spark.sql.execution.streaming.StreamExecution#runStream.
it is better to move adaptive_execution&CBO disabled warning messages to the disallowing code.
(Please fill in changes proposed in this fix)

## How was this patch tested?
unit tests
(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,iamhumanbeing,8821698,MDQ6VXNlcjg4MjE2OTg=,https://avatars1.githubusercontent.com/u/8821698?v=4,,https://api.github.com/users/iamhumanbeing,https://github.com/iamhumanbeing,https://api.github.com/users/iamhumanbeing/followers,https://api.github.com/users/iamhumanbeing/following{/other_user},https://api.github.com/users/iamhumanbeing/gists{/gist_id},https://api.github.com/users/iamhumanbeing/starred{/owner}{/repo},https://api.github.com/users/iamhumanbeing/subscriptions,https://api.github.com/users/iamhumanbeing/orgs,https://api.github.com/users/iamhumanbeing/repos,https://api.github.com/users/iamhumanbeing/events{/privacy},https://api.github.com/users/iamhumanbeing/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23381,https://github.com/apache/spark/pull/23381,https://github.com/apache/spark/pull/23381.diff,https://github.com/apache/spark/pull/23381.patch
339,https://api.github.com/repos/apache/spark/issues/23371,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23371/labels{/name},https://api.github.com/repos/apache/spark/issues/23371/comments,https://api.github.com/repos/apache/spark/issues/23371/events,https://github.com/apache/spark/pull/23371,393639943,MDExOlB1bGxSZXF1ZXN0MjQwNjE0MjE2,23371,[SPARK-26223][SQL] Track metastore operation time in scan node,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-12-22T03:48:57Z,2019-11-12T20:51:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

As the comment in [SPARK-26223](https://issues.apache.org/jira/browse/SPARK-26223?focusedCommentId=16723111&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16723111), track all metastore operation time in scan node and show the duration in metrics as well as the timeline when hover. The mainly changes including:
- Add `metastoreOpsPhaseSummaries` in CatalogTable for recording all timeline between the table and the metastore.
- Create phase summaries in all scenarios described in Jira and memorize them into `metastoreOpsPhaseSummaries`.
- Display support and driver metrics updating for metastore operations.

## How was this patch tested?
Add UT in SQLMetricsSuite.
Manually test and check UI:

|             Scenario            | UI |
|:-------------------------------:|----|
| FileSourceScanExec with partition pruning |![image](https://user-images.githubusercontent.com/4833765/50370288-04547300-05df-11e9-9bab-549c89d180fa.png)|
| HiveTableScanExec |![image](https://user-images.githubusercontent.com/4833765/50370268-ae7fcb00-05de-11e9-9f4c-eeb8e3e8f2d1.png)|
| FileSourceScanExec with RelationConversion for hive table |![image](https://user-images.githubusercontent.com/4833765/50370310-5e553880-05df-11e9-98af-766248504fc8.png)|
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23371,https://github.com/apache/spark/pull/23371,https://github.com/apache/spark/pull/23371.diff,https://github.com/apache/spark/pull/23371.patch
340,https://api.github.com/repos/apache/spark/issues/23340,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23340/labels{/name},https://api.github.com/repos/apache/spark/issues/23340/comments,https://api.github.com/repos/apache/spark/issues/23340/events,https://github.com/apache/spark/pull/23340,391974853,MDExOlB1bGxSZXF1ZXN0MjM5MzQ4NzI4,23340,[SPARK-23431][CORE] Expose the new executor memory metrics at the stage level,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2018-12-18T02:51:06Z,2019-09-16T18:17:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Collect and show the new executor memory metrics for each stage, to provide more information on how memory is used per stage. Peak values for metrics are show for each stage. For executor summaries for each stage, the peak values per executor are also shown.

## How was this patch tested?

Added new unit tests.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,edwinalu,26153035,MDQ6VXNlcjI2MTUzMDM1,https://avatars0.githubusercontent.com/u/26153035?v=4,,https://api.github.com/users/edwinalu,https://github.com/edwinalu,https://api.github.com/users/edwinalu/followers,https://api.github.com/users/edwinalu/following{/other_user},https://api.github.com/users/edwinalu/gists{/gist_id},https://api.github.com/users/edwinalu/starred{/owner}{/repo},https://api.github.com/users/edwinalu/subscriptions,https://api.github.com/users/edwinalu/orgs,https://api.github.com/users/edwinalu/repos,https://api.github.com/users/edwinalu/events{/privacy},https://api.github.com/users/edwinalu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23340,https://github.com/apache/spark/pull/23340,https://github.com/apache/spark/pull/23340.diff,https://github.com/apache/spark/pull/23340.patch
341,https://api.github.com/repos/apache/spark/issues/23327,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23327/labels{/name},https://api.github.com/repos/apache/spark/issues/23327/comments,https://api.github.com/repos/apache/spark/issues/23327/events,https://github.com/apache/spark/pull/23327,391500242,MDExOlB1bGxSZXF1ZXN0MjM4OTg3MTY1,23327,[SPARK-26222][SQL] Track file listing time,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2018-12-16T20:07:16Z,2019-10-16T13:34:12Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Tracking file listing time and add them into scan node's metrics, also add the start and end timestamp into the metadata message of the plan.
![image](https://user-images.githubusercontent.com/4833765/50227532-e6b9ba80-03e0-11e9-9f65-921275da0493.png)


## How was this patch tested?

Add new tests in SQLMetricsSuite.
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23327,https://github.com/apache/spark/pull/23327,https://github.com/apache/spark/pull/23327.diff,https://github.com/apache/spark/pull/23327.patch
342,https://api.github.com/repos/apache/spark/issues/23306,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23306/labels{/name},https://api.github.com/repos/apache/spark/issues/23306/comments,https://api.github.com/repos/apache/spark/issues/23306/events,https://github.com/apache/spark/pull/23306,390533235,MDExOlB1bGxSZXF1ZXN0MjM4Mjc2MjI5,23306,[SPARK-26357][Core] Expose executors' procfs metrics to Metrics system,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2018-12-13T06:15:40Z,2019-09-16T19:08:07Z,,CONTRIBUTOR,"This change exposes the executors' proscfs metrics that were introduced in SPARK-24958 to Metrics system. To do this a new metric source is defined and a new config is also added. Using the configs user can choose whether they want to see procfs metrics through Metrics system. To avoid overhead a cache is added in ProsfsMtericsGetter to avoid computing metrics if they have beean computed in the past one second.

This was tested manually and I verified that procfs metrics are reporting in Metrics system using Console sink:
application_1544653637885_0020.driver.procfs.processTree.JVMRSSMemory
             value = 696242176
application_1544653637885_0020.driver.procfs.processTree.JVMVMemory
             value = 4959170560
application_1544653637885_0020.driver.procfs.processTree.OtherRSSMemory
             value = 0
application_1544653637885_0020.driver.procfs.processTree.OtherVMemory
             value = 0
application_1544653637885_0020.driver.procfs.processTree.PythonRSSMemory
             value = 33714176
application_1544653637885_0020.driver.procfs.processTree.PythonVMemory
             value = 401711104

And they got updated as well:
application_1544653637885_0020.driver.procfs.processTree.JVMRSSMemory
             value = 732999680
application_1544653637885_0020.driver.procfs.processTree.JVMVMemory
             value = 4977057792
application_1544653637885_0020.driver.procfs.processTree.OtherRSSMemory
             value = 0
application_1544653637885_0020.driver.procfs.processTree.OtherVMemory
             value = 0
application_1544653637885_0020.driver.procfs.processTree.PythonRSSMemory
             value = 33714176
application_1544653637885_0020.driver.procfs.processTree.PythonVMemory
             value = 401711104",spark,apache,rezasafi,20547172,MDQ6VXNlcjIwNTQ3MTcy,https://avatars2.githubusercontent.com/u/20547172?v=4,,https://api.github.com/users/rezasafi,https://github.com/rezasafi,https://api.github.com/users/rezasafi/followers,https://api.github.com/users/rezasafi/following{/other_user},https://api.github.com/users/rezasafi/gists{/gist_id},https://api.github.com/users/rezasafi/starred{/owner}{/repo},https://api.github.com/users/rezasafi/subscriptions,https://api.github.com/users/rezasafi/orgs,https://api.github.com/users/rezasafi/repos,https://api.github.com/users/rezasafi/events{/privacy},https://api.github.com/users/rezasafi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23306,https://github.com/apache/spark/pull/23306,https://github.com/apache/spark/pull/23306.diff,https://github.com/apache/spark/pull/23306.patch
343,https://api.github.com/repos/apache/spark/issues/23211,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23211/labels{/name},https://api.github.com/repos/apache/spark/issues/23211/comments,https://api.github.com/repos/apache/spark/issues/23211/events,https://github.com/apache/spark/pull/23211,387033510,MDExOlB1bGxSZXF1ZXN0MjM1NjE0MDU2,23211,[SPARK-19712][SQL] Move PullupCorrelatedPredicates and RewritePredicateSubquery after OptimizeSubqueries,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-12-03T22:25:56Z,2019-09-16T19:08:08Z,,CONTRIBUTOR,"Currently predicate subqueries (IN/EXISTS) are converted to Joins at the end of optimizer in RewritePredicateSubquery. This change moves the rewrite close to beginning of optimizer. The original idea was to keep the subquery expressions in Filter form so that we can push them down as deep as possible. One disadvantage is that, after the subqueries are rewritten in join form, they are not subjected to further optimizations. In this change, we convert the subqueries to join form early in the rewrite phase and then add logic to push the left-semi and left-anti joins down like we do for normal filter ops. I can think of the following advantages : 

1. We will produce consistent optimized plans for subqueries written using SQL dialect and data frame apis or queries using left semi/anti joins directly.
2. Will hopefully make it easier to do the next phase of de-correlations when we open up more cases of de-correlation. In this case, it would be beneficial to expose the rewritten queries to all the other optimization rules, i think.
3. We can now hopefully get-rid of PullupCorrelatedPredicates rule and combine this with RewritePredicateSubquery. I haven't tried it. Will take it on a followup.

(P.S Thanks to Natt for his original work in [here](https://github.com/apache/spark/pull/17520). I have based this pr on his work)

## How was this patch tested?
A new suite LeftSemiOrAntiPushDownSuite is added. Existing subquery suite should verify the results and any potential regressions.
",spark,apache,dilipbiswal,14225158,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23211,https://github.com/apache/spark/pull/23211,https://github.com/apache/spark/pull/23211.diff,https://github.com/apache/spark/pull/23211.patch
344,https://api.github.com/repos/apache/spark/issues/23206,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23206/labels{/name},https://api.github.com/repos/apache/spark/issues/23206/comments,https://api.github.com/repos/apache/spark/issues/23206/events,https://github.com/apache/spark/pull/23206,386737667,MDExOlB1bGxSZXF1ZXN0MjM1Mzg0ODQw,23206,[SPARK-26249][SQL] Add ability to inject a rule in order and to add a batch via the Spark Extension Points API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-12-03T09:54:53Z,2019-09-16T18:20:09Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Motivation
Spark has extension points API to allow third parties to extend Spark with custom optimization rules. The current API does not allow fine grain control on when the optimization rule will be exercised. In the current API,  there is no way to add a batch to the optimization using the SparkSessionExtensions API, similar to the postHocOptimizationBatches in SparkOptimizer.
In our use cases, we have optimization rules that we want to add as extensions to a batch in a specific order.

Changes: 
Add two new methods to SparkSessionExtensions to: 
- Inject optimizer rule in a specific order
- Inject optimizer batch. 

Design details is here: 
https://drive.google.com/file/d/1m7rQZ9OZFl0MH5KS12CiIg3upLJSYfsA/view?usp=sharing

## How was this patch tested?
- New unit tests have been added to the SparkSessionExtensionSuite
- Ran the sql, hive, catalyst unit tests without any issues. ",spark,apache,skambha,16563220,MDQ6VXNlcjE2NTYzMjIw,https://avatars3.githubusercontent.com/u/16563220?v=4,,https://api.github.com/users/skambha,https://github.com/skambha,https://api.github.com/users/skambha/followers,https://api.github.com/users/skambha/following{/other_user},https://api.github.com/users/skambha/gists{/gist_id},https://api.github.com/users/skambha/starred{/owner}{/repo},https://api.github.com/users/skambha/subscriptions,https://api.github.com/users/skambha/orgs,https://api.github.com/users/skambha/repos,https://api.github.com/users/skambha/events{/privacy},https://api.github.com/users/skambha/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23206,https://github.com/apache/spark/pull/23206,https://github.com/apache/spark/pull/23206.diff,https://github.com/apache/spark/pull/23206.patch
345,https://api.github.com/repos/apache/spark/issues/23163,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23163/labels{/name},https://api.github.com/repos/apache/spark/issues/23163/comments,https://api.github.com/repos/apache/spark/issues/23163/events,https://github.com/apache/spark/pull/23163,385188815,MDExOlB1bGxSZXF1ZXN0MjM0MjE0MjQ1,23163,[SPARK-26164][SQL] Allow FileFormatWriter to write multiple partitions/buckets without sort,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,24,2018-11-28T10:01:48Z,2019-09-17T00:25:00Z,,NONE,"## What changes were proposed in this pull request?

Currently spark always requires a local sort before writing to output table on partition/bucket columns (see `write.requiredOrdering` in `FileFormatWriter.scala`), which is unnecessary, and can be avoided by keeping multiple output writers concurrently in `FileFormatDataWriter.scala`.

This pr is first doing hash-based write, then falling back to sort-based write (current implementation) when number of opened writer exceeding a threshold (controlled by a config). Specifically:

1. (hash-based write) Maintain mapping between file path and output writer, and re-use writer for writing input row. In case of the number of opened output writers exceeding a threshold (can be changed by a config), we go to 2.

2. (sort-based write) Sort the rest of input rows (use the same sorter in SortExec). Then writing the rest of sorted rows, and we can close the writer on the fly, in case no more rows for current file path.

## How was this patch tested?

Added unit test in `DataFrameReaderWriterSuite.scala`. Existing test like `SQLMetricsSuite.scala` would already exercise the code path of executor write metrics.
",spark,apache,c21,4629931,MDQ6VXNlcjQ2Mjk5MzE=,https://avatars2.githubusercontent.com/u/4629931?v=4,,https://api.github.com/users/c21,https://github.com/c21,https://api.github.com/users/c21/followers,https://api.github.com/users/c21/following{/other_user},https://api.github.com/users/c21/gists{/gist_id},https://api.github.com/users/c21/starred{/owner}{/repo},https://api.github.com/users/c21/subscriptions,https://api.github.com/users/c21/orgs,https://api.github.com/users/c21/repos,https://api.github.com/users/c21/events{/privacy},https://api.github.com/users/c21/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23163,https://github.com/apache/spark/pull/23163,https://github.com/apache/spark/pull/23163.diff,https://github.com/apache/spark/pull/23163.patch
346,https://api.github.com/repos/apache/spark/issues/23146,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23146/labels{/name},https://api.github.com/repos/apache/spark/issues/23146/comments,https://api.github.com/repos/apache/spark/issues/23146/events,https://github.com/apache/spark/pull/23146,384409262,MDExOlB1bGxSZXF1ZXN0MjMzNjE4NzYz,23146,[SPARK-26173] [MLlib] Prior regularization for Logistic Regression,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-11-26T16:27:33Z,2019-09-16T18:20:07Z,,NONE,"Implementation of [SPARK-26173](https://issues.apache.org/jira/browse/SPARK-26173).

This feature enables¬†Maximum A Posteriori (MAP) optimization for Logistic Regression based on a Gaussian prior.¬†In practice, this is just implementing¬†a more general form of L2 regularization parameterized by a (multivariate) mean and precisions vectors.¬†

_Reference:¬†Bishop, Christopher M. (2006).¬†Pattern Recognition and Machine Learning (section 4.5). Berlin, Heidelberg: Springer-Verlag._

### Existing implementations
* Python: [bayes_logistic](https://pypi.org/project/bayes_logistic/)

## Implementation
* 2 new parameters added to `LogisticRegression`: `priorMean` and `priorPrecisions`.
* 1 new class (`PriorRegularization`) implements the calculations of the value and gradient of the prior regularization term.
* Prior regularization is enabled when both vectors are provided and `regParam` > 0 and `elasticNetParam` < 1.

## Tests
* `DifferentiableRegularizationSuite`
  * `Prior regularization`
* `LogisticRegressionSuite`
  * `prior precisions should be required when prior mean is set`
  * `prior mean should be required when prior precisions is set`
  * `regParam should be positive when using prior regularization`
  * `elasticNetParam should be less than 1.0 when using prior regularization`
  * `prior mean and precisions should have equal length`
  * `priors' length should match number of features`
  * `binary logistic regression with prior regularization equivalent to L2`
  * `binary logistic regression with prior regularization equivalent to L2 (bis)`
  * `binary logistic regression with prior regularization`",spark,apache,elfausto,4399814,MDQ6VXNlcjQzOTk4MTQ=,https://avatars0.githubusercontent.com/u/4399814?v=4,,https://api.github.com/users/elfausto,https://github.com/elfausto,https://api.github.com/users/elfausto/followers,https://api.github.com/users/elfausto/following{/other_user},https://api.github.com/users/elfausto/gists{/gist_id},https://api.github.com/users/elfausto/starred{/owner}{/repo},https://api.github.com/users/elfausto/subscriptions,https://api.github.com/users/elfausto/orgs,https://api.github.com/users/elfausto/repos,https://api.github.com/users/elfausto/events{/privacy},https://api.github.com/users/elfausto/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23146,https://github.com/apache/spark/pull/23146,https://github.com/apache/spark/pull/23146.diff,https://github.com/apache/spark/pull/23146.patch
347,https://api.github.com/repos/apache/spark/issues/23140,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23140/labels{/name},https://api.github.com/repos/apache/spark/issues/23140/comments,https://api.github.com/repos/apache/spark/issues/23140/events,https://github.com/apache/spark/pull/23140,384167313,MDExOlB1bGxSZXF1ZXN0MjMzNDMzNjMy,23140,[SPARK-25774][SQL] truncate table with partition and path,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2018-11-26T04:12:10Z,2019-09-16T18:20:04Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
When we run SPARK SQL TRUNCATE TABLE command on a  managed table in Hive, it deletes the files in HDFS but leaves the partitions and partition folder structure,more details refers to SPARK-25774.This pr is to resolve this problem.

## How was this patch tested?
Unit test added in DDLSuite.

",spark,apache,lcqzte10192193,13733151,MDQ6VXNlcjEzNzMzMTUx,https://avatars2.githubusercontent.com/u/13733151?v=4,,https://api.github.com/users/lcqzte10192193,https://github.com/lcqzte10192193,https://api.github.com/users/lcqzte10192193/followers,https://api.github.com/users/lcqzte10192193/following{/other_user},https://api.github.com/users/lcqzte10192193/gists{/gist_id},https://api.github.com/users/lcqzte10192193/starred{/owner}{/repo},https://api.github.com/users/lcqzte10192193/subscriptions,https://api.github.com/users/lcqzte10192193/orgs,https://api.github.com/users/lcqzte10192193/repos,https://api.github.com/users/lcqzte10192193/events{/privacy},https://api.github.com/users/lcqzte10192193/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23140,https://github.com/apache/spark/pull/23140,https://github.com/apache/spark/pull/23140.diff,https://github.com/apache/spark/pull/23140.patch
348,https://api.github.com/repos/apache/spark/issues/23108,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23108/labels{/name},https://api.github.com/repos/apache/spark/issues/23108/comments,https://api.github.com/repos/apache/spark/issues/23108/events,https://github.com/apache/spark/pull/23108,383269979,MDExOlB1bGxSZXF1ZXN0MjMyNzgxMDIy,23108,[Spark-25993][SQL][TEST]Add test cases for CREATE EXTERNAL TABLE with subdirectories,[],open,False,,[],,27,2018-11-21T19:48:51Z,2019-09-16T18:20:02Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Add these test cases for resolution of ORC table location reported by [SPARK-25993](https://issues.apache.org/jira/browse/SPARK-25993)

also add corresponding test cases for Parquet table.

Update the 

> sql-migration-guide-update

 doc
## How was this patch tested?

This is a new test case.",spark,apache,kevinyu98,7550280,MDQ6VXNlcjc1NTAyODA=,https://avatars3.githubusercontent.com/u/7550280?v=4,,https://api.github.com/users/kevinyu98,https://github.com/kevinyu98,https://api.github.com/users/kevinyu98/followers,https://api.github.com/users/kevinyu98/following{/other_user},https://api.github.com/users/kevinyu98/gists{/gist_id},https://api.github.com/users/kevinyu98/starred{/owner}{/repo},https://api.github.com/users/kevinyu98/subscriptions,https://api.github.com/users/kevinyu98/orgs,https://api.github.com/users/kevinyu98/repos,https://api.github.com/users/kevinyu98/events{/privacy},https://api.github.com/users/kevinyu98/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23108,https://github.com/apache/spark/pull/23108,https://github.com/apache/spark/pull/23108.diff,https://github.com/apache/spark/pull/23108.patch
349,https://api.github.com/repos/apache/spark/issues/23104,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23104/labels{/name},https://api.github.com/repos/apache/spark/issues/23104/comments,https://api.github.com/repos/apache/spark/issues/23104/events,https://github.com/apache/spark/pull/23104,383121740,MDExOlB1bGxSZXF1ZXN0MjMyNjY2Mzc4,23104,[SPARK-26138][SQL] Cross join requires push LocalLimit in LimitPushDown rule,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2018-11-21T13:37:43Z,2019-09-16T18:17:32Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

In LimitPushDown batch, cross join can push down the limit.

## How was this patch tested?

manual tests

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,guoxiaolongzte,26266482,MDQ6VXNlcjI2MjY2NDgy,https://avatars2.githubusercontent.com/u/26266482?v=4,,https://api.github.com/users/guoxiaolongzte,https://github.com/guoxiaolongzte,https://api.github.com/users/guoxiaolongzte/followers,https://api.github.com/users/guoxiaolongzte/following{/other_user},https://api.github.com/users/guoxiaolongzte/gists{/gist_id},https://api.github.com/users/guoxiaolongzte/starred{/owner}{/repo},https://api.github.com/users/guoxiaolongzte/subscriptions,https://api.github.com/users/guoxiaolongzte/orgs,https://api.github.com/users/guoxiaolongzte/repos,https://api.github.com/users/guoxiaolongzte/events{/privacy},https://api.github.com/users/guoxiaolongzte/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23104,https://github.com/apache/spark/pull/23104,https://github.com/apache/spark/pull/23104.diff,https://github.com/apache/spark/pull/23104.patch
350,https://api.github.com/repos/apache/spark/issues/23094,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23094/labels{/name},https://api.github.com/repos/apache/spark/issues/23094/comments,https://api.github.com/repos/apache/spark/issues/23094/events,https://github.com/apache/spark/pull/23094,382604089,MDExOlB1bGxSZXF1ZXN0MjMyMjcwMDM4,23094,[SPARK-26077][SQL] Reserved SQL words are not escaped by JDBC writer for table names,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-11-20T10:47:02Z,2019-09-16T18:17:34Z,,NONE,"## What changes were proposed in this pull request?

Currently, JDBC Writer doesn't quote tables names. This PR uses dialects to quote the them, too.

## How was this patch tested?

Related unit tests adjusted. In addition added new unit test to cover this bug. All tests are passing.
",spark,apache,golovan,20478245,MDQ6VXNlcjIwNDc4MjQ1,https://avatars2.githubusercontent.com/u/20478245?v=4,,https://api.github.com/users/golovan,https://github.com/golovan,https://api.github.com/users/golovan/followers,https://api.github.com/users/golovan/following{/other_user},https://api.github.com/users/golovan/gists{/gist_id},https://api.github.com/users/golovan/starred{/owner}{/repo},https://api.github.com/users/golovan/subscriptions,https://api.github.com/users/golovan/orgs,https://api.github.com/users/golovan/repos,https://api.github.com/users/golovan/events{/privacy},https://api.github.com/users/golovan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23094,https://github.com/apache/spark/pull/23094,https://github.com/apache/spark/pull/23094.diff,https://github.com/apache/spark/pull/23094.patch
351,https://api.github.com/repos/apache/spark/issues/23074,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23074/labels{/name},https://api.github.com/repos/apache/spark/issues/23074/comments,https://api.github.com/repos/apache/spark/issues/23074/events,https://github.com/apache/spark/pull/23074,381912048,MDExOlB1bGxSZXF1ZXN0MjMxNzU4MzYw,23074,[SPARK-19798][SQL] Refresh table does not have effect on other sessions than the issuing one,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-11-17T23:39:25Z,2019-09-16T18:19:39Z,,NONE,"## What changes were proposed in this pull request?
Refresh table command does not have effect on other sessions than the issuing one.

Move table relation cache from session catalog to session shared state so that different sessions can synchronize when a table is modified and refreshed.

## How was this patch tested?
New test in HiveMetadataCacheSuite",spark,apache,gbloisi,17691645,MDQ6VXNlcjE3NjkxNjQ1,https://avatars2.githubusercontent.com/u/17691645?v=4,,https://api.github.com/users/gbloisi,https://github.com/gbloisi,https://api.github.com/users/gbloisi/followers,https://api.github.com/users/gbloisi/following{/other_user},https://api.github.com/users/gbloisi/gists{/gist_id},https://api.github.com/users/gbloisi/starred{/owner}{/repo},https://api.github.com/users/gbloisi/subscriptions,https://api.github.com/users/gbloisi/orgs,https://api.github.com/users/gbloisi/repos,https://api.github.com/users/gbloisi/events{/privacy},https://api.github.com/users/gbloisi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23074,https://github.com/apache/spark/pull/23074,https://github.com/apache/spark/pull/23074.diff,https://github.com/apache/spark/pull/23074.patch
352,https://api.github.com/repos/apache/spark/issues/23042,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23042/labels{/name},https://api.github.com/repos/apache/spark/issues/23042/comments,https://api.github.com/repos/apache/spark/issues/23042/events,https://github.com/apache/spark/pull/23042,381046943,MDExOlB1bGxSZXF1ZXN0MjMxMTA0NTUy,23042,"[SPARK-26070][SQL] add rule for implicit type coercion for decimal(x,0)","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-11-15T08:18:20Z,2019-09-16T18:19:37Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Adding another excpetion rule for a popular case where ID columns are saved as decimal(x,0) and are compared with same ID columns in other tables which are saved as strings:
```
select '22222222222222222224' = 22222222222222222223BD -- returns true
```
this causes a major bug when we join to tables if one is defined as decimal and the other as string. in current situation we get a mix of good and bad results. we would prefer not to get results at all.

## How was this patch tested?

added a unit tests and made some manual tests
",spark,apache,uzadude,15645757,MDQ6VXNlcjE1NjQ1NzU3,https://avatars3.githubusercontent.com/u/15645757?v=4,,https://api.github.com/users/uzadude,https://github.com/uzadude,https://api.github.com/users/uzadude/followers,https://api.github.com/users/uzadude/following{/other_user},https://api.github.com/users/uzadude/gists{/gist_id},https://api.github.com/users/uzadude/starred{/owner}{/repo},https://api.github.com/users/uzadude/subscriptions,https://api.github.com/users/uzadude/orgs,https://api.github.com/users/uzadude/repos,https://api.github.com/users/uzadude/events{/privacy},https://api.github.com/users/uzadude/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23042,https://github.com/apache/spark/pull/23042,https://github.com/apache/spark/pull/23042.diff,https://github.com/apache/spark/pull/23042.patch
353,https://api.github.com/repos/apache/spark/issues/23032,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23032/labels{/name},https://api.github.com/repos/apache/spark/issues/23032/comments,https://api.github.com/repos/apache/spark/issues/23032/events,https://github.com/apache/spark/pull/23032,380623866,MDExOlB1bGxSZXF1ZXN0MjMwNzc2NjAz,23032,[WIP][SPARK-26061][SQL][MINOR] Reduce the number of unused UnsafeRowWriters created in whole-stage codegen,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2018-11-14T10:08:38Z,2019-09-16T19:08:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Reduce the number of unused `UnsafeRowWriter`s created in whole-stage generated code.
They come from the `CodegenSupport.consume()` calling `prepareRowVar()`, which uses `GenerateUnsafeProjection.createCode()` and registers an `UnsafeRowWriter` mutable state, regardless of whether or not the downstream (parent) operator will use the `rowVar` or not.
Even when the downstream `doConsume` function doesn't use the `rowVar` (i.e. doesn't put `row.code` as a part of this operator's codegen template), the registered `UnsafeRowWriter` stays there, which makes the init function of the generated code a bit bloated.

This PR doesn't heal the root issue, but makes it slightly less painful: when the `doConsume` function is split out, the `prepareRowVar()` function is called twice, so it's double the pain of unused `UnsafeRowWriter`s. This PR simply moves the original call to `prepareRowVar()` down into the `doConsume` split/no-split branch so that we're back to just 1x the pain.

To fix the root issue, something that allows the `CodegenSupport` operators to indicate whether or not they're going to use the `rowVar` would be needed. That's a much more elaborate change so I'd like to just make a minor fix first.

e.g. for this query: `spark.range(10).as[Long].map(x => x + 1).map(x => x * x + 2)`
**Before** (in Spark 2.4.0):
```java
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */
/* 027 */     range_taskContext_0 = TaskContext.get();
/* 028 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();
/* 029 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 030 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 031 */     range_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 032 */     range_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 033 */     range_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 034 */     range_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 035 */     range_mutableStateArray_0[6] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 036 */     range_mutableStateArray_0[7] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 037 */     range_mutableStateArray_0[8] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 038 */
/* 039 */   }
```
9 `UnsafeRowWriter`s created, 1 actually used.

**After**:
```java
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */
/* 027 */     range_taskContext_0 = TaskContext.get();
/* 028 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();
/* 029 */     deserializetoobject_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 030 */     deserializetoobject_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 031 */     deserializetoobject_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 032 */     deserializetoobject_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 033 */     deserializetoobject_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 034 */
/* 035 */   }
```
5 `UnsafeRowWriter`s created, 1 actually used.

## How was this patch tested?

Existing tests.",spark,apache,rednaxelafx,107834,MDQ6VXNlcjEwNzgzNA==,https://avatars1.githubusercontent.com/u/107834?v=4,,https://api.github.com/users/rednaxelafx,https://github.com/rednaxelafx,https://api.github.com/users/rednaxelafx/followers,https://api.github.com/users/rednaxelafx/following{/other_user},https://api.github.com/users/rednaxelafx/gists{/gist_id},https://api.github.com/users/rednaxelafx/starred{/owner}{/repo},https://api.github.com/users/rednaxelafx/subscriptions,https://api.github.com/users/rednaxelafx/orgs,https://api.github.com/users/rednaxelafx/repos,https://api.github.com/users/rednaxelafx/events{/privacy},https://api.github.com/users/rednaxelafx/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23032,https://github.com/apache/spark/pull/23032,https://github.com/apache/spark/pull/23032.diff,https://github.com/apache/spark/pull/23032.patch
354,https://api.github.com/repos/apache/spark/issues/23008,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/23008/labels{/name},https://api.github.com/repos/apache/spark/issues/23008/comments,https://api.github.com/repos/apache/spark/issues/23008/events,https://github.com/apache/spark/pull/23008,379568954,MDExOlB1bGxSZXF1ZXN0MjI5OTgxODgx,23008,[SPARK-22674][PYTHON] Removed the namedtuple pickling patch,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2018-11-11T22:11:35Z,2019-09-16T18:17:43Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Prior to this PR PySpark patched ``collections.namedtuple`` to make
namedtuple instances serializable even if the namedtuple class has been
defined outside of ``globals()``, e.g.

    def do_something():
        Foo = namedtuple(""Foo"", [""foo""])
        sc.parallelize(range(1)).map(lambda _: Foo(42))

The patch changed the pickled representation of the namedtuple instance
to include the structure of namedtuple class, and recreate the class on
each unpickling. This behaviour causes hard to diagnose failures both
in the user code with namedtuples, as well as third-party libraries
relying on them. See [1] and [2] for details.

[1]: https://superbobry.github.io/pyspark-silently-breaks-your-namedtuples.html
[2]: https://superbobry.github.io/tensorflowonspark-or-the-namedtuple-patch-strikes-again.html

The PR changes the default serializer to `CloudPickleSerializer` which natively supports pickling namedtuples and does not require the aforementioned patch. To the best of my knowledge, this is **not** a breaking change.

## How was this patch tested?

PySpark test suite.
",spark,apache,superbobry,185856,MDQ6VXNlcjE4NTg1Ng==,https://avatars1.githubusercontent.com/u/185856?v=4,,https://api.github.com/users/superbobry,https://github.com/superbobry,https://api.github.com/users/superbobry/followers,https://api.github.com/users/superbobry/following{/other_user},https://api.github.com/users/superbobry/gists{/gist_id},https://api.github.com/users/superbobry/starred{/owner}{/repo},https://api.github.com/users/superbobry/subscriptions,https://api.github.com/users/superbobry/orgs,https://api.github.com/users/superbobry/repos,https://api.github.com/users/superbobry/events{/privacy},https://api.github.com/users/superbobry/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/23008,https://github.com/apache/spark/pull/23008,https://github.com/apache/spark/pull/23008.diff,https://github.com/apache/spark/pull/23008.patch
355,https://api.github.com/repos/apache/spark/issues/22964,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22964/labels{/name},https://api.github.com/repos/apache/spark/issues/22964/comments,https://api.github.com/repos/apache/spark/issues/22964/events,https://github.com/apache/spark/pull/22964,378268647,MDExOlB1bGxSZXF1ZXN0MjI5MDEyNzIy,22964,[SPARK-25963] Optimize generate followed by window,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2018-11-07T12:32:51Z,2019-09-16T18:19:29Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
I've added an optimizer rule to add Repartition operator when we have a Generate operator followed by a Window function

## How was this patch tested?

I've added a plan unittest 
",spark,apache,uzadude,15645757,MDQ6VXNlcjE1NjQ1NzU3,https://avatars3.githubusercontent.com/u/15645757?v=4,,https://api.github.com/users/uzadude,https://github.com/uzadude,https://api.github.com/users/uzadude/followers,https://api.github.com/users/uzadude/following{/other_user},https://api.github.com/users/uzadude/gists{/gist_id},https://api.github.com/users/uzadude/starred{/owner}{/repo},https://api.github.com/users/uzadude/subscriptions,https://api.github.com/users/uzadude/orgs,https://api.github.com/users/uzadude/repos,https://api.github.com/users/uzadude/events{/privacy},https://api.github.com/users/uzadude/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22964,https://github.com/apache/spark/pull/22964,https://github.com/apache/spark/pull/22964.diff,https://github.com/apache/spark/pull/22964.patch
356,https://api.github.com/repos/apache/spark/issues/22957,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22957/labels{/name},https://api.github.com/repos/apache/spark/issues/22957/comments,https://api.github.com/repos/apache/spark/issues/22957/events,https://github.com/apache/spark/pull/22957,377917724,MDExOlB1bGxSZXF1ZXN0MjI4NzQ1Nzkx,22957,[SPARK-25951][SQL] Ignore aliases for distributions and orderings,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,63,2018-11-06T16:07:10Z,2019-09-16T19:08:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When we canonicalize an `Expression`, we do not remove `Alias`. So two expressions which are the same but are renamed are considered to be semantically different. As we rely on semantic equality in order to check if the result is the same for 2 expressions, some optimizations - as showed in the JIRA - may fail to apply, eg. removing redundant shuffles, when a column is renamed.

The PR proposes to ignore `Alias`es when checking whether distributions and orderings are satisfied by introducing a new method `sameResults` which ignore `Alias`es.

Credit should be given to @maropu for the approach suggestion which follows #17400.

Closes #17400.

## How was this patch tested?

added UT",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22957,https://github.com/apache/spark/pull/22957,https://github.com/apache/spark/pull/22957.diff,https://github.com/apache/spark/pull/22957.patch
357,https://api.github.com/repos/apache/spark/issues/22947,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22947/labels{/name},https://api.github.com/repos/apache/spark/issues/22947/comments,https://api.github.com/repos/apache/spark/issues/22947/events,https://github.com/apache/spark/pull/22947,377369014,MDExOlB1bGxSZXF1ZXN0MjI4MzI4MzQ0,22947,[SPARK-24913][SQL] Make AssertNotNull and AssertTrue non-deterministic,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2018-11-05T11:58:55Z,2019-09-16T19:08:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

https://github.com/apache/spark/pull/21848 introduces an optimization which causes wrong behavior with `AssertNotNull` and `AssertTrue`. This PR makes the two mentioned expressions as non-deterministic in order to avoid skipping evaluating them.

## How was this patch tested?

added UT
",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22947,https://github.com/apache/spark/pull/22947,https://github.com/apache/spark/pull/22947.diff,https://github.com/apache/spark/pull/22947.patch
358,https://api.github.com/repos/apache/spark/issues/22945,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22945/labels{/name},https://api.github.com/repos/apache/spark/issues/22945/comments,https://api.github.com/repos/apache/spark/issues/22945/events,https://github.com/apache/spark/pull/22945,377337755,MDExOlB1bGxSZXF1ZXN0MjI4MzA1MjYy,22945,[SPARK-24066][SQL]Add new optimization rule to eliminate unnecessary sort by exchanged adjacent Window expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-11-05T10:31:30Z,2019-09-16T19:08:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, when two adjacent window functions have the same partition and the same intersection of order, 
 There will be two sorted after shuffling, which is not necessary. This PR adds a new optimization rule to eliminate unnecessary sort by exchanged adjacent Window expressions.

For example:

```
val df = Seq((""a"", ""p1"", 10.0, 20.0, 30.0), (""a"", ""p2"", 20.0, 10.0, 40.0)).toDF(""key"", ""value"", ""value1"", ""value2"", ""value3"").select($""key"", sum(""value1"").over(Window.partitionBy(""key"").orderBy(""value"")), max(""value2"").over(Window.partitionBy(""key"").orderBy(""value"", ""value1"")), avg(""value3"").over(Window.partitionBy(""key"").orderBy(""value"", ""value1"", ""value2""))).queryExecution.executedPlan
println(df)
```
 
Before  this PR:

```
*(5) Project key#16, sum(value1) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST unspecifiedframe$())#29, max(value2) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST unspecifiedframe$())#30, avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31
 +- Window max(value2#19) windowspecdefinition(key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS max(value2) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST unspecifiedframe$())#30, key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST
    +- *(4) Project key#16, value1#18, value#17, value2#19, sum(value1) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST unspecifiedframe$())#29, avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31
       +- Window avg(value3#20) windowspecdefinition(key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, value2#19 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31, key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, value2#19 ASC NULLS FIRST
          +- *(3) Sort key#16 ASC NULLS FIRST, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, value2#19 ASC NULLS FIRST, false, 0
             +- Window sum(value1#18) windowspecdefinition(key#16, value#17 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(value1) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST unspecifiedframe$())#29, key#16, value#17 ASC NULLS FIRST
                +- *(2) Sort key#16 ASC NULLS FIRST, value#17 ASC NULLS FIRST, false, 0
                   +- Exchange hashpartitioning(key#16, 5)
                      +- *(1) Project _1#5 AS key#16, _3#7 AS value1#18, _2#6 AS value#17, _4#8 AS value2#19, _5#9 AS value3#20
                         +- LocalTableScan _1#5, _2#6, _3#7, _4#8, _5#9
```

After  this PR:

```
*(5) Project key#16, sum(value1) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST unspecifiedframe$())#29, max(value2) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST unspecifiedframe$())#30, avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31
 +- Window sum(value1#18) windowspecdefinition(key#16, value#17 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(value1) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST unspecifiedframe$())#29, key#16, value#17 ASC NULLS FIRST
    +- *(4) Project key#16, value1#18, value#17, avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31, max(value2) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST unspecifiedframe$())#30
       +- Window max(value2#19) windowspecdefinition(key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS max(value2) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST unspecifiedframe$())#30, key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST
          +- *(3) Project key#16, value1#18, value#17, value2#19, avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31
             +- Window avg(value3#20) windowspecdefinition(key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, value2#19 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS avg(value3) OVER (PARTITION BY key ORDER BY value ASC NULLS FIRST, value1 ASC NULLS FIRST, value2 ASC NULLS FIRST unspecifiedframe$())#31, key#16, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, value2#19 ASC NULLS FIRST
                +- *(2) Sort key#16 ASC NULLS FIRST, value#17 ASC NULLS FIRST, value1#18 ASC NULLS FIRST, value2#19 ASC NULLS FIRST, false, 0
                   +- Exchange hashpartitioning(key#16, 5)
                      +- *(1) Project _1#5 AS key#16, _3#7 AS value1#18, _2#6 AS value#17, _4#8 AS value2#19, _5#9 AS value3#20
                         +- LocalTableScan _1#5, _2#6, _3#7, _4#8, _5#9
```

## How was this patch tested?

add new unit tested
",spark,apache,heary-cao,20164092,MDQ6VXNlcjIwMTY0MDky,https://avatars1.githubusercontent.com/u/20164092?v=4,,https://api.github.com/users/heary-cao,https://github.com/heary-cao,https://api.github.com/users/heary-cao/followers,https://api.github.com/users/heary-cao/following{/other_user},https://api.github.com/users/heary-cao/gists{/gist_id},https://api.github.com/users/heary-cao/starred{/owner}{/repo},https://api.github.com/users/heary-cao/subscriptions,https://api.github.com/users/heary-cao/orgs,https://api.github.com/users/heary-cao/repos,https://api.github.com/users/heary-cao/events{/privacy},https://api.github.com/users/heary-cao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22945,https://github.com/apache/spark/pull/22945,https://github.com/apache/spark/pull/22945.diff,https://github.com/apache/spark/pull/22945.patch
359,https://api.github.com/repos/apache/spark/issues/22907,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22907/labels{/name},https://api.github.com/repos/apache/spark/issues/22907/comments,https://api.github.com/repos/apache/spark/issues/22907/events,https://github.com/apache/spark/pull/22907,376020989,MDExOlB1bGxSZXF1ZXN0MjI3MzMyMDQ1,22907,[SPARK-25896][CORE][WIP] Accumulator should only be updated once for each successful task in shuffle map stage,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-10-31T15:28:29Z,2019-10-01T01:13:54Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/19877

For the same reason, we should also update accumulator once for each successful task in shuffle map stage.

TODO:
1. `ShuffleMapStage` has `pendingPartitions` and `findMissingPartitions`, I'm not sure which one is the single source of truth
2. When we receive repeated successful shuffle map tasks, seems we will override the previous one. Need a double check.
3. add tests.

## How was this patch tested?

TODO",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22907,https://github.com/apache/spark/pull/22907,https://github.com/apache/spark/pull/22907.diff,https://github.com/apache/spark/pull/22907.patch
360,https://api.github.com/repos/apache/spark/issues/22905,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22905/labels{/name},https://api.github.com/repos/apache/spark/issues/22905/comments,https://api.github.com/repos/apache/spark/issues/22905/events,https://github.com/apache/spark/pull/22905,375950206,MDExOlB1bGxSZXF1ZXN0MjI3Mjc2MDQ5,22905,[SPARK-25894][SQL] Add a ColumnarFileFormat type which returns the column count for a given schema,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2018-10-31T12:59:38Z,2019-09-16T19:09:08Z,,CONTRIBUTOR,"(link to Jira: https://issues.apache.org/jira/browse/SPARK-25894)

## What changes were proposed in this pull request?

Knowing the number of physical columns Spark will read from a columnar file format (such as Parquet) is extremely helpful (if not critical) in debugging poor query performance on a parquet table with a deeply nested schema. This PR adds a new metadata field for `FileSourceScanExec` which identifies the maximum number of columns Spark will read from that file source. (N.B. the actual number of columns read may be lower if the physical file Spark is reading is missing some of them. For example, this can occur in a large partitioned table with a wide schema with sparse data‚Äîsome partitions may not have data for some columns.) This metadata is printed as part of a physical plan. For example, take a `contacts` table with the following schema:

```
root
 |-- id: integer (nullable = true)
 |-- name: struct (nullable = true)
 |    |-- first: string (nullable = true)
 |    |-- middle: string (nullable = true)
 |    |-- last: string (nullable = true)
 |-- address: string (nullable = true)
 |-- employer: struct (nullable = true)
 |    |-- id: integer (nullable = true)
 |    |-- company: struct (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- address: string (nullable = true)
```

With schema pruning, the following `explain` query

```
explain select name.first, employer.company.address from contacts
```

prints

```
== Physical Plan ==
*(1) Project [name#3726.first AS first#3742, employer#3728.company.address AS address#3743]
+- *(1) FileScan parquet [name#3726,employer#3728,p#3729] Batched: false, ColumnCount: 2,
        DataFilters: [], Format: Parquet, Location:
        InMemoryFileIndex[file:/blah/blah/spark/target/tmp/spark-686cac53-0...,
        PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema:
        struct<name:struct<first:string>,employer:struct<company:struct<address:string>>>
```

Note `ColumnCount: 2`. This tells us that schema pruning is working at planning time. Without schema pruning, that same query prints

```
== Physical Plan ==
*(1) Project [name#3726.first AS first#3742, employer#3728.company.address AS address#3743]
+- *(1) FileScan parquet [name#3726,employer#3728,p#3729] Batched: false, ColumnCount: 6,
        DataFilters: [], Format: Parquet, Location:
        InMemoryFileIndex[file:/blah/blah/spark/target/tmp/spark-947d2af3-8...,
        PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema:
        struct<name:struct<first:string,middle:string,last:string>,employer:struct<id:int,
        company:struct<...
```

Note `ColumnCount: 6`. This tells us either schema pruning is disabled or is not working as expected. If we've enabled schema pruning, this query plan gives us an obvious avenue for investigation of poor query performance.

## How was this patch tested?

A new test was added to `ParquetSchemaPruningSuite.scala`.",spark,apache,mallman,833693,MDQ6VXNlcjgzMzY5Mw==,https://avatars3.githubusercontent.com/u/833693?v=4,,https://api.github.com/users/mallman,https://github.com/mallman,https://api.github.com/users/mallman/followers,https://api.github.com/users/mallman/following{/other_user},https://api.github.com/users/mallman/gists{/gist_id},https://api.github.com/users/mallman/starred{/owner}{/repo},https://api.github.com/users/mallman/subscriptions,https://api.github.com/users/mallman/orgs,https://api.github.com/users/mallman/repos,https://api.github.com/users/mallman/events{/privacy},https://api.github.com/users/mallman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22905,https://github.com/apache/spark/pull/22905,https://github.com/apache/spark/pull/22905.diff,https://github.com/apache/spark/pull/22905.patch
361,https://api.github.com/repos/apache/spark/issues/22878,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22878/labels{/name},https://api.github.com/repos/apache/spark/issues/22878/comments,https://api.github.com/repos/apache/spark/issues/22878/events,https://github.com/apache/spark/pull/22878,375089346,MDExOlB1bGxSZXF1ZXN0MjI2NjE3MTM4,22878,[SPARK-25789][SQL] Support for Dataset of Avro,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2018-10-29T15:59:08Z,2019-09-16T19:09:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Please credit to @bdrillard cause this mainly based on his previous work.

This PR add support for Dataset of Avro records in an API that would allow the user to provide a class to an Encoder for Avro, analogous to the Bean encoder.

- Add `ObjectCast` and `InitializeAvroObject`(analogous to `InitializeJavaBean`) expression.
- Add an AvroEncoder for Datasets of Avro records to Spark.
- Add type-inference utilities `AvroTypeInference` for Avro object and SQL DataType (analogous to `JavaTypeInference`).

## How was this patch tested?

Add UT in AvroSuite.scala and manual test by modified SQLExample with external avro package.
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22878,https://github.com/apache/spark/pull/22878,https://github.com/apache/spark/pull/22878.diff,https://github.com/apache/spark/pull/22878.patch
362,https://api.github.com/repos/apache/spark/issues/22866,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22866/labels{/name},https://api.github.com/repos/apache/spark/issues/22866/comments,https://api.github.com/repos/apache/spark/issues/22866/events,https://github.com/apache/spark/pull/22866,374705822,MDExOlB1bGxSZXF1ZXN0MjI2MzM1NTA4,22866,WIP [SPARK-12172][SPARKR] Remove internal-only RDD methods,"[{'id': 1406606336, 'node_id': 'MDU6TGFiZWwxNDA2NjA2MzM2', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARKR', 'name': 'SPARKR', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-10-28T01:07:28Z,2019-11-15T17:39:02Z,,MEMBER,"## What changes were proposed in this pull request?

Remove non-public internal only methods for RDD in SparkR

## How was this patch tested?

Jenkins",spark,apache,felixcheung,8969467,MDQ6VXNlcjg5Njk0Njc=,https://avatars3.githubusercontent.com/u/8969467?v=4,,https://api.github.com/users/felixcheung,https://github.com/felixcheung,https://api.github.com/users/felixcheung/followers,https://api.github.com/users/felixcheung/following{/other_user},https://api.github.com/users/felixcheung/gists{/gist_id},https://api.github.com/users/felixcheung/starred{/owner}{/repo},https://api.github.com/users/felixcheung/subscriptions,https://api.github.com/users/felixcheung/orgs,https://api.github.com/users/felixcheung/repos,https://api.github.com/users/felixcheung/events{/privacy},https://api.github.com/users/felixcheung/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22866,https://github.com/apache/spark/pull/22866,https://github.com/apache/spark/pull/22866.diff,https://github.com/apache/spark/pull/22866.patch
363,https://api.github.com/repos/apache/spark/issues/22788,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22788/labels{/name},https://api.github.com/repos/apache/spark/issues/22788/comments,https://api.github.com/repos/apache/spark/issues/22788/events,https://github.com/apache/spark/pull/22788,372331817,MDExOlB1bGxSZXF1ZXN0MjI0NTUyNTAw,22788,[SPARK-25769][SQL]make UnresolvedAttribute.sql escape nested columns correctly,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2018-10-21T16:24:32Z,2019-09-16T19:09:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently, the nested columns are not escaped correctly:
```
scala> $""a.b"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res0: String = `a.b`

scala> $""`a.b`"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res1: String = `a.b`   // ambiguous

scala> $""`a`.b"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res2: String = `a.b`    // ambiquous

scala> $""`a.b`.c"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res3: String = ```a.b``.c`    // two verbose
```
It should be something as the following:
```
scala> $""a.b"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res0: String = a.b

scala> $""`a.b`"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res1: String = `a.b`

scala> $""`a`.b"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res2: String = a.b

scala> $""`a.b`.c"".expr.asInstanceOf[org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute].sql
res3: String = `a.b`.c
```




## How was this patch tested?
Add test


",spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22788,https://github.com/apache/spark/pull/22788,https://github.com/apache/spark/pull/22788.diff,https://github.com/apache/spark/pull/22788.patch
364,https://api.github.com/repos/apache/spark/issues/22774,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22774/labels{/name},https://api.github.com/repos/apache/spark/issues/22774/comments,https://api.github.com/repos/apache/spark/issues/22774/events,https://github.com/apache/spark/pull/22774,371831989,MDExOlB1bGxSZXF1ZXN0MjI0MTg5Nzcw,22774,[SPARK-25780][CORE]Scheduling the tasks which have no higher level locality first,"[{'id': 1406606875, 'node_id': 'MDU6TGFiZWwxNDA2NjA2ODc1', 'url': 'https://api.github.com/repos/apache/spark/labels/SCHEDULER', 'name': 'SCHEDULER', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2018-10-19T07:07:13Z,2019-09-16T19:10:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
For example:
An application has two executors: (exe1, host1), (exe2,host2), and 3 tasks with locality:  {task0, Seq(TaskLocation(""host1"", ""exec1""))}, {task1, Seq(TaskLocation(""host1"", ""exec1""), TaskLocation(""host2""))},  {task2, Seq(TaskLocation(""host2"")}

If task0 is runing in exe1, when `allowedLocality` is NODE_LOCAL for exe2, it is better to schedule task2 fisrt, not task1, because task1 may be scheduled to exe1 later.

## How was this patch tested?
Added a UT
",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22774,https://github.com/apache/spark/pull/22774,https://github.com/apache/spark/pull/22774.diff,https://github.com/apache/spark/pull/22774.patch
365,https://api.github.com/repos/apache/spark/issues/22758,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22758/labels{/name},https://api.github.com/repos/apache/spark/issues/22758/comments,https://api.github.com/repos/apache/spark/issues/22758/events,https://github.com/apache/spark/pull/22758,371261093,MDExOlB1bGxSZXF1ZXN0MjIzNzQ4NzY1,22758,[SPARK-25332][SQL] select broadcast join instead of sortMergeJoin for the small size table even query fired via new session/context ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2018-10-17T20:49:38Z,2019-09-16T19:10:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Problem:
Instead of broadcast hash join ,Sort merge join has selected when restart spark-shell/spark-JDBC for hive provider.
```
spark.sql(""create table x1(name string,age int) stored as parquet "")
spark.sql(""insert into x1 select 'a',29"")
spark.sql(""create table x2 (name string,age int) stored as parquet '"")
spark.sql(""insert into x2_ex select 'a',29"")
scala> spark.sql(""select * from x1 t1 ,x2 t2 where t1.name=t2.name"").explain
```
Query Plan:
```
== Physical Plan ==
*(2) BroadcastHashJoin name#101, name#103, Inner, BuildRight
:- *(2) Project name#101, age#102
: +- *(2) Filter isnotnull(name#101)
: +- *(2) FileScan parquet default.x1_exname#101,age#102 Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/D:/spark_release/spark/bin/spark-warehouse/x1, PartitionFilters: [], PushedFilters: [IsNotNull(name)], ReadSchema: struct<name:string,age:int>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))
+- *(1) Project name#103, age#104
+- *(1) Filter isnotnull(name#103)
+- *(1) FileScan parquet default.x2_exname#103,age#104 Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/D:/spark_release/spark/bin/spark-warehouse/x2, PartitionFilters: [], PushedFilters: [IsNotNull(name)], ReadSchema: struct<name:string,age:int>
```

Now Restart Spark-Shell or do spark-submit or restsrt JDBCServer  again and run same select query
```
 scala> spark.sql(""select * from x1 t1 ,x2 t2 where t1.name=t2.name"").explain
scala> spark.sql(""select * from x1 t1 ,x2 t2 where t1.name=t2.name"").explain
```
Query plan will be modified as below.
```
== Physical Plan ==
*(5) SortMergeJoin [name#43], name#45, Inner
:- *(2) Sort name#43 ASC NULLS FIRST, false, 0
: +- Exchange hashpartitioning(name#43, 200)
: +- *(1) Project name#43, age#44
: +- *(1) Filter isnotnull(name#43)
: +- *(1) FileScan parquet default.x1name#43,age#44 Batched: true, Format: Parquet, Location: InMemoryFileIndexfile:/D:/spark_release/spark/bin/spark-warehouse/x1, PartitionFilters: [], PushedFilters: [IsNotNull(name)], ReadSchema: struct<name:string,age:int>
+- *(4) Sort name#45 ASC NULLS FIRST, false, 0
+- Exchange hashpartitioning(name#45, 200)
+- *(3) Project name#45, age#46
+- *(3) Filter isnotnull(name#45)
+- *(3) FileScan parquet default.x2name#45,age#46 Batched: true, Format: Parquet, Location: InMemoryFileIndexfile:/D:/spark_release/spark/bin/spark-warehouse/x2, PartitionFilters: [], PushedFilters: [IsNotNull(name)], ReadSchema: struct<name:string,age:int>

```
## What changes were proposed in this pull request?
Here the main problem is hive stats is not getting recorded after insert command, this is because of a condition  ""if (table.stats.nonEmpty)"" in updateTableStats()  which will be executed as part of InsertIntoHadoopFsRelationCommand command. since the CatalogTable stats never initialized, this condition never meets. so again if we fire same query in a new context/session the plan ll changed to SortMergeJoin.

![image](https://user-images.githubusercontent.com/12999161/47115820-58f50e00-d27d-11e8-97b0-ccee7c22bff5.png).

So as part of fix we initialized a default value for the CatalogTable stats if there is no cache of a particular LogicalRelation.


As per this  statement after insert we are expecting the Hivestats shall get updated, please correct me if i am missing something. 

## How was this patch tested?
Manually tested, attaching the snapshot.
**After Fix**
Login to spark-shell => create 2 tables => Run insert commad => Explain the check the plan =>Plan contains Broadcast join => Exit

![step-1-afterfix-spark-25332](https://user-images.githubusercontent.com/12999161/47113323-52af6380-d276-11e8-9eb9-71d1076d7e38.PNG)

Step 2:
Relaunch Spark-shell => Run explain command of particular select statement => verify the plan => Plan still contains Broadcast join since after fix Hivestats is available for the table.
![step-2-afterfix-spark-25332](https://user-images.githubusercontent.com/12999161/47113407-94400e80-d276-11e8-99c1-66fa0c333beb.PNG)

Step 3:
Again Run insert command => Run explain command of particular select statement => verify the plan
we can observer the plan still retains BroadcastJoin - Nowonwards the results are always consistent
",spark,apache,sujith71955,12999161,MDQ6VXNlcjEyOTk5MTYx,https://avatars1.githubusercontent.com/u/12999161?v=4,,https://api.github.com/users/sujith71955,https://github.com/sujith71955,https://api.github.com/users/sujith71955/followers,https://api.github.com/users/sujith71955/following{/other_user},https://api.github.com/users/sujith71955/gists{/gist_id},https://api.github.com/users/sujith71955/starred{/owner}{/repo},https://api.github.com/users/sujith71955/subscriptions,https://api.github.com/users/sujith71955/orgs,https://api.github.com/users/sujith71955/repos,https://api.github.com/users/sujith71955/events{/privacy},https://api.github.com/users/sujith71955/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22758,https://github.com/apache/spark/pull/22758,https://github.com/apache/spark/pull/22758.diff,https://github.com/apache/spark/pull/22758.patch
366,https://api.github.com/repos/apache/spark/issues/22721,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22721/labels{/name},https://api.github.com/repos/apache/spark/issues/22721/comments,https://api.github.com/repos/apache/spark/issues/22721/events,https://github.com/apache/spark/pull/22721,370009387,MDExOlB1bGxSZXF1ZXN0MjIyNzg5MTI0,22721,[SPARK-19784][SPARK-25403][SQL] Refresh the table even table stats is empty,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2018-10-15T05:56:44Z,2019-09-18T12:38:08Z,,MEMBER,"## What changes were proposed in this pull request?

We invalidate table relation once table data is changed by [SPARK-21237](https://issues.apache.org/jira/browse/SPARK-21237). But there is a situation we have not invalidated(`spark.sql.statistics.size.autoUpdate.enabled=false` and `table.stats.isEmpty`):
https://github.com/apache/spark/blob/07c4b9bd1fb055f283af076b2a995db8f6efe7a5/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala#L44-L54

This will introduce some issues, e.g. [SPARK-19784](https://issues.apache.org/jira/browse/SPARK-19784), [SPARK-19845](https://issues.apache.org/jira/browse/SPARK-19845), [SPARK-25403](https://issues.apache.org/jira/browse/SPARK-25403), [SPARK-25332](https://issues.apache.org/jira/browse/SPARK-25332) and [SPARK-28413](https://issues.apache.org/jira/browse/SPARK-28413). 

This is a example to reproduce [SPARK-19784](https://issues.apache.org/jira/browse/SPARK-19784):
```scala
val path = ""/tmp/spark/parquet""
spark.sql(""CREATE TABLE t (a INT) USING parquet"")
spark.sql(""INSERT INTO TABLE t VALUES (1)"")
spark.range(5).toDF(""a"").write.parquet(path)
spark.sql(s""ALTER TABLE t SET LOCATION '${path}'"")
spark.table(""t"").count() // return 1
spark.sql(""refresh table t"")
spark.table(""t"").count() // return 5
```

This PR invalidates the table relation in this case(`spark.sql.statistics.size.autoUpdate.enabled=false` and `table.stats.isEmpty`) to fix this issue.

## How was this patch tested?

unit tests
",spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22721,https://github.com/apache/spark/pull/22721,https://github.com/apache/spark/pull/22721.diff,https://github.com/apache/spark/pull/22721.patch
367,https://api.github.com/repos/apache/spark/issues/22707,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22707/labels{/name},https://api.github.com/repos/apache/spark/issues/22707/comments,https://api.github.com/repos/apache/spark/issues/22707/events,https://github.com/apache/spark/pull/22707,369479541,MDExOlB1bGxSZXF1ZXN0MjIyNDEwOTAz,22707,[SPARK-25717][SQL] Insert overwrite a recreated external and partitioned table may result in incorrect query results,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2018-10-12T09:50:31Z,2019-09-16T18:20:59Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Consider the following scenario:

```
spark.range(100).createTempView(""temp"")
(0 until 3).foreach { _ =>
  spark.sql(""drop table if exists tableA"")
  spark.sql(""create table if not exists tableA(a int) partitioned by (p int) location 'file:/e:/study/warehouse/tableA'"")
  spark.sql(""insert overwrite table tableA partition(p=1) select * from temp"")
  spark.sql(""select count(1) from tableA where p=1"").show
}
```

We expect the count always be 100, but the actual results are as follows:

```
+--------+
|count(1)|
+--------+
|     100|
+--------+

+--------+
|count(1)|
+--------+
|   200|
+--------+

+--------+
|count(1)|
+--------+
|   300|
+--------+
```

when spark executes an `insert overwrite` command,  it gets the historical partition first, and then delete it from fileSystem.

But for recreated external and partitioned table, the partitions were all deleted by the `drop  table` command with data unremoved. So the historical data is preserved which lead to the query results incorrect.

## How was this patch tested?
Add test.",spark,apache,fjh100456,26785576,MDQ6VXNlcjI2Nzg1NTc2,https://avatars2.githubusercontent.com/u/26785576?v=4,,https://api.github.com/users/fjh100456,https://github.com/fjh100456,https://api.github.com/users/fjh100456/followers,https://api.github.com/users/fjh100456/following{/other_user},https://api.github.com/users/fjh100456/gists{/gist_id},https://api.github.com/users/fjh100456/starred{/owner}{/repo},https://api.github.com/users/fjh100456/subscriptions,https://api.github.com/users/fjh100456/orgs,https://api.github.com/users/fjh100456/repos,https://api.github.com/users/fjh100456/events{/privacy},https://api.github.com/users/fjh100456/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22707,https://github.com/apache/spark/pull/22707,https://github.com/apache/spark/pull/22707.diff,https://github.com/apache/spark/pull/22707.patch
368,https://api.github.com/repos/apache/spark/issues/22646,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22646/labels{/name},https://api.github.com/repos/apache/spark/issues/22646/comments,https://api.github.com/repos/apache/spark/issues/22646/events,https://github.com/apache/spark/pull/22646,367344060,MDExOlB1bGxSZXF1ZXN0MjIwODA1NDg2,22646,"[SPARK-25654][SQL] Support for nested JavaBean arrays, lists and maps in createDataFrame","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-10-05T19:45:27Z,2019-09-16T18:20:55Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Continuing from #22527, this PR seeks to add support for beans in array, list and map fields when creating DataFrames from Java beans.

## How was this patch tested?

Appropriate unit tests were amended.

Also manually tested in Spark shell:

```
scala> import scala.beans.BeanProperty
import scala.beans.BeanProperty

scala> class Nested(@BeanProperty var i: Int) extends Serializable
defined class Nested

scala> class Test(@BeanProperty var array: Array[Nested], @BeanProperty var list: java.util.List[Nested], @BeanProperty var map: java.util.Map[Integer, Nested]) extends Serializable
defined class Test

scala> import scala.collection.JavaConverters._
import scala.collection.JavaConverters._

scala> val array = Array(new Nested(1))
array: Array[Nested] = Array(Nested@757ad227)

scala> val list = Seq(new Nested(2), new Nested(3)).asJava
list: java.util.List[Nested] = [Nested@633dce39, Nested@4dd28982]

scala> val map = Map(Int.box(1) -> new Nested(4), Int.box(2) -> new Nested(5)).asJava
map: java.util.Map[Integer,Nested] = {1=Nested@57421e4e, 2=Nested@5a75bad4}

scala> val df = spark.createDataFrame(Seq(new Test(array, list, map)).asJava, classOf[Test])
df: org.apache.spark.sql.DataFrame = [array: array<struct<i:int>>, list: array<struct<i:int>> ... 1 more field]

scala> df.show()
+-----+----------+--------------------+
|array|      list|                 map|
+-----+----------+--------------------+
|[[1]]|[[2], [3]]|[1 -> [4], 2 -> [5]]|
+-----+----------+--------------------+
```

Previous behavior:

```
scala> val df = spark.createDataFrame(Seq(new Test(array, list, map)).asJava, classOf[Test])
java.lang.IllegalArgumentException: The value (Nested@3dedc8b8) of the type (Nested) cannot be converted to struct<i:int>
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:262)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:238)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:103)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter$$anonfun$toCatalystImpl$1.apply(CatalystTypeConverters.scala:162)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter.toCatalystImpl(CatalystTypeConverters.scala:162)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter.toCatalystImpl(CatalystTypeConverters.scala:154)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:103)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToCatalystConverter$2.apply(CatalystTypeConverters.scala:396)
  at org.apache.spark.sql.SQLContext$$anonfun$createStructConverter$1$1$$anonfun$apply$1.apply(SQLContext.scala:1114)
  at org.apache.spark.sql.SQLContext$$anonfun$createStructConverter$1$1$$anonfun$apply$1.apply(SQLContext.scala:1113)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.SQLContext$$anonfun$createStructConverter$1$1.apply(SQLContext.scala:1113)
  at org.apache.spark.sql.SQLContext$$anonfun$createStructConverter$1$1.apply(SQLContext.scala:1108)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
  at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
  at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
  at scala.collection.TraversableOnce$class.toSeq(TraversableOnce.scala:298)
  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1334)
  at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:423)
  ... 51 elided
```",spark,apache,michalsenkyr,8831737,MDQ6VXNlcjg4MzE3Mzc=,https://avatars1.githubusercontent.com/u/8831737?v=4,,https://api.github.com/users/michalsenkyr,https://github.com/michalsenkyr,https://api.github.com/users/michalsenkyr/followers,https://api.github.com/users/michalsenkyr/following{/other_user},https://api.github.com/users/michalsenkyr/gists{/gist_id},https://api.github.com/users/michalsenkyr/starred{/owner}{/repo},https://api.github.com/users/michalsenkyr/subscriptions,https://api.github.com/users/michalsenkyr/orgs,https://api.github.com/users/michalsenkyr/repos,https://api.github.com/users/michalsenkyr/events{/privacy},https://api.github.com/users/michalsenkyr/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22646,https://github.com/apache/spark/pull/22646,https://github.com/apache/spark/pull/22646.diff,https://github.com/apache/spark/pull/22646.patch
369,https://api.github.com/repos/apache/spark/issues/22625,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22625/labels{/name},https://api.github.com/repos/apache/spark/issues/22625/comments,https://api.github.com/repos/apache/spark/issues/22625/events,https://github.com/apache/spark/pull/22625,366565661,MDExOlB1bGxSZXF1ZXN0MjIwMjA4MTQz,22625,[SPARK-25637][CORE] SparkException: Could not find CoarseGrainedScheduler occurs during the application stop,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-10-03T23:18:43Z,2019-11-06T15:15:06Z,,NONE,"## What changes were proposed in this pull request?

If the driverEndpointStopped is stopped, with this PR CoarseGrainedSchedulerBackend will not send reviveOffers message to the driverEndpoint.

## How was this patch tested?

Verified this manually, 'org.apache.spark.SparkException: Could not find CoarseGrainedScheduler' error is not occurring after applying this PR.
",spark,apache,devaraj-kavali,3174804,MDQ6VXNlcjMxNzQ4MDQ=,https://avatars0.githubusercontent.com/u/3174804?v=4,,https://api.github.com/users/devaraj-kavali,https://github.com/devaraj-kavali,https://api.github.com/users/devaraj-kavali/followers,https://api.github.com/users/devaraj-kavali/following{/other_user},https://api.github.com/users/devaraj-kavali/gists{/gist_id},https://api.github.com/users/devaraj-kavali/starred{/owner}{/repo},https://api.github.com/users/devaraj-kavali/subscriptions,https://api.github.com/users/devaraj-kavali/orgs,https://api.github.com/users/devaraj-kavali/repos,https://api.github.com/users/devaraj-kavali/events{/privacy},https://api.github.com/users/devaraj-kavali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22625,https://github.com/apache/spark/pull/22625,https://github.com/apache/spark/pull/22625.diff,https://github.com/apache/spark/pull/22625.patch
370,https://api.github.com/repos/apache/spark/issues/22583,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22583/labels{/name},https://api.github.com/repos/apache/spark/issues/22583/comments,https://api.github.com/repos/apache/spark/issues/22583/events,https://github.com/apache/spark/pull/22583,364854480,MDExOlB1bGxSZXF1ZXN0MjE4OTQwOTI5,22583,[SPARK-10816][SS] SessionWindow support for Structure Streaming,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2018-09-28T11:42:24Z,2019-09-16T19:10:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

cc co-author @LiangchangZ @ivoson @yanlin-Lynn

As the discussion in [SPARK-10816](https://issues.apache.org/jira/browse/SPARK-10816), here we add the support for session window support in Structure Streaming. For convenient review, this PR can be split to several part:

- Implement for session_window build-in function in dataset api, add related analysis rule and exec node.  [LINK](https://github.com/apache/spark/pull/22583/files/a2517b610cbb38123d1edf4eda2a5186d44b1a22#diff-5b6b40452a72e24b45e1cc36f4f88fac)
- Implement for integrating with streaming frame work, including the SessionWindowStateStoreRestoreExec/SessionWindowMergeExec/SessionWindowStateStoreSaveExec. [LINK](https://github.com/apache/spark/pull/22583/files/a2517b610cbb38123d1edf4eda2a5186d44b1a22..cb770c76147bbc20bb3627c913c3526b2f8dc161)

## How was this patch tested?

Test for dataset/sql and end-to-end streaming tests.",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22583,https://github.com/apache/spark/pull/22583,https://github.com/apache/spark/pull/22583.diff,https://github.com/apache/spark/pull/22583.patch
371,https://api.github.com/repos/apache/spark/issues/22575,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22575/labels{/name},https://api.github.com/repos/apache/spark/issues/22575/comments,https://api.github.com/repos/apache/spark/issues/22575/events,https://github.com/apache/spark/pull/22575,364713186,MDExOlB1bGxSZXF1ZXN0MjE4ODMzNjU1,22575,[SPARK-24630][SS] Support SQLStreaming in Spark,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2018-09-28T01:59:41Z,2019-09-16T18:20:45Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This patch propose new support of SQLStreaming in Spark, Please refer [SPARK-24630](https://issues.apache.org/jira/browse/SPARK-24630) for more details.

This patch supports: 
1. Support create stream table, which can be used as Source and Sink in SQLStreaming;
`create table kafka_sql_test using kafka 
options(
    isStreaming = 'true',
    subscribe = 'topic', 
    kafka.bootstrap.servers = 'localhost:9092')`
2. Add keyword 'STREAM' in sql to support SQLStreaming queries;
`select stream * from kafka_sql_test`
3. As for those complex queries, they all can be supported as long as SQL and StructStreaming support.

## How was this patch tested?
Some UTs are added to verify sqlstreaming.",spark,apache,stczwd,10897625,MDQ6VXNlcjEwODk3NjI1,https://avatars0.githubusercontent.com/u/10897625?v=4,,https://api.github.com/users/stczwd,https://github.com/stczwd,https://api.github.com/users/stczwd/followers,https://api.github.com/users/stczwd/following{/other_user},https://api.github.com/users/stczwd/gists{/gist_id},https://api.github.com/users/stczwd/starred{/owner}{/repo},https://api.github.com/users/stczwd/subscriptions,https://api.github.com/users/stczwd/orgs,https://api.github.com/users/stczwd/repos,https://api.github.com/users/stczwd/events{/privacy},https://api.github.com/users/stczwd/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22575,https://github.com/apache/spark/pull/22575,https://github.com/apache/spark/pull/22575.diff,https://github.com/apache/spark/pull/22575.patch
372,https://api.github.com/repos/apache/spark/issues/22573,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22573/labels{/name},https://api.github.com/repos/apache/spark/issues/22573/comments,https://api.github.com/repos/apache/spark/issues/22573/events,https://github.com/apache/spark/pull/22573,364600503,MDExOlB1bGxSZXF1ZXN0MjE4NzQ3NzQ5,22573,[SPARK-25558][SQL] Pushdown predicates for nested fields in DataSource Strategy,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2018-09-27T18:30:28Z,2019-09-16T19:10:07Z,,MEMBER,"## What changes were proposed in this pull request?

Allows Spark to translate a Catalyst `Expression` on a nested field into a data source `Filter`, and it's a building block to have Parquet, ORC, and other data sources to support the nested predicate pushdown.

## How was this patch tested?

Tests added",spark,apache,dbtsai,1134574,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22573,https://github.com/apache/spark/pull/22573,https://github.com/apache/spark/pull/22573.diff,https://github.com/apache/spark/pull/22573.patch
373,https://api.github.com/repos/apache/spark/issues/22560,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22560/labels{/name},https://api.github.com/repos/apache/spark/issues/22560/comments,https://api.github.com/repos/apache/spark/issues/22560/events,https://github.com/apache/spark/pull/22560,364214726,MDExOlB1bGxSZXF1ZXN0MjE4NDU3MzIy,22560,[SPARK-25547][SQL] Pluggable JDBC connection factory,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2018-09-26T21:15:35Z,2019-09-16T18:18:55Z,,NONE,"## What changes were proposed in this pull request?

Allow for pluggable connection factories in the spark jdbc package.

* new option in JDBCOptions called JDBC_CONNECTION_FACTORY_PROVIDER
* changes to JdbcUtils.createConnectionFactory to use the above
* when unspecified, the existing DefaultConnectionFactoryProvider is used
* provided unit tests

Without these changes we had to copy most of the spark jdbc package into our own codebase
to allow us to create our own connection factory in order to load balance queries against
an AWS Aurora postgres cluster.

## How was this patch tested?

added unit tests and we use this at Kabbage to load-balance queries against an AWS Aurora postgres cluster with code like the following:

```scala
package com.kabbage.rds

class RDSLoadBalancingConnectionFactory extends ConnectionFactoryProvider {
  override def createConnectionFactory(options: JDBCOptions): () => Connection = {
    () => LoadDriver(options).connect(RDS.balancedUrl, options.asConnectionProperties)
  }
}

object RDS  {
  lazy val config: Config = ConfigFactory.load(s""application-${System.getProperty(""env"")}"").getConfig(""jobconfig"")

  lazy val clusterId = config.getString(""rds.cluster"")

  lazy val rds = AmazonRDSClientBuilder.defaultClient() // requires AWS_REGION environment variable as well as AWS creds

  private var endpoints: Seq[String] = null

  def balancedUrl: String = this.synchronized {
    // initialize or rotate list of endpoints
    endpoints = if (endpoints == null) {
      rds.describeDBInstances().getDBInstances.asScala
        .filter(i => i.getDBClusterIdentifier == clusterId && i.getDBInstanceStatus == ""available"")
        .map(instance => s""${instance.getEndpoint.getAddress}:${instance.getEndpoint.getPort}"")
    } else endpoints.drop(1) ++ endpoints.take(1)
    endpoints.mkString(s""jdbc:postgresql://"","","",""/db"")
  }

}
``` 
and then enable the factory with something like:

```scala
    spark.sqlContext.read
      .format(""jdbc"")
      .option(JDBCOptions.JDBC_CONNECTION_FACTORY_PROVIDER, ""com.kabbage.rds.RDSLoadBalancingConnectionFactory"")
      .options(connectionProps)
      .option(JDBCOptions.JDBC_TABLE_NAME, table)
  }
```",spark,apache,fsauer65,869597,MDQ6VXNlcjg2OTU5Nw==,https://avatars1.githubusercontent.com/u/869597?v=4,,https://api.github.com/users/fsauer65,https://github.com/fsauer65,https://api.github.com/users/fsauer65/followers,https://api.github.com/users/fsauer65/following{/other_user},https://api.github.com/users/fsauer65/gists{/gist_id},https://api.github.com/users/fsauer65/starred{/owner}{/repo},https://api.github.com/users/fsauer65/subscriptions,https://api.github.com/users/fsauer65/orgs,https://api.github.com/users/fsauer65/repos,https://api.github.com/users/fsauer65/events{/privacy},https://api.github.com/users/fsauer65/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22560,https://github.com/apache/spark/pull/22560,https://github.com/apache/spark/pull/22560.diff,https://github.com/apache/spark/pull/22560.patch
374,https://api.github.com/repos/apache/spark/issues/22535,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22535/labels{/name},https://api.github.com/repos/apache/spark/issues/22535/comments,https://api.github.com/repos/apache/spark/issues/22535/events,https://github.com/apache/spark/pull/22535,363066104,MDExOlB1bGxSZXF1ZXN0MjE3NTk0NTUx,22535,[SPARK-17636][SQL][WIP] Parquet predicate pushdown in nested fields,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-09-24T09:14:29Z,2019-12-10T02:51:06Z,,MEMBER,"## What changes were proposed in this pull request?

Support Parquet predicate pushdown in nested fields

## How was this patch tested?

Existing tests and new tests are added.",spark,apache,dbtsai,1134574,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22535,https://github.com/apache/spark/pull/22535,https://github.com/apache/spark/pull/22535.diff,https://github.com/apache/spark/pull/22535.patch
375,https://api.github.com/repos/apache/spark/issues/22487,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22487/labels{/name},https://api.github.com/repos/apache/spark/issues/22487/comments,https://api.github.com/repos/apache/spark/issues/22487/events,https://github.com/apache/spark/pull/22487,362079549,MDExOlB1bGxSZXF1ZXN0MjE2ODg5MDkz,22487,[SPARK-25477] ‚ÄúINSERT OVERWRITE LOCAL DIRECTORY‚ÄùÔºå the data files allo‚Ä¶,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-09-20T08:42:47Z,2019-09-16T18:22:03Z,,CONTRIBUTOR,"‚Ä¶cated on the non-driver node will not be written to the specified output directory

## What changes were proposed in this pull request?
As  The ""INSERT OVERWRITE LOCAL DIRECTORY"" features use the local staging directory to load data into the specified output directory , the data files allocated on the non-driver node will not be written to the specified output directory. 
In saveAsHiveFile.scala, the code is based on the output directory to determine whether to use the local staging directory or the distributed staging directory. I change the getStagingDir() method. Modify the first parameter from 
"" new Path(extURI.getScheme, extURI.getAuthority, extURI.getPath) "" to ""new Path(extURI.getPath)""

If spark depends on the distributed storage system, then it will be used first. If it is not, it will be used locally. You can directly adjust it to let it be automatically selected instead of specifying it according to the output directory.

## How was this patch tested?
manual tests

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,httfighter,15611708,MDQ6VXNlcjE1NjExNzA4,https://avatars0.githubusercontent.com/u/15611708?v=4,,https://api.github.com/users/httfighter,https://github.com/httfighter,https://api.github.com/users/httfighter/followers,https://api.github.com/users/httfighter/following{/other_user},https://api.github.com/users/httfighter/gists{/gist_id},https://api.github.com/users/httfighter/starred{/owner}{/repo},https://api.github.com/users/httfighter/subscriptions,https://api.github.com/users/httfighter/orgs,https://api.github.com/users/httfighter/repos,https://api.github.com/users/httfighter/events{/privacy},https://api.github.com/users/httfighter/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22487,https://github.com/apache/spark/pull/22487,https://github.com/apache/spark/pull/22487.diff,https://github.com/apache/spark/pull/22487.patch
376,https://api.github.com/repos/apache/spark/issues/22482,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22482/labels{/name},https://api.github.com/repos/apache/spark/issues/22482/comments,https://api.github.com/repos/apache/spark/issues/22482/events,https://github.com/apache/spark/pull/22482,362018103,MDExOlB1bGxSZXF1ZXN0MjE2ODQ0NDc1,22482,WIP - [SPARK-10816][SS] Support session window natively,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,41,2018-09-20T04:53:09Z,2019-09-16T18:19:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This patch proposes native support of session window, like Spark has been supporting for time window.

Please refer the attached doc in [SPARK-10816](https://issues.apache.org/jira/browse/SPARK-10816) for more details on rationalization, concepts, and limitation, etc.

In point of end users' view, only the change is addition of ""session"" SQL function. End users could define query with session window as replacing ""window"" function to ""session"" function, and ""window"" column to ""session"" column. After then the patch will provide same experience with time window.

Internally, this patch will change the physical plan of aggregation a bit: if there's session function being used in query, it will sort the input rows as ""grouping keys"" + ""session"", and merge overlapped sessions into one with applying aggregations, so it's like a sort based aggregation but the unit of group is grouping keys + session.

Due to handle late event, there's a case multiple session windows co-exist per key which are not yet to evict. This patch handles the case via borrowing state implementation from streaming join which can handle multiple values for given key.

## How was this patch tested?

Many UTs are added to verify session window queries for both batch and streaming.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22482,https://github.com/apache/spark/pull/22482,https://github.com/apache/spark/pull/22482.diff,https://github.com/apache/spark/pull/22482.patch
377,https://api.github.com/repos/apache/spark/issues/22466,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22466/labels{/name},https://api.github.com/repos/apache/spark/issues/22466/comments,https://api.github.com/repos/apache/spark/issues/22466/events,https://github.com/apache/spark/pull/22466,361709390,MDExOlB1bGxSZXF1ZXN0MjE2NjA5NzEy,22466,"[SPARK-25464][SQL] Create Database to the location,only if it is empty or does not exists.","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,68,2018-09-19T11:47:00Z,2019-09-16T18:19:08Z,,CONTRIBUTOR,"Modification content:If the database is created with location URI then should be empty/should not exists

 What changes were proposed in this pull request?

If the user specifies the location while creating the Database then location should be empty or should not exists

for e.g create database db1 location '/user/hive/warehouse';
this command will be passed only if 'user/hive/warehouse' is empty or should not exists.

 How was this patch tested?
Added testcases and also manually verified in the cluster
",spark,apache,sandeep-katta,35216143,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22466,https://github.com/apache/spark/pull/22466,https://github.com/apache/spark/pull/22466.diff,https://github.com/apache/spark/pull/22466.patch
378,https://api.github.com/repos/apache/spark/issues/22448,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22448/labels{/name},https://api.github.com/repos/apache/spark/issues/22448/comments,https://api.github.com/repos/apache/spark/issues/22448/events,https://github.com/apache/spark/pull/22448,361149596,MDExOlB1bGxSZXF1ZXN0MjE2MTkzMTk0,22448,[SPARK-25417][SQL] Improve findTightestCommonType to coerce Integral and decimal types,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2018-09-18T06:12:10Z,2019-09-16T19:11:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently `findTightestCommonType` is not able to coerce between integral and decimal types properly. For example, while trying to find a common type between (int, decimal) , it is able to find a common type only when the number of digits to the left of decimal point of the decimal number is >= 10. This PR enhances the logic to to correctly find a wider decimal type between the integral and decimal types.

Here are some examples of the result of `findTightestCommonType`
```
int, decimal(3, 2) => decimal(12, 2)
int, decimal(4, 3) => decimal(13, 3)
int, decimal(11, 3) => decimal(14, 3)
int, decimal(38, 18) => decimal(38, 18)
int, decimal(38, 29) => None 
```

## How was this patch tested?
Added tests to TypeCoercionSuite.",spark,apache,dilipbiswal,14225158,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22448,https://github.com/apache/spark/pull/22448,https://github.com/apache/spark/pull/22448.diff,https://github.com/apache/spark/pull/22448.patch
379,https://api.github.com/repos/apache/spark/issues/22424,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22424/labels{/name},https://api.github.com/repos/apache/spark/issues/22424/comments,https://api.github.com/repos/apache/spark/issues/22424/events,https://github.com/apache/spark/pull/22424,360484501,MDExOlB1bGxSZXF1ZXN0MjE1NzM0Mjgw,22424,"[SPARK-25303][STREAMING] For checkpointed DStreams, remove the parent‚Ä¶","[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2018-09-14T23:40:17Z,2019-09-16T18:21:52Z,,NONE,"‚Ä¶RemeberDuration - this will be recalculated to a minimum value

        ## How was this patch tested?


## What changes were proposed in this pull request?

When a DStream gets checkpointed, there is no need to remember
        parentRDDs (unless indicated by other DStreams from that parent).
        This change sets the parentRememberDuration to null for checkpointed
        DStreams. Please note that this does get recalculated during validation
        to a minimum value in the parent as expected for any DStream as usual.
        Before this change, even after the fix for SPARK-25302 that cut the
        lineage to the parent, the parentDStreams and all the ones before that
        were being remembered for long durations. This was unnecessary and
        resulted in input RDDs being persisted for much longer than needed.

        Please see post below for original issue and a reply to it about
        resolution

        http://apache-spark-user-list.1001560.n3.nabble.com/DStream-reduceByKeyAndWindow-not-using-checkpointed-data-for-inverse-reducing-old-data-td33332.html

        A separate patch is being created for the issue in SPARK-25302

## How was this patch tested?

        Running the existing Unit tests.

## What improvement does this patch make?

        When combined with the fix in SPARK-25302,
        unpersits the intermediate RDDs and Input DStreams that
        were being remembered for too long, resulting in much lower memory usage
        on the Executors after the fixes were applied. Observed from the DAG chart differences and
        data from the Storage tab on the Driver UI.

",spark,apache,nikunjb,5704596,MDQ6VXNlcjU3MDQ1OTY=,https://avatars1.githubusercontent.com/u/5704596?v=4,,https://api.github.com/users/nikunjb,https://github.com/nikunjb,https://api.github.com/users/nikunjb/followers,https://api.github.com/users/nikunjb/following{/other_user},https://api.github.com/users/nikunjb/gists{/gist_id},https://api.github.com/users/nikunjb/starred{/owner}{/repo},https://api.github.com/users/nikunjb/subscriptions,https://api.github.com/users/nikunjb/orgs,https://api.github.com/users/nikunjb/repos,https://api.github.com/users/nikunjb/events{/privacy},https://api.github.com/users/nikunjb/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22424,https://github.com/apache/spark/pull/22424,https://github.com/apache/spark/pull/22424.diff,https://github.com/apache/spark/pull/22424.patch
380,https://api.github.com/repos/apache/spark/issues/22423,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22423/labels{/name},https://api.github.com/repos/apache/spark/issues/22423/comments,https://api.github.com/repos/apache/spark/issues/22423/events,https://github.com/apache/spark/pull/22423,360484093,MDExOlB1bGxSZXF1ZXN0MjE1NzMzOTcx,22423,[SPARK-25302][STREAMING] Checkpoint the reducedStream in ReducedWindo‚Ä¶,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-09-14T23:37:11Z,2019-09-16T18:21:50Z,,NONE,"‚Ä¶wDStream so as to cut the lineage completely to parent

  ## What changes were proposed in this pull request?

  Dstream.reduceByKeyAndWindow() with inverse reduce functions eventually creates
  a ReducedWindowDStream but did not checkpoint it.
  When combined with the issue described in SPARK-25303,
  it results in the problem described in the post here:

  http://apache-spark-user-list.1001560.n3.nabble.com/DStream-reduceByKeyAndWindow-not-using-checkpointed-data-for-inverse-reducing-old-data-td33332.html

  This change will checkpoint reducedStream inside to cut the lineage.
  A separate patch is being created for the issue in SPARK-25303

  ## How was this patch tested?

  Running the existing Unit tests. A couple of existing unit test classes were
  written assuming they will not be used in serialization. Had to declare
  SparkContext as transient like other tests in order to make them work
  due to new checkpointing causing serialization to occur.

  ## What improvement does this patch make?

  Cuts the lineage to the parent DStream. When combined with the fix
  in SPARK-25303, unpersits the intermediate RDDs and Input DStreams that
  were being remembered for too long, resulting in much lower memory usage
  on the Executors after the fixes. Observed from the DAG chart differences and
  data from the Storage tab on the Driver UI.

",spark,apache,nikunjb,5704596,MDQ6VXNlcjU3MDQ1OTY=,https://avatars1.githubusercontent.com/u/5704596?v=4,,https://api.github.com/users/nikunjb,https://github.com/nikunjb,https://api.github.com/users/nikunjb/followers,https://api.github.com/users/nikunjb/following{/other_user},https://api.github.com/users/nikunjb/gists{/gist_id},https://api.github.com/users/nikunjb/starred{/owner}{/repo},https://api.github.com/users/nikunjb/subscriptions,https://api.github.com/users/nikunjb/orgs,https://api.github.com/users/nikunjb/repos,https://api.github.com/users/nikunjb/events{/privacy},https://api.github.com/users/nikunjb/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22423,https://github.com/apache/spark/pull/22423,https://github.com/apache/spark/pull/22423.diff,https://github.com/apache/spark/pull/22423.patch
381,https://api.github.com/repos/apache/spark/issues/22412,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22412/labels{/name},https://api.github.com/repos/apache/spark/issues/22412/comments,https://api.github.com/repos/apache/spark/issues/22412/events,https://github.com/apache/spark/pull/22412,359861241,MDExOlB1bGxSZXF1ZXN0MjE1MjU1NTAx,22412,[SPARK-25404][SQL] Staging path may not on the expected place when table path contains the stagingDir string,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2018-09-13T11:48:59Z,2019-09-16T18:21:48Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
As described in [#SPARK-25404](https://issues.apache.org/jira/browse/SPARK-25404),  staging path may not on the right place we expect. I'm not quiet sure  in which case the `inputPathName` contains the `stagingDir`, but it seems `new Path(inputPathName, stagingDir).toString`  is enough.
```scala
    var stagingPathName: String =
      if (inputPathName.indexOf(stagingDir) == -1) {
        new Path(inputPathName, stagingDir).toString
      } else {
        inputPathName.substring(0, inputPathName.indexOf(stagingDir) + stagingDir.length)
      }
```

## How was this patch tested?
Manually test with debug mode, and check the staging files on right path.

",spark,apache,fjh100456,26785576,MDQ6VXNlcjI2Nzg1NTc2,https://avatars2.githubusercontent.com/u/26785576?v=4,,https://api.github.com/users/fjh100456,https://github.com/fjh100456,https://api.github.com/users/fjh100456/followers,https://api.github.com/users/fjh100456/following{/other_user},https://api.github.com/users/fjh100456/gists{/gist_id},https://api.github.com/users/fjh100456/starred{/owner}{/repo},https://api.github.com/users/fjh100456/subscriptions,https://api.github.com/users/fjh100456/orgs,https://api.github.com/users/fjh100456/repos,https://api.github.com/users/fjh100456/events{/privacy},https://api.github.com/users/fjh100456/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22412,https://github.com/apache/spark/pull/22412,https://github.com/apache/spark/pull/22412.diff,https://github.com/apache/spark/pull/22412.patch
382,https://api.github.com/repos/apache/spark/issues/22411,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22411/labels{/name},https://api.github.com/repos/apache/spark/issues/22411/comments,https://api.github.com/repos/apache/spark/issues/22411/events,https://github.com/apache/spark/pull/22411,359848035,MDExOlB1bGxSZXF1ZXN0MjE1MjQ0OTkz,22411,[SPARK-25421][SQL] Abstract an output path field in trait DataWritingCommand,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2018-09-13T11:06:18Z,2019-09-16T18:21:46Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

#22353 import a metadata field in ```SparkPlanInfo``` and it could dump the input location for read. Corresponding, we need add a field in ```DataWritingCommand``` for output path.

## How was this patch tested?

Unit test
",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22411,https://github.com/apache/spark/pull/22411,https://github.com/apache/spark/pull/22411.diff,https://github.com/apache/spark/pull/22411.patch
383,https://api.github.com/repos/apache/spark/issues/22318,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22318/labels{/name},https://api.github.com/repos/apache/spark/issues/22318/comments,https://api.github.com/repos/apache/spark/issues/22318/events,https://github.com/apache/spark/pull/22318,356336607,MDExOlB1bGxSZXF1ZXN0MjEyNjI1ODMw,22318,[SPARK-25150][SQL] Rewrite condition when deduplicate Join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2018-09-03T02:06:47Z,2019-09-16T19:11:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

ResolveReferences rule modifies (deduplicates) conflicting AttributeReferences in left and right side of a Join. It changes the right side to avoid conflicts but it forgot to change the references in the condition of the join. Solution is to rewrite the condition too.

Details:

With this PR, if any of the resolved attributes in a join condition refer to an attribute that is replaced during the deduplication (`dedupRight()`) then the join condition is modified to reflect the change.
The modification is done regardless whether the reference is on the left or the right side of the condition.

This PR has no effect on unresolved attributes. Those attributes are resolved later by different rules. 

This PR helps in those cases where same origin dataframes (`b`, `c`) are joined to a different origin one (`a`): 
```
val a = spark.range(1, 5)
val b = spark.range(10)
val c = b.filter($""id"" % 2 === 0)
val r = a.join(b, a(""id"") === b(""id""), ""inner"").join(c, a(""id"") === c(""id""), ""inner"")
```
Here the result of `a join b` contains an `id` attribute coming from `b`. That attribute conflicts with the `id` attribute of `c` in the second join so `c(""id"")` is deduplicated to let's say `X` and the join condition `a(""id"") === c(""id"")` needs to be modified to `a(""id"") === X` to get correct results.

I believe it is also worth explaining some situations where this PR does't help (but also does no harm). In cases where same origin dataframes are joined to each other:
```
val b = spark.range(10)
val c = b.filter($""id"" % 2 === 0)
val r = b.join(c, b(""id"") === c(""id""), ""inner"")
```
In this latter case the deduplication is applied as both `b` and `c` have an `id` attribute (that actually refer to the same resolved attribute (have the same `ExprId`)). Deduplication results `c(""id"")` to be changed to let's say `X`. So in this case both `b(""id"")` and `c(""id"")` in the join condition are modified to `X` by the changes introduced in this PR. The reason why the result will be correct with this PR (and is correct without this PR) is that the current implementation of Spark contains a ""hack"" in `Dataframe` that fixes `EqualTo` and `EqualNullSafe` type of conditions with identical references on both sides. The hack rewrites these join conditions by re-resolving the attributes with the name of the attribute on both the left and right sides.

Please note that until a resolved attribute doesn't contain some kind of reference to it's dataframe not all  cases of join can be handled properly. Such an initiative can be found in https://github.com/apache/spark/pull/21449.

## How was this patch tested?

Added unit test.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22318,https://github.com/apache/spark/pull/22318,https://github.com/apache/spark/pull/22318.diff,https://github.com/apache/spark/pull/22318.patch
384,https://api.github.com/repos/apache/spark/issues/22309,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22309/labels{/name},https://api.github.com/repos/apache/spark/issues/22309/comments,https://api.github.com/repos/apache/spark/issues/22309/events,https://github.com/apache/spark/pull/22309,356212289,MDExOlB1bGxSZXF1ZXN0MjEyNTUyMzUw,22309,[SPARK-20384][SQL] Support value class in schema of Dataset,"[{'id': 1406605297, 'node_id': 'MDU6TGFiZWwxNDA2NjA1Mjk3', 'url': 'https://api.github.com/repos/apache/spark/labels/OPTIMIZER', 'name': 'OPTIMIZER', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2018-09-01T15:47:20Z,2019-09-16T18:21:45Z,,NONE,"## What changes were proposed in this pull request?
This PR adds support for [Scala value class][1] in schema of Datasets (as both top level class and nested field). 
The idea is to treat  value class as its underlying type at run time. For example:
```scala
case class Id(get: Int) extends AnyVal
case class User(id: Id) // field `id` will be treated as Int
```
However, if the value class is top-level (e.g. `Dataset[Id]`) then it must be treated like a boxed type and must be instantiated. I'm not sure why it behaves this way but I suspect it is related to the [expansion of value class][2] when we do casting (e.g. `asInstanceOf[T]`)

Actually, this feature is addressed before in [SPARK-17368][3] but the patch only supports top-level case. Hence we see the error when value class is nested as in [SPARK-19741][4] and [SPARK-20384][5]

[1]: https://docs.scala-lang.org/sips/value-classes.html
[2]: https://docs.scala-lang.org/sips/value-classes.html#example-1
[3]: https://issues.apache.org/jira/browse/SPARK-17368
[4]: https://issues.apache.org/jira/browse/SPARK-19741
[5]: https://issues.apache.org/jira/browse/SPARK-20384

## How was this patch tested?
I added unit tests for top-level and nested case.
",spark,apache,mt40,8610576,MDQ6VXNlcjg2MTA1NzY=,https://avatars1.githubusercontent.com/u/8610576?v=4,,https://api.github.com/users/mt40,https://github.com/mt40,https://api.github.com/users/mt40/followers,https://api.github.com/users/mt40/following{/other_user},https://api.github.com/users/mt40/gists{/gist_id},https://api.github.com/users/mt40/starred{/owner}{/repo},https://api.github.com/users/mt40/subscriptions,https://api.github.com/users/mt40/orgs,https://api.github.com/users/mt40/repos,https://api.github.com/users/mt40/events{/privacy},https://api.github.com/users/mt40/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22309,https://github.com/apache/spark/pull/22309,https://github.com/apache/spark/pull/22309.diff,https://github.com/apache/spark/pull/22309.patch
385,https://api.github.com/repos/apache/spark/issues/22277,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22277/labels{/name},https://api.github.com/repos/apache/spark/issues/22277/comments,https://api.github.com/repos/apache/spark/issues/22277/events,https://github.com/apache/spark/pull/22277,355413097,MDExOlB1bGxSZXF1ZXN0MjExOTQ3MTg4,22277,[SPARK-25276] OutOfMemoryError: GC overhead limit exceeded when using alias,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2018-08-30T03:44:57Z,2019-09-16T18:21:43Z,,CONTRIBUTOR,"### Test sql :  
[test.txt](https://github.com/apache/spark/files/2365330/test.txt)



### Output :
```
spark-2.3.1-bin-hadoop2.7/bin # ./spark-sql -f test.txt
Time taken: 3.405 seconds
Time taken: 0.373 seconds
Time taken: 0.202 seconds
Time taken: 0.024 seconds
18/09/06 11:29:49 WARN HiveMetaStore: Location: file:/user/hive/warehouse/table11 specified for non-external table:table11
Time taken: 0.541 seconds
18/09/06 11:29:49 WARN HiveMetaStore: Location: file:/user/hive/warehouse/table22 specified for non-external table:table22
Time taken: 0.115 seconds
18/09/06 11:29:50 WARN HiveMetaStore: Location: file:/user/hive/warehouse/table33 specified for non-external table:table33
Time taken: 6.075 seconds
18/09/06 11:31:38 ERROR SparkSQLDriver: Failed in [
create table table44 as
select a.*
from
(
select
(concat(
case when a1 is null then '' else cast(a1 as string) end,'|~|',
case when a2 is null then '' else cast(a2 as string) end,'|~|',
case when a3 is null then '' else cast(a3 as string) end,'|~|',
case when a4 is null then '' else cast(a4 as string) end,'|~|',
case when a5 is null then '' else cast(a5 as string) end,'|~|',
case when a6 is null then '' else cast(a6 as string) end,'|~|',
case when a7 is null then '' else cast(a7 as string) end,'|~|',
case when a8 is null then '' else cast(a8 as string) end,'|~|',
case when a9 is null then '' else cast(a9 as string) end,'|~|',
case when a10 is null then '' else cast(a10 as string) end,'|~|',
case when a11 is null then '' else cast(a11 as string) end,'|~|',
case when a12 is null then '' else cast(a12 as string) end,'|~|',
case when a13 is null then '' else cast(a13 as string) end,'|~|',
case when a14 is null then '' else cast(a14 as string) end,'|~|',
case when a15 is null then '' else cast(a15 as string) end,'|~|',
case when a16 is null then '' else cast(a16 as string) end,'|~|',
case when a17 is null then '' else cast(a17 as string) end,'|~|',
case when a18 is null then '' else cast(a18 as string) end,'|~|',
case when a19 is null then '' else cast(a19 as string) end
)) as KEY_ID ,
case when a1 is null then '' else cast(a1 as string) end as a1,
case when a2 is null then '' else cast(a2 as string) end as a2,
case when a3 is null then '' else cast(a3 as string) end as a3,
case when a4 is null then '' else cast(a4 as string) end as a4,
case when a5 is null then '' else cast(a5 as string) end as a5,
case when a6 is null then '' else cast(a6 as string) end as a6,
case when a7 is null then '' else cast(a7 as string) end as a7,
case when a8 is null then '' else cast(a8 as string) end as a8,
case when a9 is null then '' else cast(a9 as string) end as a9,
case when a10 is null then '' else cast(a10 as string) end as a10,
case when a11 is null then '' else cast(a11 as string) end as a11,
case when a12 is null then '' else cast(a12 as string) end as a12,
case when a13 is null then '' else cast(a13 as string) end as a13,
case when a14 is null then '' else cast(a14 as string) end as a14,
case when a15 is null then '' else cast(a15 as string) end as a15,
case when a16 is null then '' else cast(a16 as string) end as a16,
case when a17 is null then '' else cast(a17 as string) end as a17,
case when a18 is null then '' else cast(a18 as string) end as a18,
case when a19 is null then '' else cast(a19 as string) end as a19
from table22
) A
left join table11 B ON A.KEY_ID = B.KEY_ID
where b.KEY_ID is null]
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.lang.Class.copyConstructors(Class.java:3130)
        at java.lang.Class.getConstructors(Class.java:1651)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:387)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:385)
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
        at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:385)
        at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:244)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:190)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:188)
        at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:189)
        at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:189)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:189)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:188)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.add(ExpressionSet.scala:63)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet$$anonfun$$plus$plus$1.apply(ExpressionSet.scala:79)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet$$anonfun$$plus$plus$1.apply(ExpressionSet.scala:79)
        at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:316)
        at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
        at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
        at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
        at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus$plus(ExpressionSet.scala:79)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus$plus(ExpressionSet.scala:55)
        at org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$anonfun$getAliasedConstraints$1.apply(LogicalPlan.scala:254)
        at org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$anonfun$getAliasedConstraints$1.apply(LogicalPlan.scala:249)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at org.apache.spark.sql.catalyst.plans.logical.UnaryNode.getAliasedConstraints(LogicalPlan.scala:249)
```

Attaching a test to reproduce the issue. The issue seems to be with the redundant constrains, Below is a test which explains it. 

  test(""redundant constrains"") {
    val tr = LocalRelation('a.int, 'b.string, 'c.int)
    val aliasedRelation = tr.where('a.attr > 10).select('a.as('x), 'b, 'b.as('y), 'a.as('z))

    verifyConstraints(aliasedRelation.analyze.constraints,
      ExpressionSet(Seq(resolveColumn(aliasedRelation.analyze, ""x"") > 10,
        IsNotNull(resolveColumn(aliasedRelation.analyze, ""x"")),
        resolveColumn(aliasedRelation.analyze, ""b"") <=> resolveColumn(aliasedRelation.analyze, ""y""),
        resolveColumn(aliasedRelation.analyze, ""z"") <=>
          resolveColumn(aliasedRelation.analyze, ""x""))))
  }

== FAIL: Constraints do not match ===
Found: isnotnull(z#5),(z#5 > 10),(x#3 > 10),(z#5 <=> x#3),(b#1 <=> y#4),isnotnull(x#3)
Expected: (x#3 > 10),isnotnull(x#3),(b#1 <=> y#4),(z#5 <=> x#3)
== Result ==
Missing: N/A
Found but not expected: isnotnull(z#5),(z#5 > 10)
Here i think as z has a EqualNullSafe comparison with x, so having isnotnull(z#5),(z#5 > 10) is redundant. If a query has lot of aliases, this may cause overhead leading to __java.lang.OutOfMemoryError: GC overhead limit exceeded__.

So i suggest https://github.com/apache/spark/blob/v2.3.2-rc5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala#L254 instead of  addAll++= we must just assign =",spark,apache,ajithme,22072336,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22277,https://github.com/apache/spark/pull/22277,https://github.com/apache/spark/pull/22277.diff,https://github.com/apache/spark/pull/22277.patch
386,https://api.github.com/repos/apache/spark/issues/22219,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22219/labels{/name},https://api.github.com/repos/apache/spark/issues/22219/comments,https://api.github.com/repos/apache/spark/issues/22219/events,https://github.com/apache/spark/pull/22219,353762969,MDExOlB1bGxSZXF1ZXN0MjEwNzI1NzM3,22219,[SPARK-25224][SQL] Improvement of Spark SQL ThriftServer memory management,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2018-08-24T12:25:47Z,2019-11-18T20:38:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Spark SQL only have two options for managing thriftserver memory - enable spark.sql.thriftServer.incrementalCollect or not

### The case of enabling spark.sql.thriftServer.incrementalCollects
   - Pros
        - thriftserver can handle large output without OOM.
   - Cons
        - Performance degradation because of executing task partition by partition.
        - Handle queries with count-limit inefficiently because of executing all partitions.
        - Does not cache result for FETCH_FIRST

### The case of disabling spark.sql.thriftServer.incrementalCollects
   - Pros
        - Good performance for small output
   - Cons
        - Memory peak usage is too large because allocating decompressed & deserialized rows in ""batch"" manner, and OOM could occur for large output.
        - It is difficult to measure memory peak usage of Query, so configuring spark.driver.maxResultSize is very difficult.
        - If decompressed & deserialized rows fills up eden area of JVM Heap, they moves to old Gen and could increase possibility of ""Full GC"" that stops the world.
 
### The improvement idea of solving these problems is below.
   - DataSet does not decompress & deserialize result, and just return total row count & iterator to SQL-Executor. By doing that, only compressed data reside in memory, so that the memory usage is not only much lower than before but can be controlled with spark.driver.maxResultSize config.

   - After SQL-Executor get total row count & iterator from DataSet, SQL-Executor could decide whether deserializing them collectively or iteratively with considering returned row count.

## How was this patch tested?
Add test cases.
",spark,apache,Dooyoung-Hwang,42426020,MDQ6VXNlcjQyNDI2MDIw,https://avatars2.githubusercontent.com/u/42426020?v=4,,https://api.github.com/users/Dooyoung-Hwang,https://github.com/Dooyoung-Hwang,https://api.github.com/users/Dooyoung-Hwang/followers,https://api.github.com/users/Dooyoung-Hwang/following{/other_user},https://api.github.com/users/Dooyoung-Hwang/gists{/gist_id},https://api.github.com/users/Dooyoung-Hwang/starred{/owner}{/repo},https://api.github.com/users/Dooyoung-Hwang/subscriptions,https://api.github.com/users/Dooyoung-Hwang/orgs,https://api.github.com/users/Dooyoung-Hwang/repos,https://api.github.com/users/Dooyoung-Hwang/events{/privacy},https://api.github.com/users/Dooyoung-Hwang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22219,https://github.com/apache/spark/pull/22219,https://github.com/apache/spark/pull/22219.diff,https://github.com/apache/spark/pull/22219.patch
387,https://api.github.com/repos/apache/spark/issues/22202,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22202/labels{/name},https://api.github.com/repos/apache/spark/issues/22202/comments,https://api.github.com/repos/apache/spark/issues/22202/events,https://github.com/apache/spark/pull/22202,353387300,MDExOlB1bGxSZXF1ZXN0MjEwNDUyODc1,22202,[SPARK-25211][Core] speculation and fetch failed result in hang of job,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-08-23T13:43:16Z,2019-09-16T18:21:38Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

In current `DAGScheduler.handleTaskCompletion` code, when a shuffleMapStage with job not in runningStages and its `pendingPartitions` is empty, the job of this shuffleMapStage will never complete.

*Think about below*

1. Stage 0 runs and generates shuffle output data.

2. Stage 1 reads the output from stage 0 and generates more shuffle data. It has two tasks with the same partition: ShuffleMapTask0 and ShuffleMapTask0.1(speculation).

3. ShuffleMapTask0 fails to fetch blocks and sends a FetchFailed to the driver. The driver resubmits stage 0 and stage 1. The driver will place stage 0 in runningStages and place stage 1 in waitingStages.

4. ShuffleMapTask0.1 successfully finishes and sends Success back to driver. The driver will add the mapstatus to the set of output locations of stage 1. because of stage 1 not in runningStages, the job will not complete.

5. stage 0 completes and the driver will run stage 1. But, because the output sets of stage 1 is complete, the drive will not submit any tasks and make stage 1 complte right now. Because the job complete relay on the `CompletionEvent` and there will never a `CompletionEvent` come, the job will hang.

## How was this patch tested?

UT",spark,apache,liutang123,17537020,MDQ6VXNlcjE3NTM3MDIw,https://avatars3.githubusercontent.com/u/17537020?v=4,,https://api.github.com/users/liutang123,https://github.com/liutang123,https://api.github.com/users/liutang123/followers,https://api.github.com/users/liutang123/following{/other_user},https://api.github.com/users/liutang123/gists{/gist_id},https://api.github.com/users/liutang123/starred{/owner}{/repo},https://api.github.com/users/liutang123/subscriptions,https://api.github.com/users/liutang123/orgs,https://api.github.com/users/liutang123/repos,https://api.github.com/users/liutang123/events{/privacy},https://api.github.com/users/liutang123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22202,https://github.com/apache/spark/pull/22202,https://github.com/apache/spark/pull/22202.diff,https://github.com/apache/spark/pull/22202.patch
388,https://api.github.com/repos/apache/spark/issues/22198,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22198/labels{/name},https://api.github.com/repos/apache/spark/issues/22198/comments,https://api.github.com/repos/apache/spark/issues/22198/events,https://github.com/apache/spark/pull/22198,353308086,MDExOlB1bGxSZXF1ZXN0MjEwMzkyMTk4,22198,[SPARK-25121][SQL] Supports multi-part table names for broadcast hint resolution,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,57,2018-08-23T09:53:15Z,2019-12-06T21:42:17Z,,MEMBER,"## What changes were proposed in this pull request?
This pr fixed code to respect a database name for broadcast table hint resolution.
Currently, spark ignores a database name in multi-part names;
```
scala> sql(""CREATE DATABASE testDb"")
scala> spark.range(10).write.saveAsTable(""testDb.t"")

// without this patch
scala> spark.range(10).join(spark.table(""testDb.t""), ""id"").hint(""broadcast"", ""testDb.t"").explain
== Physical Plan ==
*(2) Project [id#24L]
+- *(2) BroadcastHashJoin [id#24L], [id#26L], Inner, BuildLeft
   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))
   :  +- *(1) Range (0, 10, step=1, splits=4)
   +- *(2) Project [id#26L]
      +- *(2) Filter isnotnull(id#26L)
         +- *(2) FileScan parquet testdb.t[id#26L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/maropu/Repositories/spark/spark-2.3.1-bin-hadoop2.7/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>

// with this patch
scala> spark.range(10).join(spark.table(""testDb.t""), ""id"").hint(""broadcast"", ""testDb.t"").explain
== Physical Plan ==
*(2) Project [id#3L]
+- *(2) BroadcastHashJoin [id#3L], [id#5L], Inner, BuildRight
   :- *(2) Range (0, 10, step=1, splits=4)
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))
      +- *(1) Project [id#5L]
         +- *(1) Filter isnotnull(id#5L)
            +- *(1) FileScan parquet testdb.t[id#5L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/maropu/Repositories/spark/spark-master/spark-warehouse/testdb.db/t], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>
```

## How was this patch tested?
Added tests in `DataFrameJoinSuite`.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22198,https://github.com/apache/spark/pull/22198,https://github.com/apache/spark/pull/22198.diff,https://github.com/apache/spark/pull/22198.patch
389,https://api.github.com/repos/apache/spark/issues/22165,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22165/labels{/name},https://api.github.com/repos/apache/spark/issues/22165/comments,https://api.github.com/repos/apache/spark/issues/22165/events,https://github.com/apache/spark/pull/22165,352390413,MDExOlB1bGxSZXF1ZXN0MjA5NzExOTQ1,22165,[SPARK-25017][Core] Add test suite for ContextBarrierState,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,30,2018-08-21T05:27:17Z,2019-09-16T19:12:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently `ContextBarrierState` is only covered by end-to-end test in `BarrierTaskContextSuite`, add BarrierCoordinatorSuite to test both classes.

## How was this patch tested?

UT newly added in ContextBarrierStateSuite.
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22165,https://github.com/apache/spark/pull/22165,https://github.com/apache/spark/pull/22165.diff,https://github.com/apache/spark/pull/22165.patch
390,https://api.github.com/repos/apache/spark/issues/22163,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22163/labels{/name},https://api.github.com/repos/apache/spark/issues/22163/comments,https://api.github.com/repos/apache/spark/issues/22163/events,https://github.com/apache/spark/pull/22163,352365860,MDExOlB1bGxSZXF1ZXN0MjA5Njk0NTAz,22163,[SPARK-25166][CORE]Reduce the number of write operations for shuffle write.,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2018-08-21T02:51:04Z,2019-11-18T07:05:51Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, only one record is written to a buffer each time, which increases the number of copies.
I think we should write as many records as possible each time.

## How was this patch tested?
Existed  unit tests in `UnsafeShuffleWriterSuite`
",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22163,https://github.com/apache/spark/pull/22163,https://github.com/apache/spark/pull/22163.diff,https://github.com/apache/spark/pull/22163.patch
391,https://api.github.com/repos/apache/spark/issues/22143,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22143/labels{/name},https://api.github.com/repos/apache/spark/issues/22143/comments,https://api.github.com/repos/apache/spark/issues/22143/events,https://github.com/apache/spark/pull/22143,351922090,MDExOlB1bGxSZXF1ZXN0MjA5MzcxOTI4,22143,[SPARK-24647][SS] Report KafkaStreamWriter's written min and max offs‚Ä¶,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2018-08-19T18:03:49Z,2019-11-20T08:31:07Z,,CONTRIBUTOR,"‚Ä¶ets via CustomMetrics.

## What changes were proposed in this pull request?

Report KafkaStreamWriter's written min and max offsets via CustomMetrics. This is important for data lineage projects like Spline. Related issue: https://issues.apache.org/jira/browse/SPARK-24647

To be able to track data lineage for Structured Streaming (I intend to implement this to Open Source Project Spline), the monitoring needs to be able to not only to track where the data was read from but also where results were written to. This could be to my knowledge best implemented using monitoring StreamingQueryProgress. However currently written data offsets are not available on Sink or StreamWriter interface. Implementing as proposed would also bring symmetry to StreamingQueryProgress fields sources and sink.

## How was this patch tested?

Unit tests.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,vackosar,11373934,MDQ6VXNlcjExMzczOTM0,https://avatars3.githubusercontent.com/u/11373934?v=4,,https://api.github.com/users/vackosar,https://github.com/vackosar,https://api.github.com/users/vackosar/followers,https://api.github.com/users/vackosar/following{/other_user},https://api.github.com/users/vackosar/gists{/gist_id},https://api.github.com/users/vackosar/starred{/owner}{/repo},https://api.github.com/users/vackosar/subscriptions,https://api.github.com/users/vackosar/orgs,https://api.github.com/users/vackosar/repos,https://api.github.com/users/vackosar/events{/privacy},https://api.github.com/users/vackosar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22143,https://github.com/apache/spark/pull/22143,https://github.com/apache/spark/pull/22143.diff,https://github.com/apache/spark/pull/22143.patch
392,https://api.github.com/repos/apache/spark/issues/22141,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22141/labels{/name},https://api.github.com/repos/apache/spark/issues/22141/comments,https://api.github.com/repos/apache/spark/issues/22141/events,https://github.com/apache/spark/pull/22141,351857161,MDExOlB1bGxSZXF1ZXN0MjA5MzMzMzQ0,22141,[SPARK-25154][SQL] Support NOT IN sub-queries inside nested OR conditions.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2018-08-18T22:30:51Z,2019-09-16T19:13:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently NOT IN subqueries (predicated null aware subquery) are not allowed inside OR expressions. We currently catch this condition in checkAnalysis and throw an error.

This PR enhances the subquery rewrite to support this type of queries.

Query
```SQL
SELECT * FROM s1 WHERE a > 5 or b NOT IN (SELECT c FROM s2);
```
Optimized Plan
```SQL
== Optimized Logical Plan ==
Project [a#3, b#4]
+- Filter ((a#3 > 5) || NOT exists#7)
   +- Join ExistenceJoin(exists#7), ((b#4 = c#5) || isnull((b#4 = c#5)))
      :- HiveTableRelation `default`.`s1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#3, b#4]
      +- Project [c#5]
         +- HiveTableRelation `default`.`s2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [c#5, d#6]


```
## How was this patch tested?
Added new testsin SQLQueryTestSuite, RewriteSubquerySuite and SubquerySuite.

Output from DB2 as a reference:

[nested-not-db2.txt](https://github.com/apache/spark/files/2299945/nested-not-db2.txt)
",spark,apache,dilipbiswal,14225158,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22141,https://github.com/apache/spark/pull/22141,https://github.com/apache/spark/pull/22141.diff,https://github.com/apache/spark/pull/22141.patch
393,https://api.github.com/repos/apache/spark/issues/22088,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22088/labels{/name},https://api.github.com/repos/apache/spark/issues/22088/comments,https://api.github.com/repos/apache/spark/issues/22088/events,https://github.com/apache/spark/pull/22088,349930165,MDExOlB1bGxSZXF1ZXN0MjA3OTAxMzk2,22088,[SPARK-24931][CORE]CoarseGrainedExecutorBackend send wrong 'Reason' w‚Ä¶,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-08-13T08:18:50Z,2019-09-16T18:24:06Z,,NONE,"## What changes were proposed in this pull request?

When CoarseGrainedExecutorBackend find the executor not available, it will send a ""RemoveExecutor"" message of ""ExecutorExited"" instead ""ExecutorLossReason"". So it call tell driver whether is the executor ""exitCausedByApp"" which should be false. So when dirver(TaskSetManager) can ""handleFailedTask"" correctly to avoid task failed time up to the ""maxTaskFailures"" and finally cause job failed.


## How was this patch tested?

tested in my own cluster
",spark,apache,bingbai0912,8478238,MDQ6VXNlcjg0NzgyMzg=,https://avatars2.githubusercontent.com/u/8478238?v=4,,https://api.github.com/users/bingbai0912,https://github.com/bingbai0912,https://api.github.com/users/bingbai0912/followers,https://api.github.com/users/bingbai0912/following{/other_user},https://api.github.com/users/bingbai0912/gists{/gist_id},https://api.github.com/users/bingbai0912/starred{/owner}{/repo},https://api.github.com/users/bingbai0912/subscriptions,https://api.github.com/users/bingbai0912/orgs,https://api.github.com/users/bingbai0912/repos,https://api.github.com/users/bingbai0912/events{/privacy},https://api.github.com/users/bingbai0912/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22088,https://github.com/apache/spark/pull/22088,https://github.com/apache/spark/pull/22088.diff,https://github.com/apache/spark/pull/22088.patch
394,https://api.github.com/repos/apache/spark/issues/22054,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22054/labels{/name},https://api.github.com/repos/apache/spark/issues/22054/comments,https://api.github.com/repos/apache/spark/issues/22054/events,https://github.com/apache/spark/pull/22054,349068393,MDExOlB1bGxSZXF1ZXN0MjA3MjY5NDE2,22054, [SPARK-24703][SQL]: To add support to multiply CalendarInterval with Integral Type.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2018-08-09T10:25:32Z,2019-11-10T01:16:06Z,,NONE,"## What changes were proposed in this pull request?

        This change adds capability to multiply Calender interval.

        Earlier the multiplication was throwing exception as follow:
        spark.sql(""select  interval '1' day * 3"").show()

        org.apache.spark.sql.AnalysisException: cannot resolve '(interval 1 days * 3)' due to data type mismatch: differing types in '(interval 1 days) * 3' (int and calendarinterval).; line 1 pos 7;
        'Project [unresolvedalias((interval 1 days * 3) , None)]

        +- OneRowRelation

        at org.apache.spark.sql.catalyst.analysis.package.failAnalysis(package.scala:42)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis1433anonfun1433anonfun.applyOrElse(CheckAnalysis.scala:93)
        at

        but now, we have added this support.

        ## How was this patch tested?

        Added test case in CalendarIntervalSuite.java, ArithmeticExpressionSuite.scala and ExpressionTypeCheckingSuite.scala
        Also, tested by spark-shell by multiplying calendarinterval with Integral type.

scala> spark.sql(\""select interval '1' day\"").show()
+---------------+
|interval 1 days|
+---------------+
|interval 1 days|
+---------------+


scala> spark.sql(\""select interval '1' day * 3\"").show()
+---------------------+
|(interval 1 days * 3)|
+---------------------+
|      interval 3 days|
+---------------------+


scala> spark.sql(\""select 3 * interval '1' day * 3\"").show()
+---------------------------+
|((3 * interval 1 days) * 3)|
+---------------------------+
|       interval 1 weeks ...|
+---------------------------+


scala> spark.sql(""select 3 * interval '1' day * 3"").collect()
res7: Array[org.apache.spark.sql.Row] = Array([interval 1 weeks 2 days])

",spark,apache,priyankagargnitk,4745302,MDQ6VXNlcjQ3NDUzMDI=,https://avatars1.githubusercontent.com/u/4745302?v=4,,https://api.github.com/users/priyankagargnitk,https://github.com/priyankagargnitk,https://api.github.com/users/priyankagargnitk/followers,https://api.github.com/users/priyankagargnitk/following{/other_user},https://api.github.com/users/priyankagargnitk/gists{/gist_id},https://api.github.com/users/priyankagargnitk/starred{/owner}{/repo},https://api.github.com/users/priyankagargnitk/subscriptions,https://api.github.com/users/priyankagargnitk/orgs,https://api.github.com/users/priyankagargnitk/repos,https://api.github.com/users/priyankagargnitk/events{/privacy},https://api.github.com/users/priyankagargnitk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22054,https://github.com/apache/spark/pull/22054,https://github.com/apache/spark/pull/22054.diff,https://github.com/apache/spark/pull/22054.patch
395,https://api.github.com/repos/apache/spark/issues/22038,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22038/labels{/name},https://api.github.com/repos/apache/spark/issues/22038/comments,https://api.github.com/repos/apache/spark/issues/22038/events,https://github.com/apache/spark/pull/22038,348781827,MDExOlB1bGxSZXF1ZXN0MjA3MDUxMzQ5,22038,[SPARK-25056][SQL] Unify the InConversion and BinaryComparison behavior,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,30,2018-08-08T15:39:32Z,2019-12-04T02:26:15Z,,MEMBER,"## What changes were proposed in this pull request?
before this PR:
```scala
scala> val df = spark.range(4).toDF().selectExpr(""cast(id as decimal(9, 2)) as id"")
df: org.apache.spark.sql.DataFrame = [id: decimal(9,2)]

scala> df.filter(""id in('1', '3')"").show
+---+
| id|
+---+
+---+

scala> df.filter(""id = '1' or id ='3'"").show
+----+
|  id|
+----+
|1.00|
|3.00|
+----+
```
after this PR:
```scala
scala> val df = spark.range(4).toDF().selectExpr(""cast(id as decimal(9, 2)) as id"")
df: org.apache.spark.sql.DataFrame = [id: decimal(9,2)]

scala> df.filter(""id in('1', '3')"").show
+----+
|  id|
+----+
|1.00|
|3.00|
+----+

scala> df.filter(""id = '1' or id ='3'"").show
+----+
|  id|
+----+
|1.00|
|3.00|
+----+
```

This change is the same as [HIVE-20204](https://issues.apache.org/jira/browse/HIVE-20204).

Other database behavior:
**Teradata**:
![image](https://user-images.githubusercontent.com/5399861/44069251-778312cc-9fb0-11e8-8cf1-aa2e5f6b79d3.png)

**Oracle**:
![image](https://user-images.githubusercontent.com/5399861/44069285-95e2a1ec-9fb0-11e8-9266-c5b741cadea2.png)

**MySQL**:
![image](https://user-images.githubusercontent.com/5399861/44069455-72b73b14-9fb1-11e8-9d5c-9efcd9ce57b9.png)

**postgres**
![image](https://user-images.githubusercontent.com/5399861/44085135-316539ce-9feb-11e8-9966-495161a9e506.png)

**Hive-2.3.2**
![image](https://user-images.githubusercontent.com/5399861/44085463-39f19b9a-9fec-11e8-9cc4-ecb5dd3ed345.png)

**Hive** [**current master**](https://github.com/apache/hive/commit/1c8449cce2a961c6ca6ce38bef0d770f34221d4d)
![image](https://user-images.githubusercontent.com/5399861/44087788-86019128-9ff3-11e8-8e33-fe9a24a0361a.png)

**spark-sql**:
![image](https://user-images.githubusercontent.com/5399861/44069556-e95eb8b4-9fb1-11e8-8ad1-5c4c0582b5ae.png)

## How was this patch tested?

unit tests
",spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22038,https://github.com/apache/spark/pull/22038,https://github.com/apache/spark/pull/22038.diff,https://github.com/apache/spark/pull/22038.patch
396,https://api.github.com/repos/apache/spark/issues/22029,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22029/labels{/name},https://api.github.com/repos/apache/spark/issues/22029/comments,https://api.github.com/repos/apache/spark/issues/22029/events,https://github.com/apache/spark/pull/22029,348387911,MDExOlB1bGxSZXF1ZXN0MjA2NzQ5Nzgx,22029,[SPARK-24395][SQL] IN operator should return NULL when comparing struct with NULL fields,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,57,2018-08-07T16:04:56Z,2019-09-16T19:13:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Spark's IN operator behaves different from other RDBMS (Oracle, Postgres) when structs containing NULL fields are involved. In this case, Spark returns `false`, while other RDBMS return `NULL`. This is critical especially when there are NOT IN filters, as Spark doesn't filter rows containing NULLs in that scenario (instead other RDBMS do).

The PR proposes to change Spark's IN operator behavior in order to align with the behavior of other RDBMS and introduces a flag which can be used by users to switch back to the previous behavior.

In particular, the proposed change affects only the IN operator when the values are structs (ie. every value is a list of more than one element) and NULL are involved. The proposed behavior can be summarized as follows:
 - If the value before IN contains a NULL element, IN evaluates to `null`;
 - If the list of values contains a values with a NULL element, then: If the list contains also the value looked for, `true` is returned; otherwise, `null` is returned

Please notice that Hive and Presto, instead, behave as Spark **before this change** for this specific scenario. But they don't support IN with subqueries returning more than one value. So **before this change** Spark is anyway the only system where the IN operator behaves incoherently between IN + subquery and IN + literals.

Summarizing:

 - Spark is the only framework which behaves differently with the same data when IN contains literals and subquery;
 - Hive and Presto behave as Spark before the PR when literals are involved (but Presto throws exception when null is present in both sides...);
 - Oracle, Postgres behaves like Spark after the PR;
 - Hive and Presto don't support In with subqueries containing more than 1 output field.

The PR introduces also a the config flag `spark.sql.legacy.inOperator.falseForNullField` which can be used to turn on/off the new behavior.


## How was this patch tested?

added UTs
",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22029,https://github.com/apache/spark/pull/22029,https://github.com/apache/spark/pull/22029.diff,https://github.com/apache/spark/pull/22029.patch
397,https://api.github.com/repos/apache/spark/issues/22024,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/22024/labels{/name},https://api.github.com/repos/apache/spark/issues/22024/comments,https://api.github.com/repos/apache/spark/issues/22024/events,https://github.com/apache/spark/pull/22024,348311146,MDExOlB1bGxSZXF1ZXN0MjA2NjkwMTkw,22024,[SPARK-25034][CORE] Remove allocations in onBlockFetchSuccess,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2018-08-07T13:01:59Z,2019-09-16T18:20:15Z,,NONE,"This method is only transferring a ManagedBuffer to the caller,
so there is no reason why it should allocate 2 (!) intermediate data
buffers in order to do so.

In this commit I'm removing the conversion from any kind of managed buffer
besides FileSegment to a NioManagedBuffer.
However if you check the only calling method getRemoteBytes(), you will
see that here we either:
 - do a memory-map if we have a FileSegmentManagedBuffer
 - try again to call the nioByteBuffer() method otherwise

So in any case the conversion will occur later.

## What changes were proposed in this pull request?
Remove needless temporary allocations

## How was this patch tested?
Tested this change with a few jobs
",spark,apache,vincent-grosbois,6704932,MDQ6VXNlcjY3MDQ5MzI=,https://avatars2.githubusercontent.com/u/6704932?v=4,,https://api.github.com/users/vincent-grosbois,https://github.com/vincent-grosbois,https://api.github.com/users/vincent-grosbois/followers,https://api.github.com/users/vincent-grosbois/following{/other_user},https://api.github.com/users/vincent-grosbois/gists{/gist_id},https://api.github.com/users/vincent-grosbois/starred{/owner}{/repo},https://api.github.com/users/vincent-grosbois/subscriptions,https://api.github.com/users/vincent-grosbois/orgs,https://api.github.com/users/vincent-grosbois/repos,https://api.github.com/users/vincent-grosbois/events{/privacy},https://api.github.com/users/vincent-grosbois/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/22024,https://github.com/apache/spark/pull/22024,https://github.com/apache/spark/pull/22024.diff,https://github.com/apache/spark/pull/22024.patch
398,https://api.github.com/repos/apache/spark/issues/21985,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21985/labels{/name},https://api.github.com/repos/apache/spark/issues/21985/comments,https://api.github.com/repos/apache/spark/issues/21985/events,https://github.com/apache/spark/pull/21985,347400355,MDExOlB1bGxSZXF1ZXN0MjA2MDMzMjI2,21985,[SPARK-24884][SQL] add regexp_extract_all support,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2018-08-03T13:38:13Z,2019-09-27T07:05:40Z,,CONTRIBUTOR,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR tried to add `regexp_extract_all` sql function. It is supported in [presto](https://prestodb.io/docs/current/functions/regexp.html) and [pig](https://pig.apache.org/docs/latest/api/org/apache/pig/builtin/REGEX_EXTRACT_ALL.html)
Also it refactored regexp_extract since there were some duplicated codes.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Added `regexp_extract_all` function

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit test",spark,apache,xueyumusic,95261,MDQ6VXNlcjk1MjYx,https://avatars1.githubusercontent.com/u/95261?v=4,,https://api.github.com/users/xueyumusic,https://github.com/xueyumusic,https://api.github.com/users/xueyumusic/followers,https://api.github.com/users/xueyumusic/following{/other_user},https://api.github.com/users/xueyumusic/gists{/gist_id},https://api.github.com/users/xueyumusic/starred{/owner}{/repo},https://api.github.com/users/xueyumusic/subscriptions,https://api.github.com/users/xueyumusic/orgs,https://api.github.com/users/xueyumusic/repos,https://api.github.com/users/xueyumusic/events{/privacy},https://api.github.com/users/xueyumusic/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21985,https://github.com/apache/spark/pull/21985,https://github.com/apache/spark/pull/21985.diff,https://github.com/apache/spark/pull/21985.patch
399,https://api.github.com/repos/apache/spark/issues/21971,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21971/labels{/name},https://api.github.com/repos/apache/spark/issues/21971/comments,https://api.github.com/repos/apache/spark/issues/21971/events,https://github.com/apache/spark/pull/21971,347035374,MDExOlB1bGxSZXF1ZXN0MjA1NzUzMTMx,21971,[SPARK-24947] [Core] aggregateAsync and foldAsync,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2018-08-02T14:24:29Z,2019-09-16T18:23:50Z,,CONTRIBUTOR,"See the description in the [Jira ticket](https://issues.apache.org/jira/browse/SPARK-24947).

This contribution is my original work (inspired by similar methods in
the Spark code) and I license this work to Spark under the Apache
License 2.0.

## What changes were proposed in this pull request?

Add `aggregateAsync` and `foldAsync` methods to `AsyncRDDActions`.

## How was this patch tested?

Unit tests (included in PR).
",spark,apache,ceedubs,977929,MDQ6VXNlcjk3NzkyOQ==,https://avatars0.githubusercontent.com/u/977929?v=4,,https://api.github.com/users/ceedubs,https://github.com/ceedubs,https://api.github.com/users/ceedubs/followers,https://api.github.com/users/ceedubs/following{/other_user},https://api.github.com/users/ceedubs/gists{/gist_id},https://api.github.com/users/ceedubs/starred{/owner}{/repo},https://api.github.com/users/ceedubs/subscriptions,https://api.github.com/users/ceedubs/orgs,https://api.github.com/users/ceedubs/repos,https://api.github.com/users/ceedubs/events{/privacy},https://api.github.com/users/ceedubs/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21971,https://github.com/apache/spark/pull/21971,https://github.com/apache/spark/pull/21971.diff,https://github.com/apache/spark/pull/21971.patch
400,https://api.github.com/repos/apache/spark/issues/21957,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21957/labels{/name},https://api.github.com/repos/apache/spark/issues/21957/comments,https://api.github.com/repos/apache/spark/issues/21957/events,https://github.com/apache/spark/pull/21957,346857328,MDExOlB1bGxSZXF1ZXN0MjA1NjE3MDk2,21957,"[SPARK-24994][SQL] When the data type of the field is converted to other types, it can also support pushdown to parquet","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-08-02T04:43:55Z,2019-09-16T19:14:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
For this statement: select * from table1 where a = 100;
the data type of `a` is `smallint` , because the defaut data type of 100 is `int` ,so `a` is converted to `int`.
In this case, it does not support push down to parquet.

In our business, for our SQL statements, and we generally do not convert 100 to `smallint`, this pr can support push down to parquet for this situation.

## How was this patch tested?
added unit tests",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21957,https://github.com/apache/spark/pull/21957,https://github.com/apache/spark/pull/21957.diff,https://github.com/apache/spark/pull/21957.patch
401,https://api.github.com/repos/apache/spark/issues/21929,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21929/labels{/name},https://api.github.com/repos/apache/spark/issues/21929/comments,https://api.github.com/repos/apache/spark/issues/21929/events,https://github.com/apache/spark/pull/21929,346076911,MDExOlB1bGxSZXF1ZXN0MjA1MDI2OTI2,21929,[SPARK-24970][Kinesis] Create WriteAheadLogBackedBlockRDD for Kinesis Streaming if WAL is enabled.,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-07-31T07:59:24Z,2019-09-16T18:23:46Z,,NONE,"
## What changes were proposed in this pull request?

By default, KinesisInputDStream creates KinesisBackedBlockRDD. When an application tries to recover from streaming checkpoint, the RDD will access Kinesis directly to re-read data. As all partitions in the BlockRDD accesses Kinesis, and AWS Kinesis only supports 5 concurrency reads per shard per second, it will touch ProvisionedThroughputExceededException easily. And when the conflicts are heavy, the recover will be failed.

Mostly, when we use Spark streaming, we will enable WAL. Then we can recover data from WAL, instead of re-reading from Kinesis directly.

This PR tries to create WriteAheadLogBackedBlockRDD for Kinesis Streaming if WAL is enabled. 

## How was this patch tested?

In KinesisStreamSuite.scala, I will add a test case.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,brucezhao11,7369752,MDQ6VXNlcjczNjk3NTI=,https://avatars3.githubusercontent.com/u/7369752?v=4,,https://api.github.com/users/brucezhao11,https://github.com/brucezhao11,https://api.github.com/users/brucezhao11/followers,https://api.github.com/users/brucezhao11/following{/other_user},https://api.github.com/users/brucezhao11/gists{/gist_id},https://api.github.com/users/brucezhao11/starred{/owner}{/repo},https://api.github.com/users/brucezhao11/subscriptions,https://api.github.com/users/brucezhao11/orgs,https://api.github.com/users/brucezhao11/repos,https://api.github.com/users/brucezhao11/events{/privacy},https://api.github.com/users/brucezhao11/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21929,https://github.com/apache/spark/pull/21929,https://github.com/apache/spark/pull/21929.diff,https://github.com/apache/spark/pull/21929.patch
402,https://api.github.com/repos/apache/spark/issues/21917,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21917/labels{/name},https://api.github.com/repos/apache/spark/issues/21917/comments,https://api.github.com/repos/apache/spark/issues/21917/events,https://github.com/apache/spark/pull/21917,345797945,MDExOlB1bGxSZXF1ZXN0MjA0ODM1MDM0,21917,[SPARK-24720][STREAMING-KAFKA]  add option to align ranges with offset having records to support kafka transaction,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2018-07-30T15:04:33Z,2019-09-16T18:23:45Z,,NONE,"## What changes were proposed in this pull request?

Update with a better fix:
With this fix, the offsets are scanned to determine the ranges. We ensure that the when we determine the range [fromOffset, untilOffset), untilOffset is always an offset with an existing record that we've been able to fetch at least once.
This logic is applied as soon as  allowNonConsecutiveOffsets is enabled.
Since we scan all the record a first time, we use this to count the number of records. OffsetRange now contains the number of record in each partition and the rdd.count() is a free operation.

Isolation level of uncomitted read is unsafe: untilOffset might become ""empty"" if the transaction is abort just after the offset range creation. The same thing could happen if the ""untilOffset"" gets compacted (it's also a potential issue before this change)

## How was this patch tested?

Unit test for the offset scan. No integration test for transaction since the current kafka version doesn't support transactions. Tested against a custom streaming use-case.",spark,apache,QuentinAmbard,96781,MDQ6VXNlcjk2Nzgx,https://avatars1.githubusercontent.com/u/96781?v=4,,https://api.github.com/users/QuentinAmbard,https://github.com/QuentinAmbard,https://api.github.com/users/QuentinAmbard/followers,https://api.github.com/users/QuentinAmbard/following{/other_user},https://api.github.com/users/QuentinAmbard/gists{/gist_id},https://api.github.com/users/QuentinAmbard/starred{/owner}{/repo},https://api.github.com/users/QuentinAmbard/subscriptions,https://api.github.com/users/QuentinAmbard/orgs,https://api.github.com/users/QuentinAmbard/repos,https://api.github.com/users/QuentinAmbard/events{/privacy},https://api.github.com/users/QuentinAmbard/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21917,https://github.com/apache/spark/pull/21917,https://github.com/apache/spark/pull/21917.diff,https://github.com/apache/spark/pull/21917.patch
403,https://api.github.com/repos/apache/spark/issues/21888,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21888/labels{/name},https://api.github.com/repos/apache/spark/issues/21888/comments,https://api.github.com/repos/apache/spark/issues/21888/events,https://github.com/apache/spark/pull/21888,345024520,MDExOlB1bGxSZXF1ZXN0MjA0MjgxODYw,21888,[SPARK-24253][SQL][WIP] Implement DeleteFrom for v2 tables,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-07-26T22:16:17Z,2019-11-14T05:51:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This adds support for DELETE FROM in SQL using the new DeleteFrom logical plan and v2 DeleteSupport mix-in.

To identify the v2 table to delete data from, this uses the TableCatalog API introduced in #21306.

TableIdentifier has been updated with a new superclass, CatalogTableIdentifier. Code paths that don't support identifiers with a catalog continue to use TableIdentifier, allowing a smooth transition even though some code assumes there is no support for multiple catalogs.

UnresovledRelation now supports CatalogTableIdentifier, but resolution will only happen when the catalog is not defined or through a new rule with support for v2 TableCatalog. Existing uses of UnresolvedRelation access the catalog using tableIdentifier, which asserts that the catalog is not defined before returning to ensure catalog identifiers don't leak to code without catalog support.

WIP: This is based on #21306, #21305, and #21877 and includes the changes from those PRs.

## How was this patch tested?

WIP, will add tests.",spark,apache,rdblue,87915,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21888,https://github.com/apache/spark/pull/21888,https://github.com/apache/spark/pull/21888.diff,https://github.com/apache/spark/pull/21888.patch
404,https://api.github.com/repos/apache/spark/issues/21881,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21881/labels{/name},https://api.github.com/repos/apache/spark/issues/21881/comments,https://api.github.com/repos/apache/spark/issues/21881/events,https://github.com/apache/spark/pull/21881,344761351,MDExOlB1bGxSZXF1ZXN0MjA0MDc3NjE1,21881,[SPARK-24930][SQL] Improve exception information when using LOAD DATA LOCAL INPATH,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-07-26T09:17:02Z,2019-09-16T18:23:37Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
1. root user create a test.txt file contains a record '123'  in /root/ directory
2. switch mr user to execute spark-shell --master local

```
scala> spark.version
res2: String = 2.2.1

scala> spark.sql(""create table t1(id int) partitioned by(area string)"");
2018-07-26 17:20:37,523 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: hdfs://nameservice/spark/t1 specified for non-external table:t1
res4: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""load data local inpath '/root/test.txt' into table t1 partition(area ='025')"")
org.apache.spark.sql.AnalysisException: LOAD DATA input path does not exist: /root/test.txt;
 at org.apache.spark.sql.execution.command.LoadDataCommand.run(tables.scala:339)
 at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
 at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
 at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:639)
 ... 48 elided

scala>
```

In fact, the input path exists, but the mr user does not have permission to access the directory `/root/` ,so the message throwed by `AnalysisException` can confuse user.

## How was this patch tested?
existing test case

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,ouyangxiaochen,22695134,MDQ6VXNlcjIyNjk1MTM0,https://avatars0.githubusercontent.com/u/22695134?v=4,,https://api.github.com/users/ouyangxiaochen,https://github.com/ouyangxiaochen,https://api.github.com/users/ouyangxiaochen/followers,https://api.github.com/users/ouyangxiaochen/following{/other_user},https://api.github.com/users/ouyangxiaochen/gists{/gist_id},https://api.github.com/users/ouyangxiaochen/starred{/owner}{/repo},https://api.github.com/users/ouyangxiaochen/subscriptions,https://api.github.com/users/ouyangxiaochen/orgs,https://api.github.com/users/ouyangxiaochen/repos,https://api.github.com/users/ouyangxiaochen/events{/privacy},https://api.github.com/users/ouyangxiaochen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21881,https://github.com/apache/spark/pull/21881,https://github.com/apache/spark/pull/21881.diff,https://github.com/apache/spark/pull/21881.patch
405,https://api.github.com/repos/apache/spark/issues/21877,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21877/labels{/name},https://api.github.com/repos/apache/spark/issues/21877/comments,https://api.github.com/repos/apache/spark/issues/21877/events,https://github.com/apache/spark/pull/21877,344638560,MDExOlB1bGxSZXF1ZXN0MjAzOTg3ODI1,21877,[SPARK-24923][SQL][WIP] Add unpartitioned CTAS and RTAS support for DataSourceV2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2018-07-25T23:09:40Z,2019-09-16T19:15:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

* Remove extends from `ReadSupport` and `WriteSupport` classes for use with `Table`
* Add CTAS and RTAS logical plans
* Refactor physical write plans so AppendData, CTAS, and RTAS use the same base class
* Add support for `TableCatalog` to `DataFrameReader` and `DataFrameWriter`
* Add `TableV2Relation` for tables that are loaded by `TableCatalog` and have no `DataSource` instance
* Move implicit helpers into `DataSourceV2Implicits` to avoid future churn

Note that this doesn't handle `partitionBy` in `DataFrameWriter`. Adding support for partitioned tables will require validation rules.

This is based on unmerged work and includes the commits from #21306 and #21305.

## How was this patch tested?

Adding unit tests for CTAS and RTAS.",spark,apache,rdblue,87915,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21877,https://github.com/apache/spark/pull/21877,https://github.com/apache/spark/pull/21877.diff,https://github.com/apache/spark/pull/21877.patch
406,https://api.github.com/repos/apache/spark/issues/21861,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21861/labels{/name},https://api.github.com/repos/apache/spark/issues/21861/comments,https://api.github.com/repos/apache/spark/issues/21861/events,https://github.com/apache/spark/pull/21861,344027958,MDExOlB1bGxSZXF1ZXN0MjAzNTE5NjE3,21861,[SPARK-24907][SQL][WIP] Migrate JDBC DataSource to JDBCDataSourceV2 Read using DataSourceV2 API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2018-07-24T13:04:47Z,2019-09-16T18:23:35Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

(After the update of DataSourceV2 API, this PR will be updated)

Changes: Migrate JDBC DataSource to JDBCDataSourceV2 Read using DataSourceV2 API

- This PR is minimum viable and other JDBC features will be added soon, but it is ready for review to see if we are in the right direction.
- This PR also contains a minor improvement on DataSourceOptions (to make it serializable). 
- It does not support DLL due to the limitations of the new API

## How was this patch tested?
New unit tests.
",spark,apache,tengpeng,2724786,MDQ6VXNlcjI3MjQ3ODY=,https://avatars0.githubusercontent.com/u/2724786?v=4,,https://api.github.com/users/tengpeng,https://github.com/tengpeng,https://api.github.com/users/tengpeng/followers,https://api.github.com/users/tengpeng/following{/other_user},https://api.github.com/users/tengpeng/gists{/gist_id},https://api.github.com/users/tengpeng/starred{/owner}{/repo},https://api.github.com/users/tengpeng/subscriptions,https://api.github.com/users/tengpeng/orgs,https://api.github.com/users/tengpeng/repos,https://api.github.com/users/tengpeng/events{/privacy},https://api.github.com/users/tengpeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21861,https://github.com/apache/spark/pull/21861,https://github.com/apache/spark/pull/21861.diff,https://github.com/apache/spark/pull/21861.patch
407,https://api.github.com/repos/apache/spark/issues/21791,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21791/labels{/name},https://api.github.com/repos/apache/spark/issues/21791/comments,https://api.github.com/repos/apache/spark/issues/21791/events,https://github.com/apache/spark/pull/21791,341864893,MDExOlB1bGxSZXF1ZXN0MjAxODk5MDI4,21791,[SPARK-24925][SQL] input bytesRead metrics fluctuate from time to time,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2018-07-17T10:44:29Z,2019-09-16T18:20:46Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, ColumnarBatch's bytesRead needs to be updated every 4096 * 1000 rows, which makes the metrics out of date. This PR makes it update at each batch.

## How was this patch tested?

Existing UTs.
",spark,apache,yucai,2989575,MDQ6VXNlcjI5ODk1NzU=,https://avatars0.githubusercontent.com/u/2989575?v=4,,https://api.github.com/users/yucai,https://github.com/yucai,https://api.github.com/users/yucai/followers,https://api.github.com/users/yucai/following{/other_user},https://api.github.com/users/yucai/gists{/gist_id},https://api.github.com/users/yucai/starred{/owner}{/repo},https://api.github.com/users/yucai/subscriptions,https://api.github.com/users/yucai/orgs,https://api.github.com/users/yucai/repos,https://api.github.com/users/yucai/events{/privacy},https://api.github.com/users/yucai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21791,https://github.com/apache/spark/pull/21791,https://github.com/apache/spark/pull/21791.diff,https://github.com/apache/spark/pull/21791.patch
408,https://api.github.com/repos/apache/spark/issues/21709,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21709/labels{/name},https://api.github.com/repos/apache/spark/issues/21709/comments,https://api.github.com/repos/apache/spark/issues/21709/events,https://github.com/apache/spark/pull/21709,338096784,MDExOlB1bGxSZXF1ZXN0MTk5MTE1Mjcw,21709,[SPARK-5152][CORE] Read metrics config file from Hadoop compatible file system,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2018-07-04T00:52:44Z,2019-09-16T19:15:06Z,,MEMBER,"## What changes were proposed in this pull request?

Enable metrics config file to be read directly from Hadoop compatible file system.
```
spark-submit --conf spark.metrics.conf=hdfs://<nn>:8020/config/spark-metrics.properties
spark-submit --conf spark.metrics.conf=s3a://<bucket>/config/spark-metrics.properties
```

There is no need to distribute the config file if a distributed file system is used.

If no URI scheme is specified, assume local file system. This ensures backwards compatibility.

## How was this patch tested?

Unit tests:
- MetricsSystemSuite
- MetricsConfigSuite
- TaskContextSuite

Manual tests:
- Local file without URI scheme
- Local file with file://
- HDFS file path
- S3 file path

Author: John Zhuge <jzhuge@apache.org>",spark,apache,jzhuge,1883812,MDQ6VXNlcjE4ODM4MTI=,https://avatars2.githubusercontent.com/u/1883812?v=4,,https://api.github.com/users/jzhuge,https://github.com/jzhuge,https://api.github.com/users/jzhuge/followers,https://api.github.com/users/jzhuge/following{/other_user},https://api.github.com/users/jzhuge/gists{/gist_id},https://api.github.com/users/jzhuge/starred{/owner}{/repo},https://api.github.com/users/jzhuge/subscriptions,https://api.github.com/users/jzhuge/orgs,https://api.github.com/users/jzhuge/repos,https://api.github.com/users/jzhuge/events{/privacy},https://api.github.com/users/jzhuge/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21709,https://github.com/apache/spark/pull/21709,https://github.com/apache/spark/pull/21709.diff,https://github.com/apache/spark/pull/21709.patch
409,https://api.github.com/repos/apache/spark/issues/21694,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21694/labels{/name},https://api.github.com/repos/apache/spark/issues/21694/comments,https://api.github.com/repos/apache/spark/issues/21694/events,https://github.com/apache/spark/pull/21694,337408209,MDExOlB1bGxSZXF1ZXN0MTk4NTk5NzEx,21694,SPARK-24714 AnalysisSuite should use ClassTag to check the runtime in‚Ä¶,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2018-07-02T07:59:13Z,2019-09-16T18:23:27Z,,NONE,"‚Ä¶stance

## What changes were proposed in this pull request?

Use scala classTag to do the type check.

## How was this patch tested?

this PR is against test

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,chia7712,6234750,MDQ6VXNlcjYyMzQ3NTA=,https://avatars3.githubusercontent.com/u/6234750?v=4,,https://api.github.com/users/chia7712,https://github.com/chia7712,https://api.github.com/users/chia7712/followers,https://api.github.com/users/chia7712/following{/other_user},https://api.github.com/users/chia7712/gists{/gist_id},https://api.github.com/users/chia7712/starred{/owner}{/repo},https://api.github.com/users/chia7712/subscriptions,https://api.github.com/users/chia7712/orgs,https://api.github.com/users/chia7712/repos,https://api.github.com/users/chia7712/events{/privacy},https://api.github.com/users/chia7712/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21694,https://github.com/apache/spark/pull/21694,https://github.com/apache/spark/pull/21694.diff,https://github.com/apache/spark/pull/21694.patch
410,https://api.github.com/repos/apache/spark/issues/21685,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21685/labels{/name},https://api.github.com/repos/apache/spark/issues/21685/comments,https://api.github.com/repos/apache/spark/issues/21685/events,https://github.com/apache/spark/pull/21685,337259440,MDExOlB1bGxSZXF1ZXN0MTk4NTA0Nzcz,21685,[SPARK-24707][DSTREAMS] Enable spark-kafka-streaming to maintain min ‚Ä¶,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2018-07-01T03:08:39Z,2019-12-03T16:16:19Z,,NONE,"‚Ä¶buffer using async thread to avoid blocking kafka poll

## What changes were proposed in this pull request?

Currently Spark Kafka RDD will block on kafka consumer poll. Specially in Spark-Kafka-streaming job this poll duration adds into batch processing time which result in 
	* Increased batch processing time (which is apart from time taken to process records)
	* Results in unpredictable batch processing time based on poll time.

This PR consists of changes to maintain min records in buffer, so that streaming batches processing do not have to get blocked on kafka poll.

## How was this patch tested?

Unit test / manual test.
[Before_change.pdf](https://github.com/apache/spark/files/2152353/Before_change.pdf)
[After_change_2000_buffer_per_part.pdf](https://github.com/apache/spark/files/2152354/After_change_2000_buffer_per_part.pdf)
",spark,apache,sidhavratha,2279976,MDQ6VXNlcjIyNzk5NzY=,https://avatars2.githubusercontent.com/u/2279976?v=4,,https://api.github.com/users/sidhavratha,https://github.com/sidhavratha,https://api.github.com/users/sidhavratha/followers,https://api.github.com/users/sidhavratha/following{/other_user},https://api.github.com/users/sidhavratha/gists{/gist_id},https://api.github.com/users/sidhavratha/starred{/owner}{/repo},https://api.github.com/users/sidhavratha/subscriptions,https://api.github.com/users/sidhavratha/orgs,https://api.github.com/users/sidhavratha/repos,https://api.github.com/users/sidhavratha/events{/privacy},https://api.github.com/users/sidhavratha/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21685,https://github.com/apache/spark/pull/21685,https://github.com/apache/spark/pull/21685.diff,https://github.com/apache/spark/pull/21685.patch
411,https://api.github.com/repos/apache/spark/issues/21671,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21671/labels{/name},https://api.github.com/repos/apache/spark/issues/21671/comments,https://api.github.com/repos/apache/spark/issues/21671/events,https://github.com/apache/spark/pull/21671,337032777,MDExOlB1bGxSZXF1ZXN0MTk4MzQzNjgy,21671,[SPARK-24682] [SQL] from_json / to_json now handle java.sql.Date/Timestamp as Map key,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-06-29T15:01:25Z,2019-09-16T18:23:21Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When generating JSON the key is not just converted to a String now, instead the type is checked and for Dates and Timestamps the formatting happens first (if provided). When reading JSON, if the Schema says that a Map has a Date or Timestamp as the key then once again the formatting is done before the String is converted to the correct value for a Date or Timestamp.

## How was this patch tested?

I have added Unit Tests.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,patrickmcgloin,12597336,MDQ6VXNlcjEyNTk3MzM2,https://avatars3.githubusercontent.com/u/12597336?v=4,,https://api.github.com/users/patrickmcgloin,https://github.com/patrickmcgloin,https://api.github.com/users/patrickmcgloin/followers,https://api.github.com/users/patrickmcgloin/following{/other_user},https://api.github.com/users/patrickmcgloin/gists{/gist_id},https://api.github.com/users/patrickmcgloin/starred{/owner}{/repo},https://api.github.com/users/patrickmcgloin/subscriptions,https://api.github.com/users/patrickmcgloin/orgs,https://api.github.com/users/patrickmcgloin/repos,https://api.github.com/users/patrickmcgloin/events{/privacy},https://api.github.com/users/patrickmcgloin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21671,https://github.com/apache/spark/pull/21671,https://github.com/apache/spark/pull/21671.diff,https://github.com/apache/spark/pull/21671.patch
412,https://api.github.com/repos/apache/spark/issues/21642,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21642/labels{/name},https://api.github.com/repos/apache/spark/issues/21642/comments,https://api.github.com/repos/apache/spark/issues/21642/events,https://github.com/apache/spark/pull/21642,335727357,MDExOlB1bGxSZXF1ZXN0MTk3MzUzMjE5,21642,[SPARK-22425][CORE][SQL] record inputs/outputs that imported/generate‚Ä¶,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-06-26T09:12:54Z,2019-09-16T18:21:08Z,,NONE,"‚Ä¶d by DataFrameReader/DataFrameWriter to event log

## What changes were proposed in this pull request?

(Please fill in changes proposed in this fix)

## How was this patch tested?

(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,voidfunction,6745693,MDQ6VXNlcjY3NDU2OTM=,https://avatars3.githubusercontent.com/u/6745693?v=4,,https://api.github.com/users/voidfunction,https://github.com/voidfunction,https://api.github.com/users/voidfunction/followers,https://api.github.com/users/voidfunction/following{/other_user},https://api.github.com/users/voidfunction/gists{/gist_id},https://api.github.com/users/voidfunction/starred{/owner}{/repo},https://api.github.com/users/voidfunction/subscriptions,https://api.github.com/users/voidfunction/orgs,https://api.github.com/users/voidfunction/repos,https://api.github.com/users/voidfunction/events{/privacy},https://api.github.com/users/voidfunction/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21642,https://github.com/apache/spark/pull/21642,https://github.com/apache/spark/pull/21642.diff,https://github.com/apache/spark/pull/21642.patch
413,https://api.github.com/repos/apache/spark/issues/21618,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21618/labels{/name},https://api.github.com/repos/apache/spark/issues/21618/comments,https://api.github.com/repos/apache/spark/issues/21618/events,https://github.com/apache/spark/pull/21618,335070885,MDExOlB1bGxSZXF1ZXN0MTk2ODgzMzA2,21618,[SPARK-20408][SQL] Get the glob path in parallel to reduce resolve relation time,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,24,2018-06-23T05:33:14Z,2019-09-16T19:16:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR change the work of getting glob path in parallel, which can make complex wildcard path more quickly, the mainly changes in details:
1.Add new function getGlobbedPaths in DataSource, return all paths represented by the wildcard pattern, use a local thread pool to do this while the paths number expanded from patten lager than `spark.sql.sources.parallelGetGlobbedPath.threshold`. The local thread pool size controlled by `spark.sql.sources.parallelGetGlobbedPath.numThreads`
2.Add new function expandGlobPath in SparkHadoopUtil, to expand the dir represented by the patten, here we mainly reuse the logic in org.apache.hadoop.fs.Globber.glob().

## How was this patch tested?

Add UT in SparkHadoopUtilSuite.
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21618,https://github.com/apache/spark/pull/21618,https://github.com/apache/spark/pull/21618.diff,https://github.com/apache/spark/pull/21618.patch
414,https://api.github.com/repos/apache/spark/issues/21613,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21613/labels{/name},https://api.github.com/repos/apache/spark/issues/21613/comments,https://api.github.com/repos/apache/spark/issues/21613/events,https://github.com/apache/spark/pull/21613,334827542,MDExOlB1bGxSZXF1ZXN0MTk2Njk4OTA4,21613,[SPARK-24629][SQL]thrift server memory leaks when Beeline session is closed,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-06-22T10:26:14Z,2019-09-16T18:21:13Z,,NONE,"## What changes were proposed in this pull request?

Maintaining a running statement map.
When the session is closed, the relative statements should receive a cancel event. 

## How was this patch tested?
the attached UI show statements is successfully cancelled in UI.
<img width=""1263"" alt=""3333"" src=""https://user-images.githubusercontent.com/2654887/41772001-94117874-7649-11e8-9701-6edf13593b9d.png"">

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,ChenjunZou,2654887,MDQ6VXNlcjI2NTQ4ODc=,https://avatars2.githubusercontent.com/u/2654887?v=4,,https://api.github.com/users/ChenjunZou,https://github.com/ChenjunZou,https://api.github.com/users/ChenjunZou/followers,https://api.github.com/users/ChenjunZou/following{/other_user},https://api.github.com/users/ChenjunZou/gists{/gist_id},https://api.github.com/users/ChenjunZou/starred{/owner}{/repo},https://api.github.com/users/ChenjunZou/subscriptions,https://api.github.com/users/ChenjunZou/orgs,https://api.github.com/users/ChenjunZou/repos,https://api.github.com/users/ChenjunZou/events{/privacy},https://api.github.com/users/ChenjunZou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21613,https://github.com/apache/spark/pull/21613,https://github.com/apache/spark/pull/21613.diff,https://github.com/apache/spark/pull/21613.patch
415,https://api.github.com/repos/apache/spark/issues/21525,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21525/labels{/name},https://api.github.com/repos/apache/spark/issues/21525/comments,https://api.github.com/repos/apache/spark/issues/21525/events,https://github.com/apache/spark/pull/21525,331099972,MDExOlB1bGxSZXF1ZXN0MTkzOTEzODk5,21525,[SPARK-24513][ML] Attribute support in UnaryTransformer,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-06-11T08:56:05Z,2019-09-16T18:21:15Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR adds Metadata support in `UnaryTransformer`, as a preliminary work of [SPARK-13998](https://issues.apache.org/jira/browse/SPARK-13998) and [SPARK-13964](https://issues.apache.org/jira/browse/SPARK-13964).

## How was this patch tested?

unit test: `build/mvn -Dtest=none -DwildcardSuites=org.apache.spark.ml.feature.* test`",spark,apache,dongjinleekr,2375128,MDQ6VXNlcjIzNzUxMjg=,https://avatars2.githubusercontent.com/u/2375128?v=4,,https://api.github.com/users/dongjinleekr,https://github.com/dongjinleekr,https://api.github.com/users/dongjinleekr/followers,https://api.github.com/users/dongjinleekr/following{/other_user},https://api.github.com/users/dongjinleekr/gists{/gist_id},https://api.github.com/users/dongjinleekr/starred{/owner}{/repo},https://api.github.com/users/dongjinleekr/subscriptions,https://api.github.com/users/dongjinleekr/orgs,https://api.github.com/users/dongjinleekr/repos,https://api.github.com/users/dongjinleekr/events{/privacy},https://api.github.com/users/dongjinleekr/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21525,https://github.com/apache/spark/pull/21525,https://github.com/apache/spark/pull/21525.diff,https://github.com/apache/spark/pull/21525.patch
416,https://api.github.com/repos/apache/spark/issues/21516,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21516/labels{/name},https://api.github.com/repos/apache/spark/issues/21516/comments,https://api.github.com/repos/apache/spark/issues/21516/events,https://github.com/apache/spark/pull/21516,330831812,MDExOlB1bGxSZXF1ZXN0MTkzNzQ1MTk1,21516,[SPARK-24501][MESOS] Add Dispatcher and Driver metrics,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2018-06-08T23:45:01Z,2019-09-16T18:21:18Z,,NONE,"Adds metrics for the Mesos Dispatcher:
- Counters: The total number of times that submissions have entered states
- Timers: The duration from submit or launch until a submission entered a given state
- Histogram: The retry counts at time of retry

Also adds metrics for the Mesos coarse-grained driver implementation. The deprecated fine-grained driver is left as-is.
The driver metrics include e.g. frequency of RPCs to/from Mesos, the internal state tracking of tasks/agents, and durations for bringing up the tasks.

## What changes were proposed in this pull request?

(Please fill in changes proposed in this fix)

## How was this patch tested?

(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,nickbp,35933,MDQ6VXNlcjM1OTMz,https://avatars2.githubusercontent.com/u/35933?v=4,,https://api.github.com/users/nickbp,https://github.com/nickbp,https://api.github.com/users/nickbp/followers,https://api.github.com/users/nickbp/following{/other_user},https://api.github.com/users/nickbp/gists{/gist_id},https://api.github.com/users/nickbp/starred{/owner}{/repo},https://api.github.com/users/nickbp/subscriptions,https://api.github.com/users/nickbp/orgs,https://api.github.com/users/nickbp/repos,https://api.github.com/users/nickbp/events{/privacy},https://api.github.com/users/nickbp/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21516,https://github.com/apache/spark/pull/21516,https://github.com/apache/spark/pull/21516.diff,https://github.com/apache/spark/pull/21516.patch
417,https://api.github.com/repos/apache/spark/issues/21486,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21486/labels{/name},https://api.github.com/repos/apache/spark/issues/21486/comments,https://api.github.com/repos/apache/spark/issues/21486/events,https://github.com/apache/spark/pull/21486,328726777,MDExOlB1bGxSZXF1ZXN0MTkyMTg3NzMy,21486,[SPARK-24387][Core] Heartbeat-timeout executor is added back and used again,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-06-02T08:42:18Z,2019-09-16T18:23:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When an executor's heartbeat is lost, we call scheduler.executorLost before we tell the backend to kill the executor. TaskSchedulerImpl asks the backend to revive offers in executorLost. If this is the only executor, it's possible the backend will offer it again to TaskSchedulerImpl, and the retried task is scheduled to this executor.

This patch proposes to call scheduler.executorLost after the executor is killed. At this point, the executor has been marked as pending-to-remove and won't be offered again.

## How was this patch tested?

Added a new test case in HeartbeatReceiverSuite. W/o the fix this test case fails.
",spark,apache,lirui-apache,5210788,MDQ6VXNlcjUyMTA3ODg=,https://avatars1.githubusercontent.com/u/5210788?v=4,,https://api.github.com/users/lirui-apache,https://github.com/lirui-apache,https://api.github.com/users/lirui-apache/followers,https://api.github.com/users/lirui-apache/following{/other_user},https://api.github.com/users/lirui-apache/gists{/gist_id},https://api.github.com/users/lirui-apache/starred{/owner}{/repo},https://api.github.com/users/lirui-apache/subscriptions,https://api.github.com/users/lirui-apache/orgs,https://api.github.com/users/lirui-apache/repos,https://api.github.com/users/lirui-apache/events{/privacy},https://api.github.com/users/lirui-apache/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21486,https://github.com/apache/spark/pull/21486,https://github.com/apache/spark/pull/21486.diff,https://github.com/apache/spark/pull/21486.patch
418,https://api.github.com/repos/apache/spark/issues/21470,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21470/labels{/name},https://api.github.com/repos/apache/spark/issues/21470/comments,https://api.github.com/repos/apache/spark/issues/21470/events,https://github.com/apache/spark/pull/21470,328243995,MDExOlB1bGxSZXF1ZXN0MTkxODMzNTE4,21470,[SPARK-24443][SQL] comparison should accept structurally-equal types,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-05-31T18:14:52Z,2019-09-16T19:17:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

When comparing struct type values, it's a little hard to make the field names same. Simple queries like
`SELECT (a, b) = (1, 2) FROM t` may fail because the field names do not match.

In Postgres, when comparing struct type values, it will do safe type coercion and ignore field name difference
```
# create table t(i smallint, j smallint);
CREATE TABLE

# select Row(i, j) = Row(1, 1) from t;                              
 ?column? 
----------
 t
(1 row)

# select Row(i, j) < Row(1, 1) from t;
 ?column? 
----------
 f
(1 row)

# select Row(i, j) = Row(j, i) from t;
 ?column? 
----------
 t
(1 row)
```

This PR follows Postgres and accept structurally-equal types in comparison.

This also fixes the test failures in https://github.com/apache/spark/pull/21442 .

TODO:
* check Hive and Presto.
* think about array/map type. Postgres doesn't support type coercion for elements in array/map when comparison.

## How was this patch tested?

new test cases.",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21470,https://github.com/apache/spark/pull/21470,https://github.com/apache/spark/pull/21470.diff,https://github.com/apache/spark/pull/21470.patch
419,https://api.github.com/repos/apache/spark/issues/21449,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21449/labels{/name},https://api.github.com/repos/apache/spark/issues/21449/comments,https://api.github.com/repos/apache/spark/issues/21449/events,https://github.com/apache/spark/pull/21449,327302585,MDExOlB1bGxSZXF1ZXN0MTkxMTMyMzA3,21449,[SPARK-24385][SQL] Resolve self-join condition ambiguity for all BinaryComparisons,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2018-05-29T12:39:46Z,2019-09-16T19:17:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

In `Dataset.join` we have a small hack for resolving ambiguity in the column name for self-joins. The current code supports only `EqualTo`, but we may have other conditions involving columns on self joins: in general any `BinaryComparison` can be specified and faces the same issue.

The PR extends the fix to all `BinaryComparison`s.

## How was this patch tested?

added UT
",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21449,https://github.com/apache/spark/pull/21449,https://github.com/apache/spark/pull/21449.diff,https://github.com/apache/spark/pull/21449.patch
420,https://api.github.com/repos/apache/spark/issues/21348,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21348/labels{/name},https://api.github.com/repos/apache/spark/issues/21348/comments,https://api.github.com/repos/apache/spark/issues/21348/events,https://github.com/apache/spark/pull/21348,323832745,MDExOlB1bGxSZXF1ZXN0MTg4NTk1NDU4,21348,[SPARK-22739][Catalyst] Additional Expression Support for Objects,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2018-05-17T00:32:11Z,2019-09-16T18:23:02Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR is a working followup to the expression work begun in #20085. It provides necessary `Expression` definitions to support custom encoders (see this discussion in the [Spark-Avro](https://github.com/databricks/spark-avro/pull/217#issuecomment-342856719) project).

It adds the following expressions:

* `ObjectCast` - performs explicit casting of an `Expression` result to a `DataType`
* `StaticField` - retrieves a static field against a class that otherwise has no accessor method
* `InstanceOf` - an `Expression` for the Java `instanceof` operation

Modifies `NewInstance` to take a sequence of method-name and arguments initialization tuples, which are executed against the newly constructed object instance.

Removes `InitializeJavaBean`, as the generalized `NewInstance` subsumes its use-case. 

## How was this patch tested?

Adds unit test for `NewInstance` supporting post-constructor initializations. All previous ""JavaBean"" tests were refactored to use `NewInstance`.

Additional examples of working encoders that would use these new expressions can be seen in the [Spark-Avro](https://github.com/bdrillard/spark-avro/blob/avro_encoder_2-4/src/main/scala/com/databricks/spark/avro/AvroEncoder.scala) and [Bunsen](https://github.com/bdrillard/bunsen/blob/issue-23/bunsen-core/src/main/scala/com/cerner/bunsen/EncoderBuilder.scala) projects.",spark,apache,bdrillard,1728813,MDQ6VXNlcjE3Mjg4MTM=,https://avatars3.githubusercontent.com/u/1728813?v=4,,https://api.github.com/users/bdrillard,https://github.com/bdrillard,https://api.github.com/users/bdrillard/followers,https://api.github.com/users/bdrillard/following{/other_user},https://api.github.com/users/bdrillard/gists{/gist_id},https://api.github.com/users/bdrillard/starred{/owner}{/repo},https://api.github.com/users/bdrillard/subscriptions,https://api.github.com/users/bdrillard/orgs,https://api.github.com/users/bdrillard/repos,https://api.github.com/users/bdrillard/events{/privacy},https://api.github.com/users/bdrillard/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21348,https://github.com/apache/spark/pull/21348,https://github.com/apache/spark/pull/21348.diff,https://github.com/apache/spark/pull/21348.patch
421,https://api.github.com/repos/apache/spark/issues/21339,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21339/labels{/name},https://api.github.com/repos/apache/spark/issues/21339/comments,https://api.github.com/repos/apache/spark/issues/21339/events,https://github.com/apache/spark/pull/21339,323390516,MDExOlB1bGxSZXF1ZXN0MTg4MjYxNDA5,21339,"[SPARK-24287][Core] Spark -packages option should support classifier, no-transitive, and custom conf","[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2018-05-15T21:30:04Z,2019-09-16T18:21:33Z,,NONE,"## What changes were proposed in this pull request?

We should extend Spark's -package option to support:

1. Turn-off transitive dependency on a given artifact(like spark-avro)
2. Resolving a given artifact with classifier (like avro-mapred-1.7.4-h2.jar)
3. Resolving a given artifact with custom ivy conf
4. Excluding particular transitive dependencies from a given artifact. This can help when artifacts have conflict transitive dependencies. We currently only have top-level exclusion rule applies for all artifacts.

New artifact spec to be reviewed:

Coordinates are split by ','
Each coordinate should be provided in the format `groupId:artifactId:version?param1=value1\&param2\&value2:..` or `groupId/artifactId:version?param1=value1\&param2=value2:..`

Param splitter needs to be escaped as \& , or entire coordinates string needs to be enclosed in double quotes, since & is the background process char in cli.

Optional params are 'classifier', 'transitive', 'exclude', 'conf':
  classifier: classifier of the artifact
  transitive: whether to resolve transitive deps for the artifact
  exlude: exclude list of transitive artifacts for this artifact(e.g. ""a#b#c"")
  conf: the ivy conf of the artifact

We have tested this patch internally and it greatly increases the flexibility when user uses -packages option 

## How was this patch tested?
added unit test
",spark,apache,fangshil,2416746,MDQ6VXNlcjI0MTY3NDY=,https://avatars0.githubusercontent.com/u/2416746?v=4,,https://api.github.com/users/fangshil,https://github.com/fangshil,https://api.github.com/users/fangshil/followers,https://api.github.com/users/fangshil/following{/other_user},https://api.github.com/users/fangshil/gists{/gist_id},https://api.github.com/users/fangshil/starred{/owner}{/repo},https://api.github.com/users/fangshil/subscriptions,https://api.github.com/users/fangshil/orgs,https://api.github.com/users/fangshil/repos,https://api.github.com/users/fangshil/events{/privacy},https://api.github.com/users/fangshil/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21339,https://github.com/apache/spark/pull/21339,https://github.com/apache/spark/pull/21339.diff,https://github.com/apache/spark/pull/21339.patch
422,https://api.github.com/repos/apache/spark/issues/21286,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21286/labels{/name},https://api.github.com/repos/apache/spark/issues/21286/comments,https://api.github.com/repos/apache/spark/issues/21286/events,https://github.com/apache/spark/pull/21286,321785743,MDExOlB1bGxSZXF1ZXN0MTg3MDg0NTYy,21286,[SPARK-24238][SQL] HadoopFsRelation can't append the same table with multi job at the same time,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,24,2018-05-10T02:15:16Z,2019-09-24T10:35:10Z,,NONE,"## What changes were proposed in this pull request?

When there are multiple jobs at the same time append a `HadoopFsRelation`, there will be an error, there are the following two errors: 

1. A job will succeed, but the data will be wrong and more data than excepted will appear
2. Other jobs will fail with `java.io.FileNotFoundException: Failed to get file status skip_dir/_temporary/0`

The main reason for this problem is because multiple job will use the same `_temporary` directory.

So the core idea of this `PR` is to create a different temporary directory with jobId for the different Job in the `output` folder , so that conflicts can be avoided.

## How was this patch tested?

I manually tested. 
",spark,apache,zheh12,2442257,MDQ6VXNlcjI0NDIyNTc=,https://avatars3.githubusercontent.com/u/2442257?v=4,,https://api.github.com/users/zheh12,https://github.com/zheh12,https://api.github.com/users/zheh12/followers,https://api.github.com/users/zheh12/following{/other_user},https://api.github.com/users/zheh12/gists{/gist_id},https://api.github.com/users/zheh12/starred{/owner}{/repo},https://api.github.com/users/zheh12/subscriptions,https://api.github.com/users/zheh12/orgs,https://api.github.com/users/zheh12/repos,https://api.github.com/users/zheh12/events{/privacy},https://api.github.com/users/zheh12/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21286,https://github.com/apache/spark/pull/21286,https://github.com/apache/spark/pull/21286.diff,https://github.com/apache/spark/pull/21286.patch
423,https://api.github.com/repos/apache/spark/issues/21242,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21242/labels{/name},https://api.github.com/repos/apache/spark/issues/21242/comments,https://api.github.com/repos/apache/spark/issues/21242/events,https://github.com/apache/spark/pull/21242,320450993,MDExOlB1bGxSZXF1ZXN0MTg2MTE5NjA3,21242,[SPARK-23657][SQL] Document and expose the internal data API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-05-04T22:44:31Z,2019-12-06T07:55:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This makes the `InternalRow`, `ArrayData`, and `MapData` classes public and adds package documentation for the internal representation used by Spark SQL.

The motivation for this change is to document the internal API because it has been leaked as a public API and used in the v2 DataSource classes.

## How was this patch tested?

Existing tests. This is a refactor and adds documentation.",spark,apache,rdblue,87915,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21242,https://github.com/apache/spark/pull/21242,https://github.com/apache/spark/pull/21242.diff,https://github.com/apache/spark/pull/21242.patch
424,https://api.github.com/repos/apache/spark/issues/21194,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21194/labels{/name},https://api.github.com/repos/apache/spark/issues/21194/comments,https://api.github.com/repos/apache/spark/issues/21194/events,https://github.com/apache/spark/pull/21194,318920027,MDExOlB1bGxSZXF1ZXN0MTg0OTc4NDA5,21194,[SPARK-24046][SS] Fix rate source when rowsPerSecond <= rampUpTime  ,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-04-30T14:55:12Z,2019-09-16T18:22:51Z,,NONE,"## What changes were proposed in this pull request?

Fixes the ramp-up of the rate source for the case `rowsPerSecond <= rampUpTime` which previously resulted in a flat ramp-up (0-value until the clock reached `rampUpTime`. For example: 
![image-2018-04-22-22-06-49-202](https://user-images.githubusercontent.com/874997/51993288-d2241400-24ae-11e9-84c4-6f948edc3303.png)

See details in  [SPARK-24046](https://issues.apache.org/jira/browse/SPARK-24046)

This implementation replaces the original code. Uses the distance quadratic function, meaning that ramp-up is a smooth parabole that joins the linear constant rate at `rampUpTime`:
Original vs. this PR:
![ramp-up-break](https://user-images.githubusercontent.com/874997/51993392-0d264780-24af-11e9-8db3-514a46fba84c.png)

## How was this patch tested?

- Improved and extended existing unit tests.
- Validated the algorithm visually in a notebook for many different scenarios

",spark,apache,maasg,874997,MDQ6VXNlcjg3NDk5Nw==,https://avatars3.githubusercontent.com/u/874997?v=4,,https://api.github.com/users/maasg,https://github.com/maasg,https://api.github.com/users/maasg/followers,https://api.github.com/users/maasg/following{/other_user},https://api.github.com/users/maasg/gists{/gist_id},https://api.github.com/users/maasg/starred{/owner}{/repo},https://api.github.com/users/maasg/subscriptions,https://api.github.com/users/maasg/orgs,https://api.github.com/users/maasg/repos,https://api.github.com/users/maasg/events{/privacy},https://api.github.com/users/maasg/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21194,https://github.com/apache/spark/pull/21194,https://github.com/apache/spark/pull/21194.diff,https://github.com/apache/spark/pull/21194.patch
425,https://api.github.com/repos/apache/spark/issues/21188,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21188/labels{/name},https://api.github.com/repos/apache/spark/issues/21188/comments,https://api.github.com/repos/apache/spark/issues/21188/events,https://github.com/apache/spark/pull/21188,318610354,MDExOlB1bGxSZXF1ZXN0MTg0NzcwNDQw,21188,[SPARK-24046][SS] Fix rate source rowsPerSecond <= rampUpTime corner case,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-04-28T07:38:24Z,2019-09-16T19:18:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Current Rate source has some issues when calculating `valueAtSecond` if `rowsPerSecond` <= `rampUpTime`, value will not be gradually increased, details can be found in [JIRA](https://issues.apache.org/jira/browse/SPARK-24046). So here propose to fix this issue. 

## How was this patch tested?

Add UT
",spark,apache,jerryshao,850797,MDQ6VXNlcjg1MDc5Nw==,https://avatars2.githubusercontent.com/u/850797?v=4,,https://api.github.com/users/jerryshao,https://github.com/jerryshao,https://api.github.com/users/jerryshao/followers,https://api.github.com/users/jerryshao/following{/other_user},https://api.github.com/users/jerryshao/gists{/gist_id},https://api.github.com/users/jerryshao/starred{/owner}{/repo},https://api.github.com/users/jerryshao/subscriptions,https://api.github.com/users/jerryshao/orgs,https://api.github.com/users/jerryshao/repos,https://api.github.com/users/jerryshao/events{/privacy},https://api.github.com/users/jerryshao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21188,https://github.com/apache/spark/pull/21188,https://github.com/apache/spark/pull/21188.diff,https://github.com/apache/spark/pull/21188.patch
426,https://api.github.com/repos/apache/spark/issues/21164,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21164/labels{/name},https://api.github.com/repos/apache/spark/issues/21164/comments,https://api.github.com/repos/apache/spark/issues/21164/events,https://github.com/apache/spark/pull/21164,317936669,MDExOlB1bGxSZXF1ZXN0MTg0MjcwMDg5,21164,[SPARK-24098][SQL] ScriptTransformationExec should wait process exiting before output iterator finish,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2018-04-26T08:58:00Z,2019-10-09T20:43:02Z,,CONTRIBUTOR,"When feed thread doesn't set its _exception variable and the progress doesn't exit completely, output Iterator will return false in hasNext function.

## What changes were proposed in this pull request?
wait script process exiting before output iterator finish.

## How was this patch tested?
manual test:

1. Add Thread.sleep(1000 * 600) before assign for _exception.
2. structure a python script witch will throw exception like follow:
test.py
```import sys 
for line in sys.stdin:   
  raise Exception('error') 
  print line
```
3. use script created in step 2 in transform.
```spark-sql>add files /path_to/test.py;```
```spark-sql>select transform(id) using 'python test.py' as id from city;```
The result is that spark will end successfully.",spark,apache,liutang123,17537020,MDQ6VXNlcjE3NTM3MDIw,https://avatars3.githubusercontent.com/u/17537020?v=4,,https://api.github.com/users/liutang123,https://github.com/liutang123,https://api.github.com/users/liutang123/followers,https://api.github.com/users/liutang123/following{/other_user},https://api.github.com/users/liutang123/gists{/gist_id},https://api.github.com/users/liutang123/starred{/owner}{/repo},https://api.github.com/users/liutang123/subscriptions,https://api.github.com/users/liutang123/orgs,https://api.github.com/users/liutang123/repos,https://api.github.com/users/liutang123/events{/privacy},https://api.github.com/users/liutang123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21164,https://github.com/apache/spark/pull/21164,https://github.com/apache/spark/pull/21164.diff,https://github.com/apache/spark/pull/21164.patch
427,https://api.github.com/repos/apache/spark/issues/21156,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21156/labels{/name},https://api.github.com/repos/apache/spark/issues/21156/comments,https://api.github.com/repos/apache/spark/issues/21156/events,https://github.com/apache/spark/pull/21156,317665993,MDExOlB1bGxSZXF1ZXN0MTg0MDY0OTg1,21156,[SPARK-24087][SQL] Avoid shuffle when join keys are a super-set of bucket keys,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2018-04-25T14:56:29Z,2019-09-16T18:22:40Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

To improve the bucket join, when join keys are a super-set of bucket keys, we should avoid shuffle.

## How was this patch tested?

Enable ignored test.
",spark,apache,yucai,2989575,MDQ6VXNlcjI5ODk1NzU=,https://avatars0.githubusercontent.com/u/2989575?v=4,,https://api.github.com/users/yucai,https://github.com/yucai,https://api.github.com/users/yucai/followers,https://api.github.com/users/yucai/following{/other_user},https://api.github.com/users/yucai/gists{/gist_id},https://api.github.com/users/yucai/starred{/owner}{/repo},https://api.github.com/users/yucai/subscriptions,https://api.github.com/users/yucai/orgs,https://api.github.com/users/yucai/repos,https://api.github.com/users/yucai/events{/privacy},https://api.github.com/users/yucai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21156,https://github.com/apache/spark/pull/21156,https://github.com/apache/spark/pull/21156.diff,https://github.com/apache/spark/pull/21156.patch
428,https://api.github.com/repos/apache/spark/issues/21150,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21150/labels{/name},https://api.github.com/repos/apache/spark/issues/21150/comments,https://api.github.com/repos/apache/spark/issues/21150/events,https://github.com/apache/spark/pull/21150,317510050,MDExOlB1bGxSZXF1ZXN0MTgzOTQ2NjA0,21150,[SPARK-24075][MESOS] Option to limit number of retries for a supervised driver,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-04-25T07:32:27Z,2019-09-16T18:21:57Z,,NONE,"## What changes were proposed in this pull request?

* Spark drivers run on mesos with supervise enabled will try the driver indefinitely on failures. This PR is to optionally limit the number of retries to a configurable number.
* Introducing sparkConf ""spark.mesos.driver.supervise.maxRetries"" which limits the number of times the driver can be retried. The default behavior is for the driver to be retried indefinitely.
* When the check is made to see if supervise is enabled and the job has to be retried, an additional check is made to see if the allowable retries have been exceeded.
* Added unit tests for method hasDriverExceededRetries.
* Added documentation for ""spark.mesos.driver.supervise.maxRetries"".

## How was this patch tested?

Added unit tests

Built spark package, dockerized it and deployed it as a service on Mesos. Once spark service was running on mesos, a series of drivers were submitted

1. With supervise disabled, ran a successful driver
2. With supervise disabled, ran a failure driver
3. With supervise enabled, ran a successful driver
4. With supervise enabled and without setting ""spark.mesos.driver.supervise.maxRetries"", ran failure driver
5. With supervise enabled and setting ""spark.mesos.driver.supervise.maxRetries=1"", ran a failure driver
6. With supervise enabled and setting ""spark.mesos.driver.supervise.maxRetries=3"", ran a failure driver
",spark,apache,nyogesh,647972,MDQ6VXNlcjY0Nzk3Mg==,https://avatars2.githubusercontent.com/u/647972?v=4,,https://api.github.com/users/nyogesh,https://github.com/nyogesh,https://api.github.com/users/nyogesh/followers,https://api.github.com/users/nyogesh/following{/other_user},https://api.github.com/users/nyogesh/gists{/gist_id},https://api.github.com/users/nyogesh/starred{/owner}{/repo},https://api.github.com/users/nyogesh/subscriptions,https://api.github.com/users/nyogesh/orgs,https://api.github.com/users/nyogesh/repos,https://api.github.com/users/nyogesh/events{/privacy},https://api.github.com/users/nyogesh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21150,https://github.com/apache/spark/pull/21150,https://github.com/apache/spark/pull/21150.diff,https://github.com/apache/spark/pull/21150.patch
429,https://api.github.com/repos/apache/spark/issues/21148,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21148/labels{/name},https://api.github.com/repos/apache/spark/issues/21148/comments,https://api.github.com/repos/apache/spark/issues/21148/events,https://github.com/apache/spark/pull/21148,317474769,MDExOlB1bGxSZXF1ZXN0MTgzOTIyMTIw,21148,[SPARK-24079][SQL] Update the nullability of Join output based on inferred predicates,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2018-04-25T04:26:46Z,2019-09-16T19:19:06Z,,MEMBER,"## What changes were proposed in this pull request?
This pr added code to update the nullability of `Join.output` based on inferred predicates in `InferFiltersFromConstraints`. In the master, a logical `Join` node does not respect the nullability that the optimizer rule `InferFiltersFromConstraints` might change when inferred predicates have `IsNotNull`,
e.g.,

```
scala> val df1 = Seq((Some(1), Some(2))).toDF(""k"", ""v0"")
scala> val df2 = Seq((Some(1), Some(3))).toDF(""k"", ""v1"")
scala> val joinedDf = df1.join(df2, df1(""k"") === df2(""k""), ""inner"")
scala> joinedDf.explain
== Physical Plan ==
*(2) BroadcastHashJoin [k#83], [k#92], Inner, BuildRight
:- *(2) Project [_1#80 AS k#83, _2#81 AS v0#84]
:  +- *(2) Filter isnotnull(_1#80)
:     +- LocalTableScan [_1#80, _2#81]
+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   +- *(1) Project [_1#89 AS k#92, _2#90 AS v1#93]
      +- *(1) Filter isnotnull(_1#89)
         +- LocalTableScan [_1#89, _2#90]

scala> joinedDf.queryExecution.optimizedPlan.output.map(_.nullable)
res15: Seq[Boolean] = List(true, true, true, true)
```

But, these `nullable` values should be:

```
scala> joinedDf.queryExecution.optimizedPlan.output.map(_.nullable)
res15: Seq[Boolean] = List(false, true, false, true)
```

This ticket comes from the previous discussion: https://github.com/apache/spark/pull/18576#pullrequestreview-107585997

## How was this patch tested?
Added tests in `UpdateNullabilityInAttributeReferencesSuite`.
",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21148,https://github.com/apache/spark/pull/21148,https://github.com/apache/spark/pull/21148.diff,https://github.com/apache/spark/pull/21148.patch
430,https://api.github.com/repos/apache/spark/issues/21109,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21109/labels{/name},https://api.github.com/repos/apache/spark/issues/21109/comments,https://api.github.com/repos/apache/spark/issues/21109/events,https://github.com/apache/spark/pull/21109,316043961,MDExOlB1bGxSZXF1ZXN0MTgyOTAxMDQz,21109,[SPARK-24020][SQL] Sort-merge join inner range optimization,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,65,2018-04-19T21:16:19Z,2019-09-19T15:21:07Z,,NONE,"## JIRA description

The problem we are solving is the case where you have two big tables partitioned by X column, but also sorted within partitions by Y column and you need to calculate an expensive function on the joined rows, which reduces the number of output rows (e.g. condition based on a spatial distance calculation). But you could theoretically reduce the number of joined rows for which the calculation itself is performed by using a range condition on the Y column. Something like this:

`... WHERE t1.X = t2.X AND t1.Y BETWEEN t2.Y - d AND t2.Y + d AND <function calculation...>`

However, during a sort-merge join with this range condition specified, Spark will first cross-join all the rows with the same X value and only then will it try to apply the range condition and any function calculations. This happens because, inside the generated sort-merge join (SMJ) code, these extra conditions are put in the same block with the function being calculated and there is no way to evaluate these conditions before reading all the rows to be checked into memory (into an `ExternalAppendOnlyUnsafeRowArray`). If the two tables have a large number of rows per X, this can result in a huge number of calculations and a huge number of rows in executor memory, which can be unfeasible. 

We therefore propose a change to the sort-merge join so that, when these extra conditions are specified, a queue is used instead of the `ExternalAppendOnlyUnsafeRowArray` class. This queue is then used as a moving window through the values from the right relation as the left row changes. You could call this a combination of an equi-join and a theta join; in literature it is sometimes called an ‚Äúepsilon join‚Äù. We call it a ""sort-merge inner range join"".

This design uses much less memory (not all rows with the same values of X need to be loaded into memory at once) and requires a much lower number of comparisons (the validity of this statement depends on the actual data and conditions used). 

The optimization should be triggered automatically when an equi-join expression is present AND lower and upper range conditions on a secondary column are specified. If the tables aren't sorted by both columns, appropriate sorts should be added.

To limit the impact of this change we also propose adding a new parameter (tentatively named `spark.sql.join.smj.useInnerRangeOptimization`) which could be used to switch off the optimization entirely.

## What changes were proposed in this pull request?

The main changes are made to these classes:

`ExtractEquiJoinKeys` ‚Äì a pattern that needs to be extended to be able to recognize the case where a simple range condition with lower and upper limits is used on a secondary column (a column not included in the equi-join condition). The pattern also needs to extract the information later required for code generation etc.

`ExternalAppendOnlyUnsafeRowArray` ‚Äì changed so that it can function as a queue too. The rows need to be removed and added to/from the structure as the left key (X) changes, or the left secondary value (Y) changes, so the structure needs to be a queue. The class is no longer ""append only"", but I didn't want to change too many things.

`JoinSelection` ‚Äì a strategy that uses `ExtractEquiJoinKeys` and needs to be aware of the extracted range conditions

`SortMergeJoinExec` ‚Äì the main implementation of the optimization. Needs to support two code paths: 
    ‚Ä¢ when whole-stage code generation is turned off (method `doExecute`, which uses `sortMergeJoinInnerRangeScanner`) 
    ‚Ä¢ when whole-stage code generation is turned on (methods `doProduce` and `genScanner`)
`SortMergeJoinInnerRangeScanner` ‚Äì implements the SMJ with inner-range optimization in the case when whole-stage codegen is turned off

## How was this patch tested?
Unit tests (`InnerRangeSuite.scala`, `JoinSuite.scala`, `ExternalAppendOnlyUnsafeRowArraySuite`)

Performance tests in `JoinBenchmark`. The tests show 8x improvement over non-optimized code. Although, it should be noted that the results depend on the exact range conditions and the calculations performed on each matched row. 
In our case, we were not able to cross-match two rather large datasets (1.2 billion rows x 800 million rows) without this optimization. With the optimization, the cross-match finishes in less than 2 minutes.
",spark,apache,zecevicp,9744763,MDQ6VXNlcjk3NDQ3NjM=,https://avatars2.githubusercontent.com/u/9744763?v=4,,https://api.github.com/users/zecevicp,https://github.com/zecevicp,https://api.github.com/users/zecevicp/followers,https://api.github.com/users/zecevicp/following{/other_user},https://api.github.com/users/zecevicp/gists{/gist_id},https://api.github.com/users/zecevicp/starred{/owner}{/repo},https://api.github.com/users/zecevicp/subscriptions,https://api.github.com/users/zecevicp/orgs,https://api.github.com/users/zecevicp/repos,https://api.github.com/users/zecevicp/events{/privacy},https://api.github.com/users/zecevicp/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21109,https://github.com/apache/spark/pull/21109,https://github.com/apache/spark/pull/21109.diff,https://github.com/apache/spark/pull/21109.patch
431,https://api.github.com/repos/apache/spark/issues/21075,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21075/labels{/name},https://api.github.com/repos/apache/spark/issues/21075/comments,https://api.github.com/repos/apache/spark/issues/21075/events,https://github.com/apache/spark/pull/21075,314465099,MDExOlB1bGxSZXF1ZXN0MTgxNzI5MjYy,21075,[SPARK-23988][MESOS] Improve handling of appResource in mesos dispatcher when using Docker,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-04-16T00:02:48Z,2019-10-03T16:43:05Z,,NONE,"Improve/fix handling of appResource for mesos dispatcher when using docker

Tested with new unit tests as well as manually on private mesos cluster
",spark,apache,pmackles,1900673,MDQ6VXNlcjE5MDA2NzM=,https://avatars3.githubusercontent.com/u/1900673?v=4,,https://api.github.com/users/pmackles,https://github.com/pmackles,https://api.github.com/users/pmackles/followers,https://api.github.com/users/pmackles/following{/other_user},https://api.github.com/users/pmackles/gists{/gist_id},https://api.github.com/users/pmackles/starred{/owner}{/repo},https://api.github.com/users/pmackles/subscriptions,https://api.github.com/users/pmackles/orgs,https://api.github.com/users/pmackles/repos,https://api.github.com/users/pmackles/events{/privacy},https://api.github.com/users/pmackles/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21075,https://github.com/apache/spark/pull/21075,https://github.com/apache/spark/pull/21075.diff,https://github.com/apache/spark/pull/21075.patch
432,https://api.github.com/repos/apache/spark/issues/21012,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21012/labels{/name},https://api.github.com/repos/apache/spark/issues/21012/comments,https://api.github.com/repos/apache/spark/issues/21012/events,https://github.com/apache/spark/pull/21012,312652208,MDExOlB1bGxSZXF1ZXN0MTgwNDAwNzky,21012,[SPARK-23890][SQL] Support CHANGE COLUMN to add nested fields to structs,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2018-04-09T19:15:16Z,2019-09-16T18:25:02Z,,NONE,"## What changes were proposed in this pull request?

SPARK-23525 added the ability to `ALTER TABLE CHANGE COLUMN` for simple COMMENT only changes.  For safety, column changes are generally restricted.  However, `ALTER TABLE ADD COLUMN` is safe and allowed.  In order to add columns to a nested struct type, we must run an `ALTER TABLE CHANGE COLUMN` command, since struct type DDL look like single top level columns with a complex type.

Given on origin column declared like
```
nested struct<s1:string>
```

This patch allows users to issue SQL DDL like:

```
ALTER TABLE t1 CHANGE COLUMN nested nested struct<s1:string,s2:string>
```

to add sub columns to a struct.  It does this by recursing over StructTypes, and comparing dataTypes for each shared field between the origin table and the new destination type.

## How was this patch tested?

`testChangeColumn` in DDLSuite.scala and was amended and tests were added
to HiveDDLSuite.scala  to include tests for altering struct type columns.
",spark,apache,ottomata,1626054,MDQ6VXNlcjE2MjYwNTQ=,https://avatars2.githubusercontent.com/u/1626054?v=4,,https://api.github.com/users/ottomata,https://github.com/ottomata,https://api.github.com/users/ottomata/followers,https://api.github.com/users/ottomata/following{/other_user},https://api.github.com/users/ottomata/gists{/gist_id},https://api.github.com/users/ottomata/starred{/owner}{/repo},https://api.github.com/users/ottomata/subscriptions,https://api.github.com/users/ottomata/orgs,https://api.github.com/users/ottomata/repos,https://api.github.com/users/ottomata/events{/privacy},https://api.github.com/users/ottomata/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21012,https://github.com/apache/spark/pull/21012,https://github.com/apache/spark/pull/21012.diff,https://github.com/apache/spark/pull/21012.patch
433,https://api.github.com/repos/apache/spark/issues/21006,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/21006/labels{/name},https://api.github.com/repos/apache/spark/issues/21006/comments,https://api.github.com/repos/apache/spark/issues/21006/events,https://github.com/apache/spark/pull/21006,312441111,MDExOlB1bGxSZXF1ZXN0MTgwMjQzODEy,21006,[SPARK-22256][MESOS] - Introduce spark.mesos.driver.memoryOverhead,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2018-04-09T08:35:54Z,2019-10-08T20:51:51Z,,NONE,"When running spark driver in a container such as when using the Mesos dispatcher service, we need to apply the same rules as for executors in order to avoid the JVM going over the allotted limit and then killed.

Tested manually on spark 2.3 branch

",spark,apache,pmackles,1900673,MDQ6VXNlcjE5MDA2NzM=,https://avatars3.githubusercontent.com/u/1900673?v=4,,https://api.github.com/users/pmackles,https://github.com/pmackles,https://api.github.com/users/pmackles/followers,https://api.github.com/users/pmackles/following{/other_user},https://api.github.com/users/pmackles/gists{/gist_id},https://api.github.com/users/pmackles/starred{/owner}{/repo},https://api.github.com/users/pmackles/subscriptions,https://api.github.com/users/pmackles/orgs,https://api.github.com/users/pmackles/repos,https://api.github.com/users/pmackles/events{/privacy},https://api.github.com/users/pmackles/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/21006,https://github.com/apache/spark/pull/21006,https://github.com/apache/spark/pull/21006.diff,https://github.com/apache/spark/pull/21006.patch
434,https://api.github.com/repos/apache/spark/issues/20999,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20999/labels{/name},https://api.github.com/repos/apache/spark/issues/20999/comments,https://api.github.com/repos/apache/spark/issues/20999/events,https://github.com/apache/spark/pull/20999,312038486,MDExOlB1bGxSZXF1ZXN0MTc5OTk1NDA3,20999,[SPARK-14922][SPARK-17732][SPARK-23866][SQL] Support partition filters in ALTER TABLE DROP PARTITION,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,41,2018-04-06T16:26:34Z,2019-10-28T09:42:35Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Hive has been supporting for a while the ability of dropping partitions using any kind of comparison operator on them. Spark so far is supporting only dropping partitions by exact values. For instance, Spark doesn't support:

```
ALTER TABLE mytable DROP PARTITION(mydate < '2018-04-06')
```

The PR adds the support to this syntax too.

The PR takes input from the effort in #19691 by @DazhuangSu. As such, this closes #19691.

## How was this patch tested?

UTs to be added
",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20999,https://github.com/apache/spark/pull/20999,https://github.com/apache/spark/pull/20999.diff,https://github.com/apache/spark/pull/20999.patch
435,https://api.github.com/repos/apache/spark/issues/20992,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20992/labels{/name},https://api.github.com/repos/apache/spark/issues/20992/comments,https://api.github.com/repos/apache/spark/issues/20992/events,https://github.com/apache/spark/pull/20992,311871289,MDExOlB1bGxSZXF1ZXN0MTc5ODcxNzMx,20992,[SPARK-23779][SQL] TaskMemoryManager and UnsafeSorter related classes use MemoryBlock,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2018-04-06T07:21:58Z,2019-09-16T19:19:07Z,,MEMBER,"## What changes were proposed in this pull request?

This PR tries to use `MemoryBlock` in `TaskMemoryManager` and `UnsafeSorter` related classes. There are two advantages to use `MemoryBlock`.

1. Has clean API calls rather than using a Java array or `PlatformMemory`
2. Improve runtime performance of memory access instead of using `Object` with `Platform.get/put...`.

** ToDo: Add benchmark result **

## How was this patch tested?

Used existing UTs",spark,apache,kiszk,1315079,MDQ6VXNlcjEzMTUwNzk=,https://avatars2.githubusercontent.com/u/1315079?v=4,,https://api.github.com/users/kiszk,https://github.com/kiszk,https://api.github.com/users/kiszk/followers,https://api.github.com/users/kiszk/following{/other_user},https://api.github.com/users/kiszk/gists{/gist_id},https://api.github.com/users/kiszk/starred{/owner}{/repo},https://api.github.com/users/kiszk/subscriptions,https://api.github.com/users/kiszk/orgs,https://api.github.com/users/kiszk/repos,https://api.github.com/users/kiszk/events{/privacy},https://api.github.com/users/kiszk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20992,https://github.com/apache/spark/pull/20992,https://github.com/apache/spark/pull/20992.diff,https://github.com/apache/spark/pull/20992.patch
436,https://api.github.com/repos/apache/spark/issues/20974,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20974/labels{/name},https://api.github.com/repos/apache/spark/issues/20974/comments,https://api.github.com/repos/apache/spark/issues/20974/events,https://github.com/apache/spark/pull/20974,311085832,MDExOlB1bGxSZXF1ZXN0MTc5MjgyNTMw,20974,[SPARK-23862][SQL] Spark ExpressionEncoder should support java enum type in scala,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-04-04T04:59:30Z,2019-09-18T04:36:29Z,,NONE,"
## What changes were proposed in this pull request?

In SPARK-21255, spark upstream adds support for creating encoders for java enum types, but the support is only added to Java API(for enum working within Java Beans). Since the java enum can come from third-party java library, we have use case that requires 
1. using java enum types as field of scala case class
2. using java enum as the type T in Dataset[T]

Spark ExpressionEncoder already supports ser/de many java types in ScalaReflection, so we propose to add support for java enum as well, as a follow up of SPARK-21255.


## How was this patch tested?

Tested the patch in our production cluster.  Added unit test.
Since:
1. it is not possible to define a java enum in scala directly, since the defined enum class in scala will miss method like valueOf which is added by java compiler
2. it is not possible to define a test enum java class and use in scala test because the compilation of single scala test(-DwildcardSuites=org.apache.spark.sql.DatasetSuite) won't compile the test java class first

As a result, I use the Spark SQL public java enum API(SaveMode.java) in the test. Please advise if there is a better way to test 

",spark,apache,fangshil,2416746,MDQ6VXNlcjI0MTY3NDY=,https://avatars0.githubusercontent.com/u/2416746?v=4,,https://api.github.com/users/fangshil,https://github.com/fangshil,https://api.github.com/users/fangshil/followers,https://api.github.com/users/fangshil/following{/other_user},https://api.github.com/users/fangshil/gists{/gist_id},https://api.github.com/users/fangshil/starred{/owner}{/repo},https://api.github.com/users/fangshil/subscriptions,https://api.github.com/users/fangshil/orgs,https://api.github.com/users/fangshil/repos,https://api.github.com/users/fangshil/events{/privacy},https://api.github.com/users/fangshil/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20974,https://github.com/apache/spark/pull/20974,https://github.com/apache/spark/pull/20974.diff,https://github.com/apache/spark/pull/20974.patch
437,https://api.github.com/repos/apache/spark/issues/20958,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20958/labels{/name},https://api.github.com/repos/apache/spark/issues/20958/comments,https://api.github.com/repos/apache/spark/issues/20958/events,https://github.com/apache/spark/pull/20958,310435548,MDExOlB1bGxSZXF1ZXN0MTc4ODAyNDI0,20958,[SPARK-23844][SS] Fix socket source honors recovered offsets issue,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2018-04-02T09:07:52Z,2019-09-19T02:21:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Socket source in Structured streaming doesn't support recovering from checkpoint, but still the framework will pass the recovered offsets into this source, this will lead to unexpected error in socket source (because of wrong offsets, mentioned in JIRA).

To fix this issue, adding a new option ""failonrecovery"". By default it is true, which means socket source will be failed when using recovered offsets. if it is set to false, then socket source will ignore unexpected offsets and keep processing data, which will potentially have some data loss issue.

## How was this patch tested?

Add UT and verified in local cluster.
",spark,apache,jerryshao,850797,MDQ6VXNlcjg1MDc5Nw==,https://avatars2.githubusercontent.com/u/850797?v=4,,https://api.github.com/users/jerryshao,https://github.com/jerryshao,https://api.github.com/users/jerryshao/followers,https://api.github.com/users/jerryshao/following{/other_user},https://api.github.com/users/jerryshao/gists{/gist_id},https://api.github.com/users/jerryshao/starred{/owner}{/repo},https://api.github.com/users/jerryshao/subscriptions,https://api.github.com/users/jerryshao/orgs,https://api.github.com/users/jerryshao/repos,https://api.github.com/users/jerryshao/events{/privacy},https://api.github.com/users/jerryshao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20958,https://github.com/apache/spark/pull/20958,https://github.com/apache/spark/pull/20958.diff,https://github.com/apache/spark/pull/20958.patch
438,https://api.github.com/repos/apache/spark/issues/20935,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20935/labels{/name},https://api.github.com/repos/apache/spark/issues/20935/comments,https://api.github.com/repos/apache/spark/issues/20935/events,https://github.com/apache/spark/pull/20935,309714441,MDExOlB1bGxSZXF1ZXN0MTc4Mjk3ODM2,20935,[SPARK-23819][SQL] Fix InMemoryTableScanExec complex type pruning,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2018-03-29T11:13:21Z,2019-10-09T03:41:23Z,,NONE,"## What changes were proposed in this pull request?

This PR allows recording of upper/lower bound values in ColumnStats if the data type is orderable.

## How was this patch tested?
Added tests to ColumnStatsSuite and InMemoryColumnarQuerySuite.

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,pwoody,11271328,MDQ6VXNlcjExMjcxMzI4,https://avatars2.githubusercontent.com/u/11271328?v=4,,https://api.github.com/users/pwoody,https://github.com/pwoody,https://api.github.com/users/pwoody/followers,https://api.github.com/users/pwoody/following{/other_user},https://api.github.com/users/pwoody/gists{/gist_id},https://api.github.com/users/pwoody/starred{/owner}{/repo},https://api.github.com/users/pwoody/subscriptions,https://api.github.com/users/pwoody/orgs,https://api.github.com/users/pwoody/repos,https://api.github.com/users/pwoody/events{/privacy},https://api.github.com/users/pwoody/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20935,https://github.com/apache/spark/pull/20935,https://github.com/apache/spark/pull/20935.diff,https://github.com/apache/spark/pull/20935.patch
439,https://api.github.com/repos/apache/spark/issues/20930,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20930/labels{/name},https://api.github.com/repos/apache/spark/issues/20930/comments,https://api.github.com/repos/apache/spark/issues/20930/events,https://github.com/apache/spark/pull/20930,309619229,MDExOlB1bGxSZXF1ZXN0MTc4MjI2ODIz,20930,[SPARK-23811][Core] FetchFailed comes before Success of same task will cause child stage never succeed,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2018-03-29T04:53:25Z,2019-09-16T19:20:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This is a bug caused by abnormal scenario describe below:

1. ShuffleMapTask 1.0 running, this task will fetch data from ExecutorA
2. ExecutorA Lost, trigger `mapOutputTracker.removeOutputsOnExecutor(execId)` , shuffleStatus changed.
3. Speculative ShuffleMapTask 1.1 start, got a FetchFailed immediately.
4. ShuffleMapTask 1.0 finally succeed, but because of 1.1's FetchFailed, stage still mark as failed stage.
5. ShuffleMapTask 1 is the last task of its stage, ShuffleMapTask 1.0's success event triggered `mapOutputTracker.registerMapOutput`, this is also the root case for this scenario.
6. This ShuffleMapStage will always skipped because of there's no missing task DAGScheduler can get, and finally this will cause its child stage never succeed.

I apply the detailed screenshots in jira comments.

## How was this patch tested?

Add a new UT in `TaskSetManagerSuite`
",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20930,https://github.com/apache/spark/pull/20930,https://github.com/apache/spark/pull/20930.diff,https://github.com/apache/spark/pull/20930.patch
440,https://api.github.com/repos/apache/spark/issues/20874,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20874/labels{/name},https://api.github.com/repos/apache/spark/issues/20874/comments,https://api.github.com/repos/apache/spark/issues/20874/events,https://github.com/apache/spark/pull/20874,307238598,MDExOlB1bGxSZXF1ZXN0MTc2NDcxMDcx,20874,[SPARK-23763][SQL] OffHeapColumnVector uses MemoryBlock,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2018-03-21T13:12:29Z,2019-09-16T19:20:07Z,,MEMBER,"## What changes were proposed in this pull request?

This PR tries to use `MemoryBlock` in `OffHeapColumnVector`. There are two advantages to use `MemoryBlock`.

1. Has clean API calls rather than using a Java array or `PlatformMemory`
2. Improve runtime performance of memory access instead of using `Object` with `Platform.get/put...`.

** TODO: I will add result of ColumnarBatchBenchmark **

## How was this patch tested?

Used existing UTs",spark,apache,kiszk,1315079,MDQ6VXNlcjEzMTUwNzk=,https://avatars2.githubusercontent.com/u/1315079?v=4,,https://api.github.com/users/kiszk,https://github.com/kiszk,https://api.github.com/users/kiszk/followers,https://api.github.com/users/kiszk/following{/other_user},https://api.github.com/users/kiszk/gists{/gist_id},https://api.github.com/users/kiszk/starred{/owner}{/repo},https://api.github.com/users/kiszk/subscriptions,https://api.github.com/users/kiszk/orgs,https://api.github.com/users/kiszk/repos,https://api.github.com/users/kiszk/events{/privacy},https://api.github.com/users/kiszk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20874,https://github.com/apache/spark/pull/20874,https://github.com/apache/spark/pull/20874.diff,https://github.com/apache/spark/pull/20874.patch
441,https://api.github.com/repos/apache/spark/issues/20868,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20868/labels{/name},https://api.github.com/repos/apache/spark/issues/20868/comments,https://api.github.com/repos/apache/spark/issues/20868/events,https://github.com/apache/spark/pull/20868,307078657,MDExOlB1bGxSZXF1ZXN0MTc2MzUyOTMy,20868,[SPARK-23750][SQL] Inner Join Elimination based on Informational RI constraints,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2018-03-20T23:49:05Z,2019-09-16T19:21:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This transformation detects RI joins and eliminates the parent/PK table if none of its columns, other than the PK columns, are referenced in the query. 

**Example:**

```SQL
select fact.c1
from fact, dim1, dim2
where fact.c1 = dim1.pk /* FK = PK */ and
      fact.c2 = dim2.pk /* FK = PK */ and
      dim1.pk = 10 and
      dim2.pk like ‚Äòabc%‚Äô
```

**Internal optimized query after join elimination:**

```SQL
select fact.c1
from fact 
where fact.c1 = 10 and fact.c2 like ‚Äòabc%‚Äô
```

The transformation will apply under the following restrictions:

- No columns from the parent table are retrieved.
- No columns from the parent table other than the PK columns are referenced in the predicates.
- Conservatively, only allow local predicates on PK columns or equi-joins between PK columns and other tables.
- The join is directly above a base table access i.e. no aliases or other expressions above base table access
- Other restrictions on string data types

## How was this patch tested?

A new test suite HiveRIJElimSuite.scala was introduced.

",spark,apache,ioana-delaney,17709782,MDQ6VXNlcjE3NzA5Nzgy,https://avatars1.githubusercontent.com/u/17709782?v=4,,https://api.github.com/users/ioana-delaney,https://github.com/ioana-delaney,https://api.github.com/users/ioana-delaney/followers,https://api.github.com/users/ioana-delaney/following{/other_user},https://api.github.com/users/ioana-delaney/gists{/gist_id},https://api.github.com/users/ioana-delaney/starred{/owner}{/repo},https://api.github.com/users/ioana-delaney/subscriptions,https://api.github.com/users/ioana-delaney/orgs,https://api.github.com/users/ioana-delaney/repos,https://api.github.com/users/ioana-delaney/events{/privacy},https://api.github.com/users/ioana-delaney/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20868,https://github.com/apache/spark/pull/20868,https://github.com/apache/spark/pull/20868.diff,https://github.com/apache/spark/pull/20868.patch
442,https://api.github.com/repos/apache/spark/issues/20865,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20865/labels{/name},https://api.github.com/repos/apache/spark/issues/20865/comments,https://api.github.com/repos/apache/spark/issues/20865/events,https://github.com/apache/spark/pull/20865,306825647,MDExOlB1bGxSZXF1ZXN0MTc2MTYwMjU3,20865,[SPARK-23542] The exists action shoule be further optimized in logical plan,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-03-20T11:26:18Z,2019-09-16T19:21:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

(Please fill in changes proposed in this fix)
The optimized logical plan of query `select * from tt1 where exists (select *  from tt2  where tt1.i = tt2.i)` is

> == Optimized Logical Plan ==
Join LeftSemi, (i#14 = i#16)
  :- HiveTableRelation `default`.`tt1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [i#14, s#15]
 +- Project [i#16]
  +- HiveTableRelation `default`.`tt2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [i#16, s#17]

 The `exists` action will be rewritten as semi jion. But i the query of `select * from tt1 left semi join tt2 on tt2.i = tt1.i`, the optimized logical plan is :

> == Optimized Logical Plan ==
Join LeftSemi, (i#22 = i#20)
:- `Filter isnotnull`(i#20)
: +- HiveTableRelation `default`.`tt1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [i#20, s#21]
+- Project [i#22]
+- HiveTableRelation `default`.`tt2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [i#22, s#23]

 So i think the  optimized logical plan of 'select * from tt1 where exists (select *  from tt2  where tt1.i = tt2.i);` should be further optimization.
## How was this patch tested?

(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)

With this patch, the  optimized logical plan of 'select * from tt1 where exists (select *  from tt2  where tt1.i = tt2.i);`  is:

> == Optimized Logical Plan ==
Join LeftSemi, (i#14 = i#16)
:- `Filter isnotnull(i#14)`
  : +- HiveTableRelation `default`.`tt1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [i#14, s#15]
+- Project [i#16]
 :- `Filter isnotnull(i#16)`
  +- HiveTableRelation `default`.`tt2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [i#16, s#17]

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,KaiXinXiaoLei,9440626,MDQ6VXNlcjk0NDA2MjY=,https://avatars1.githubusercontent.com/u/9440626?v=4,,https://api.github.com/users/KaiXinXiaoLei,https://github.com/KaiXinXiaoLei,https://api.github.com/users/KaiXinXiaoLei/followers,https://api.github.com/users/KaiXinXiaoLei/following{/other_user},https://api.github.com/users/KaiXinXiaoLei/gists{/gist_id},https://api.github.com/users/KaiXinXiaoLei/starred{/owner}{/repo},https://api.github.com/users/KaiXinXiaoLei/subscriptions,https://api.github.com/users/KaiXinXiaoLei/orgs,https://api.github.com/users/KaiXinXiaoLei/repos,https://api.github.com/users/KaiXinXiaoLei/events{/privacy},https://api.github.com/users/KaiXinXiaoLei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20865,https://github.com/apache/spark/pull/20865,https://github.com/apache/spark/pull/20865.diff,https://github.com/apache/spark/pull/20865.patch
443,https://api.github.com/repos/apache/spark/issues/20854,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20854/labels{/name},https://api.github.com/repos/apache/spark/issues/20854/comments,https://api.github.com/repos/apache/spark/issues/20854/events,https://github.com/apache/spark/pull/20854,306236218,MDExOlB1bGxSZXF1ZXN0MTc1NzQ4NzU3,20854,[SPARK-23712][SQL] Interpreted UnsafeRowJoiner [WIP],"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2018-03-18T12:31:32Z,2019-09-16T19:22:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This PR adds an interpreted version of `UnsafeRowJoiner` to Spark SQL.

Its performance is almost to par with the code generated `UnsafeRowJoiner`. There seems to be an overhead of 10ns per call. It might be an idea to not use code generation at all for an `UnsafeRowJoiner`

## How was this patch tested?
Modified existing row joiner tests.
",spark,apache,hvanhovell,9616802,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20854,https://github.com/apache/spark/pull/20854,https://github.com/apache/spark/pull/20854.diff,https://github.com/apache/spark/pull/20854.patch
444,https://api.github.com/repos/apache/spark/issues/20826,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20826/labels{/name},https://api.github.com/repos/apache/spark/issues/20826/comments,https://api.github.com/repos/apache/spark/issues/20826/events,https://github.com/apache/spark/pull/20826,305321302,MDExOlB1bGxSZXF1ZXN0MTc1MDc5MzQy,20826,[SPARK-2489][SQL] Support Parquet's optional fixed_len_byte_array,[],open,False,,[],,25,2018-03-14T20:43:30Z,2019-09-17T05:26:06Z,,NONE,"## What changes were proposed in this pull request?
This PR adds support for reading Parquet FIXED_LENGTH_BYTE_ARRAYs as a Binary column if no OriginalType is specified. Parquet-avro writes the Avro fixed type as a Parquet FIXED_LENGTH_BYTE_ARRAY type. Currently when trying to load Parquet files with a column of this type with Spark SQL it throws an exception similar to the following:

```
Caused by: org.apache.spark.sql.AnalysisException: Illegal Parquet type: FIXED_LEN_BYTE_ARRAY;
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.illegalType$1(ParquetSchemaConverter.scala:108)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertPrimitiveField(ParquetSchemaConverter.scala:177)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:90)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter$$anonfun$2.apply(ParquetSchemaConverter.scala:72)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter$$anonfun$2.apply(ParquetSchemaConverter.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.org$apache$spark$sql$execution$datasources$parquet$ParquetToSparkSchemaConverter$$convert(ParquetSchemaConverter.scala:66)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convert(ParquetSchemaConverter.scala:63)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readSchemaFromFooter$2.apply(ParquetFileFormat.scala:642)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readSchemaFromFooter$2.apply(ParquetFileFormat.scala:642)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readSchemaFromFooter(ParquetFileFormat.scala:642)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:599)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:581)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

After this change Spark SQL is able to correctly load the Parquet files. There was a PR to fix this 3 years ago (https://github.com/apache/spark/pull/1737) however it was ultimately rejected as the committer went down the path of adding a new SQL Type specifically for FIXED_LENGTH_BYTE_ARRAYs and the maintainers believed this was too intrusive of a change. This PR simply defaults to Binary if no OriginalType is specified. A few updates were required to the VectorizedColumnReader to support Binary FIXED_LENGTH_BYTE_ARRAYs.

Note: All the changes to the gen-java/* files were generated by avro-tools-1.8.1 and the mostly documentation updates look to come from changes in the template avro-tools uses.

## How was this patch tested?

I added a fixed attribute to the AvroPrimitives and AvroOptionalPrimitives record types which are used by the ParquetAvroCompatibilitySuite. These values were populated by taking the same value as other type (""val_$i""), padding it to 8 bytes (the chosen fixed length), and storing it as the fixed type. I verified that before my fix the ""required primitives"" and ""optional primitives"" failed with the same exception we're seeing in our clusters. After my change the tests succeed with the expected results.
",spark,apache,aws-awinstan,30934849,MDQ6VXNlcjMwOTM0ODQ5,https://avatars1.githubusercontent.com/u/30934849?v=4,,https://api.github.com/users/aws-awinstan,https://github.com/aws-awinstan,https://api.github.com/users/aws-awinstan/followers,https://api.github.com/users/aws-awinstan/following{/other_user},https://api.github.com/users/aws-awinstan/gists{/gist_id},https://api.github.com/users/aws-awinstan/starred{/owner}{/repo},https://api.github.com/users/aws-awinstan/subscriptions,https://api.github.com/users/aws-awinstan/orgs,https://api.github.com/users/aws-awinstan/repos,https://api.github.com/users/aws-awinstan/events{/privacy},https://api.github.com/users/aws-awinstan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20826,https://github.com/apache/spark/pull/20826,https://github.com/apache/spark/pull/20826.diff,https://github.com/apache/spark/pull/20826.patch
445,https://api.github.com/repos/apache/spark/issues/20821,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20821/labels{/name},https://api.github.com/repos/apache/spark/issues/20821/comments,https://api.github.com/repos/apache/spark/issues/20821/events,https://github.com/apache/spark/pull/20821,305165799,MDExOlB1bGxSZXF1ZXN0MTc0OTYwNzE1,20821,[SPARK-23678][GraphX]  a more efficient partition strategy,"[{'id': 1406607243, 'node_id': 'MDU6TGFiZWwxNDA2NjA3MjQz', 'url': 'https://api.github.com/repos/apache/spark/labels/GRAPHX', 'name': 'GRAPHX', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2018-03-14T13:49:37Z,2019-09-16T18:24:44Z,,NONE,"## What changes were proposed in this pull request?

add a new partition strategy with several advantage:

1. nicer bound on vertex replication, sqrt(2 * numParts), which is about 23% reducing compare with EdgePartition2D  partition strategy, which has bound 2 * sqrt(numParts). This reduce the shuffle size in several operation such as aggregateMessage and triplets.
2. colocate all edges between two vertices regardless of direction. 
3. same work balance compared with EdgePartition2D  

## How was this patch tested?

manual tests, see [https://github.com/weiwee/edgePartitionTri/blob/master/EdgePartitionTriangle.ipynb](https://github.com/weiwee/edgePartitionTri/blob/master/EdgePartitionTriangle.ipynb)",spark,apache,weiwee,18696809,MDQ6VXNlcjE4Njk2ODA5,https://avatars3.githubusercontent.com/u/18696809?v=4,,https://api.github.com/users/weiwee,https://github.com/weiwee,https://api.github.com/users/weiwee/followers,https://api.github.com/users/weiwee/following{/other_user},https://api.github.com/users/weiwee/gists{/gist_id},https://api.github.com/users/weiwee/starred{/owner}{/repo},https://api.github.com/users/weiwee/subscriptions,https://api.github.com/users/weiwee/orgs,https://api.github.com/users/weiwee/repos,https://api.github.com/users/weiwee/events{/privacy},https://api.github.com/users/weiwee/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20821,https://github.com/apache/spark/pull/20821,https://github.com/apache/spark/pull/20821.diff,https://github.com/apache/spark/pull/20821.patch
446,https://api.github.com/repos/apache/spark/issues/20820,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20820/labels{/name},https://api.github.com/repos/apache/spark/issues/20820/comments,https://api.github.com/repos/apache/spark/issues/20820/events,https://github.com/apache/spark/pull/20820,305053321,MDExOlB1bGxSZXF1ZXN0MTc0ODc1ODc5,20820,[SPARK-23676][SQL]Support left join codegen in SortMergeJoinExec,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2018-03-14T07:47:12Z,2019-09-16T19:22:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This PR generates java code to directly complete the function of LeftOuter in `SortMergeJoinExec` without using an iterator. 
This PR improves runtime performance by this generates java code.

joinBenchmark result: **1.3x**
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Windows 7 6.1
Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz
left sort merge join:              Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------
left merge join wholestage=off          2439 / 2575          0.9        1163.0       1.0X
left merge join wholestage=on           1890 / 1904          1.1         901.1       1.3X
```
joinBenchmark program
```
    val N = 2 << 20
    runBenchmark(""left sort merge join"", N) {
      val df1 = sparkSession.range(N)
        .selectExpr(s""(id * 15485863) % ${N*10} as k1"")
      val df2 = sparkSession.range(N)
        .selectExpr(s""(id * 15485867) % ${N*10} as k2"")
      val df = df1.join(df2, col(""k1"") === col(""k2""), ""left"")
      assert(df.queryExecution.sparkPlan.find(_.isInstanceOf[SortMergeJoinExec]).isDefined)
      df.count()
```
code example
```
val df1 = spark.range(2 << 20).selectExpr(""id as k1"", ""id * 2 as v1"")
val df2 = spark.range(2 << 20).selectExpr(""id as k2"", ""id * 3 as v2"")
df1.join(df2, col(""k1"") === col(""k2"") && col(""v1"") < col(""v2""), ""left"").collect
```
Generated code
```
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage5(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=5
/* 006 */ final class GeneratedIteratorForCodegenStage5 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator smj_leftInput;
/* 010 */   private scala.collection.Iterator smj_rightInput;
/* 011 */   private InternalRow smj_leftRow;
/* 012 */   private InternalRow smj_rightRow;
/* 013 */   private long smj_value2;
/* 014 */   private org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray smj_matches;
/* 015 */   private long smj_value3;
/* 016 */   private long smj_value4;
/* 017 */   private long smj_value5;
/* 018 */   private long smj_value6;
/* 019 */   private boolean smj_isNull2;
/* 020 */   private long smj_value7;
/* 021 */   private boolean smj_isNull3;
/* 022 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder[] smj_mutableStateArray1 = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder[1];
/* 023 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] smj_mutableStateArray2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 024 */   private UnsafeRow[] smj_mutableStateArray = new UnsafeRow[1];
/* 025 */
/* 026 */   public GeneratedIteratorForCodegenStage5(Object[] references) {
/* 027 */     this.references = references;
/* 028 */   }
/* 029 */
/* 030 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 031 */     partitionIndex = index;
/* 032 */     this.inputs = inputs;
/* 033 */     smj_leftInput = inputs[0];
/* 034 */     smj_rightInput = inputs[1];
/* 035 */
/* 036 */     smj_matches = new org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray(2147483647, 2147483647);
/* 037 */     smj_mutableStateArray[0] = new UnsafeRow(4);
/* 038 */     smj_mutableStateArray1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(smj_mutableStateArray[0], 0);
/* 039 */     smj_mutableStateArray2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(smj_mutableStateArray1[0], 4);
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void writeJoinRows() throws java.io.IOException {
/* 044 */     smj_mutableStateArray2[0].zeroOutNullBytes();
/* 045 */
/* 046 */     smj_mutableStateArray2[0].write(0, smj_value4);
/* 047 */
/* 048 */     smj_mutableStateArray2[0].write(1, smj_value5);
/* 049 */
/* 050 */     if (smj_isNull2) {
/* 051 */       smj_mutableStateArray2[0].setNullAt(2);
/* 052 */     } else {
/* 053 */       smj_mutableStateArray2[0].write(2, smj_value6);
/* 054 */     }
/* 055 */
/* 056 */     if (smj_isNull3) {
/* 057 */       smj_mutableStateArray2[0].setNullAt(3);
/* 058 */     } else {
/* 059 */       smj_mutableStateArray2[0].write(3, smj_value7);
/* 060 */     }
/* 061 */     append(smj_mutableStateArray[0].copy());
/* 062 */
/* 063 */   }
/* 064 */
/* 065 */   private boolean findNextJoinRows(
/* 066 */     scala.collection.Iterator leftIter,
/* 067 */     scala.collection.Iterator rightIter) {
/* 068 */     smj_leftRow = null;
/* 069 */     int comp = 0;
/* 070 */     while (smj_leftRow == null) {
/* 071 */       if (!leftIter.hasNext()) return false;
/* 072 */       smj_leftRow = (InternalRow) leftIter.next();
/* 073 */
/* 074 */       long smj_value = smj_leftRow.getLong(0);
/* 075 */       if (false) {
/* 076 */         if (!smj_matches.isEmpty()) {
/* 077 */           smj_matches.clear();
/* 078 */         }
/* 079 */         return true;
/* 080 */       }
/* 081 */       if (!smj_matches.isEmpty()) {
/* 082 */         comp = 0;
/* 083 */         if (comp == 0) {
/* 084 */           comp = (smj_value > smj_value3 ? 1 : smj_value < smj_value3 ? -1 : 0);
/* 085 */         }
/* 086 */
/* 087 */         if (comp == 0) {
/* 088 */           return true;
/* 089 */         }
/* 090 */         smj_matches.clear();
/* 091 */       }
/* 092 */
/* 093 */       do {
/* 094 */         if (smj_rightRow == null) {
/* 095 */           if (!rightIter.hasNext()) {
/* 096 */             smj_value3 = smj_value;
/* 097 */             return true;
/* 098 */           }
/* 099 */           smj_rightRow = (InternalRow) rightIter.next();
/* 100 */
/* 101 */           long smj_value1 = smj_rightRow.getLong(0);
/* 102 */           if (false) {
/* 103 */             smj_rightRow = null;
/* 104 */             continue;
/* 105 */           }
/* 106 */           smj_value2 = smj_value1;
/* 107 */         }
/* 108 */
/* 109 */         comp = 0;
/* 110 */         if (comp == 0) {
/* 111 */           comp = (smj_value > smj_value2 ? 1 : smj_value < smj_value2 ? -1 : 0);
/* 112 */         }
/* 113 */
/* 114 */         if (comp > 0) {
/* 115 */           smj_rightRow = null;
/* 116 */         } else if (comp < 0) {
/* 117 */           if (!smj_matches.isEmpty()) {
/* 118 */             smj_value3 = smj_value;
/* 119 */           }
/* 120 */           return true;
/* 121 */         } else {
/* 122 */           smj_matches.add((UnsafeRow) smj_rightRow);
/* 123 */           smj_rightRow = null;
/* 124 */         }
/* 125 */       } while (smj_leftRow != null);
/* 126 */     }
/* 127 */     return false; // unreachable
/* 128 */   }
/* 129 */
/* 130 */   protected void processNext() throws java.io.IOException {
/* 131 */     while (findNextJoinRows(smj_leftInput, smj_rightInput)) {
/* 132 */       boolean smj_loaded = false;
/* 133 */       smj_value4 = smj_leftRow.getLong(0);
/* 134 */       smj_value5 = smj_leftRow.getLong(1);
/* 135 */       scala.collection.Iterator<UnsafeRow> smj_iterator = smj_matches.generateIterator();
/* 136 */       while (smj_iterator.hasNext()) {
/* 137 */         InternalRow smj_rightRow1 = (InternalRow) smj_iterator.next();
/* 138 */         smj_isNull3 = smj_rightRow1.isNullAt(1);
/* 139 */         smj_value7 = smj_rightRow1.getLong(1);
/* 140 */         boolean smj_isNull4 = true;
/* 141 */         boolean smj_value8 = false;
/* 142 */
/* 143 */         if (!smj_isNull3) {
/* 144 */           smj_isNull4 = false; // resultCode could change nullability.
/* 145 */           smj_value8 = smj_value5 < smj_value7;
/* 146 */
/* 147 */         }
/* 148 */         if (smj_isNull4 || !smj_value8) continue;
/* 149 */         smj_isNull2 = smj_rightRow1.isNullAt(0);
/* 150 */         smj_value6 = smj_rightRow1.getLong(0);
/* 151 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 152 */         smj_loaded = true;
/* 153 */         writeJoinRows();
/* 154 */       }
/* 155 */       if (!smj_loaded) {
/* 156 */         smj_isNull2 = true;
/* 157 */         smj_isNull3 = true;
/* 158 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 159 */         writeJoinRows();
/* 160 */       }
/* 161 */       if (shouldStop()) return;
/* 162 */     }
/* 163 */   }
/* 164 */
/* 165 */ }
```

## How was this patch tested?

Add test cases into `LeftJoinSuite` and `WholeStageCodegenSuite`
",spark,apache,heary-cao,20164092,MDQ6VXNlcjIwMTY0MDky,https://avatars1.githubusercontent.com/u/20164092?v=4,,https://api.github.com/users/heary-cao,https://github.com/heary-cao,https://api.github.com/users/heary-cao/followers,https://api.github.com/users/heary-cao/following{/other_user},https://api.github.com/users/heary-cao/gists{/gist_id},https://api.github.com/users/heary-cao/starred{/owner}{/repo},https://api.github.com/users/heary-cao/subscriptions,https://api.github.com/users/heary-cao/orgs,https://api.github.com/users/heary-cao/repos,https://api.github.com/users/heary-cao/events{/privacy},https://api.github.com/users/heary-cao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20820,https://github.com/apache/spark/pull/20820,https://github.com/apache/spark/pull/20820.diff,https://github.com/apache/spark/pull/20820.patch
447,https://api.github.com/repos/apache/spark/issues/20690,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20690/labels{/name},https://api.github.com/repos/apache/spark/issues/20690/comments,https://api.github.com/repos/apache/spark/issues/20690/events,https://github.com/apache/spark/pull/20690,300921501,MDExOlB1bGxSZXF1ZXN0MTcxODc3MzQ5,20690,[SPARK-23532][Standalone]Improve data locality when launching new executors for dynamic allocation,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2018-02-28T07:41:43Z,2019-10-18T09:02:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently Spark on Yarn supports better data locality by considering the preferred locations of the pending tasks when dynamic allocation is enabled, refer to _https://issues.apache.org/jira/browse/SPARK-4352_.
Mesos also supports data locality, refer to _https://issues.apache.org/jira/browse/SPARK-16944_

It would be better that Standalone can also support this feature.

## How was this patch tested?
1.Added a unit test.
2.Manual testing on HDFS in my environment.
Environment description: 5 workers, 3 HDFS nodes (**vmx01,vmx02,vmx04**).

_./spark-sql --master spark://vmx01:7077 --executor-memory 8G --total-executor-cores 26  --conf ""spark.executor.cores=2"" --conf ""spark.shuffle.service.enabled=true"" --conf ""spark.dynamicAllocation.enabled=true""  --conf ""spark.dynamicAllocation.minExecutors=0""_

Result(assigned more executors for vmx01,vmx02 and vmx04):
![default](https://user-images.githubusercontent.com/24688163/36776005-48fcd12a-1c9f-11e8-801d-8434bf14d9be.png)

",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20690,https://github.com/apache/spark/pull/20690,https://github.com/apache/spark/pull/20690.diff,https://github.com/apache/spark/pull/20690.patch
448,https://api.github.com/repos/apache/spark/issues/20665,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20665/labels{/name},https://api.github.com/repos/apache/spark/issues/20665/comments,https://api.github.com/repos/apache/spark/issues/20665/events,https://github.com/apache/spark/pull/20665,299801284,MDExOlB1bGxSZXF1ZXN0MTcxMDgwNDQw,20665,[SPARK-23499][MESOS] Support for priority queues in Mesos scheduler,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2018-02-23T18:09:15Z,2019-11-20T02:50:05Z,,NONE,"As for Yarn, Mesos users should be able to specify priority queues to define a workload management policy for queued drivers in the Mesos Cluster Dispatcher.

## What changes were proposed in this pull request?
This commit introduces the necessary changes to manage such priority queues.

## How was this patch tested?
unit tests
",spark,apache,pgillet,2465217,MDQ6VXNlcjI0NjUyMTc=,https://avatars2.githubusercontent.com/u/2465217?v=4,,https://api.github.com/users/pgillet,https://github.com/pgillet,https://api.github.com/users/pgillet/followers,https://api.github.com/users/pgillet/following{/other_user},https://api.github.com/users/pgillet/gists{/gist_id},https://api.github.com/users/pgillet/starred{/owner}{/repo},https://api.github.com/users/pgillet/subscriptions,https://api.github.com/users/pgillet/orgs,https://api.github.com/users/pgillet/repos,https://api.github.com/users/pgillet/events{/privacy},https://api.github.com/users/pgillet/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20665,https://github.com/apache/spark/pull/20665,https://github.com/apache/spark/pull/20665.diff,https://github.com/apache/spark/pull/20665.patch
449,https://api.github.com/repos/apache/spark/issues/20599,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20599/labels{/name},https://api.github.com/repos/apache/spark/issues/20599/comments,https://api.github.com/repos/apache/spark/issues/20599/events,https://github.com/apache/spark/pull/20599,296720315,MDExOlB1bGxSZXF1ZXN0MTY4ODMyMTk2,20599,[SPARK-23407][SQL] add a config to try to inline all mutable states during codegen,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2018-02-13T12:30:07Z,2019-09-16T19:23:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/19811 .

In #19811, we picked a sub-optimal solution that always compact non-primitive mutable states to arrays, to make primitive mutable states more likely to get inlined.

This PR introduces a new config to not treat primitive states specially and try to inline all states, to avoid any potential perf regression in Spark 2.3. By default it's false.

In the future, we can remove this config, and dynamically decide which states to inline. For example, we can use placeholders during codegen, and analysis all the mutable states at the end and replace the placeholders.

Note that there are no known regression cases, so this is not a blocker for Spark 2.3

## How was this patch tested?

a new test.",spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20599,https://github.com/apache/spark/pull/20599,https://github.com/apache/spark/pull/20599.diff,https://github.com/apache/spark/pull/20599.patch
450,https://api.github.com/repos/apache/spark/issues/20532,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20532/labels{/name},https://api.github.com/repos/apache/spark/issues/20532/comments,https://api.github.com/repos/apache/spark/issues/20532/events,https://github.com/apache/spark/pull/20532,295142973,MDExOlB1bGxSZXF1ZXN0MTY3Njk2NjA5,20532,[SPARK-23353][CORE] Allow ExecutorMetricsUpdate events to be logged t‚Ä¶,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2018-02-07T13:45:58Z,2019-09-16T18:25:52Z,,CONTRIBUTOR,"‚Ä¶o the event log with sampling

## What changes were proposed in this pull request?
[SPARK-22050|https://issues.apache.org/jira/browse/SPARK-22050] give a way to log BlockUpdated events. Actually, the ExecutorMetricsUpdates are also very useful if it can be persisted for further analysis. 
As a performance reason and actual use case, the PR offers a fraction configuration which can sample the events to be persisted. we also refactor for BlockUpdated with the same sampling way.

## How was this patch tested?
Unit tests
",spark,apache,LantaoJin,1853780,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20532,https://github.com/apache/spark/pull/20532,https://github.com/apache/spark/pull/20532.diff,https://github.com/apache/spark/pull/20532.patch
451,https://api.github.com/repos/apache/spark/issues/20505,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20505/labels{/name},https://api.github.com/repos/apache/spark/issues/20505/comments,https://api.github.com/repos/apache/spark/issues/20505/events,https://github.com/apache/spark/pull/20505,294241022,MDExOlB1bGxSZXF1ZXN0MTY3MDM2MDY0,20505,[SPARK-23251][SQL] Add checks for collection element Encoders,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2018-02-04T23:17:54Z,2019-11-24T02:18:07Z,,CONTRIBUTOR,"Implicit methods of `SQLImplicits` providing Encoders for collections did not check for
Encoders for their elements. This resulted in unrelated error messages during run-time instead of proper failures during compilation.

## What changes were proposed in this pull request?

`Seq`, `Map` and `Set` `Encoder` providers' type parameters and implicit parameters were modified to facilitate the appropriate checks.

Unfortunately, this doesn't result in the desired ""Unable to find encoder for type stored in a Dataset"" error as the Scala compiler generates a different error when an implicit cannot be found due to missing arguments (see [ContextErrors](https://github.com/scala/scala/blob/v2.11.12/src/compiler/scala/tools/nsc/typechecker/ContextErrors.scala#L77)). `@implicitNotFound` is not used in this case.

## How was this patch tested?

New behavior:
```
scala> implicitly[Encoder[Map[String, Any]]]
<console>:25: error: diverging implicit expansion for type org.apache.spark.sql.Encoder[Map[String,Any]]
starting with method newStringEncoder in class SQLImplicits
       implicitly[Encoder[Map[String, Any]]]
                 ^
```

Old behavior:
```
scala> implicitly[Encoder[Map[String, Any]]]
java.lang.ClassNotFoundException: scala.Any
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass(JavaMirrors.scala:555)
  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToJava$1.apply(JavaMirrors.scala:1211)
  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToJava$1.apply(JavaMirrors.scala:1203)
  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toJava$1.apply(TwoWayCaches.scala:49)
  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)
  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)
  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toJava(TwoWayCaches.scala:44)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToJava(JavaMirrors.scala:1203)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.runtimeClass(JavaMirrors.scala:194)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.runtimeClass(JavaMirrors.scala:54)
  at org.apache.spark.sql.catalyst.ScalaReflection$.getClassFromType(ScalaReflection.scala:700)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$dataTypeFor$1.apply(ScalaReflection.scala:84)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$dataTypeFor$1.apply(ScalaReflection.scala:65)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:56)
  at org.apache.spark.sql.catalyst.ScalaReflection$class.cleanUpReflectionObjects(ScalaReflection.scala:824)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:39)
  at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$dataTypeFor(ScalaReflection.scala:64)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor$1.apply(ScalaReflection.scala:512)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor$1.apply(ScalaReflection.scala:445)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:56)
  at org.apache.spark.sql.catalyst.ScalaReflection$class.cleanUpReflectionObjects(ScalaReflection.scala:824)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:39)
  at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:445)
  at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:434)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:71)
  at org.apache.spark.sql.SQLImplicits.newMapEncoder(SQLImplicits.scala:172)
  ... 49 elided
```

Also tested overriding with custom `Encoder` (this also compiles correctly in [SparkSQLExample](https://github.com/apache/spark/blob/425c4ada4c24e338b45d0e9987071f05c5766fa5/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala#L223)):
```
scala> implicit val kryoEnc = Encoders.kryo[Map[String, Any]]
kryoEnc: org.apache.spark.sql.Encoder[Map[String,Any]] = class[value[0]: binary]

scala> implicitly[Encoder[Map[String, Any]]]
res0: org.apache.spark.sql.Encoder[Map[String,Any]] = class[value[0]: binary]
```

BUT, I was unable to add this as a unit test, getting the aforementioned ""diverging implicit expansion"" error instead. I did not explore this further as it worked everywhere else.",spark,apache,michalsenkyr,8831737,MDQ6VXNlcjg4MzE3Mzc=,https://avatars1.githubusercontent.com/u/8831737?v=4,,https://api.github.com/users/michalsenkyr,https://github.com/michalsenkyr,https://api.github.com/users/michalsenkyr/followers,https://api.github.com/users/michalsenkyr/following{/other_user},https://api.github.com/users/michalsenkyr/gists{/gist_id},https://api.github.com/users/michalsenkyr/starred{/owner}{/repo},https://api.github.com/users/michalsenkyr/subscriptions,https://api.github.com/users/michalsenkyr/orgs,https://api.github.com/users/michalsenkyr/repos,https://api.github.com/users/michalsenkyr/events{/privacy},https://api.github.com/users/michalsenkyr/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20505,https://github.com/apache/spark/pull/20505,https://github.com/apache/spark/pull/20505.diff,https://github.com/apache/spark/pull/20505.patch
452,https://api.github.com/repos/apache/spark/issues/20478,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20478/labels{/name},https://api.github.com/repos/apache/spark/issues/20478/comments,https://api.github.com/repos/apache/spark/issues/20478/events,https://github.com/apache/spark/pull/20478,293603571,MDExOlB1bGxSZXF1ZXN0MTY2NTkyMjUz,20478, [SPARK-8835][Streaming] Provide pluggable Congestion Strategies for Receiver-based Streams. Take 2 / Spark 2.X merge,"[{'id': 1406587334, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzM0', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAMS', 'name': 'DSTREAMS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2018-02-01T16:49:44Z,2019-09-16T18:47:06Z,,MEMBER,"## What changes were proposed in this pull request?

Looks like the [PR](https://github.com/apache/spark/pull/9200) for this ([SPARK-8835](https://issues.apache.org/jira/browse/SPARK-8835)) got dropped quite some time ago, due to build failures. I've merged up to the current master and am re-submitting it (to be clear, I am not the original author).

## How was this patch tested?
Built and tested locally.",spark,apache,EmergentOrder,415599,MDQ6VXNlcjQxNTU5OQ==,https://avatars0.githubusercontent.com/u/415599?v=4,,https://api.github.com/users/EmergentOrder,https://github.com/EmergentOrder,https://api.github.com/users/EmergentOrder/followers,https://api.github.com/users/EmergentOrder/following{/other_user},https://api.github.com/users/EmergentOrder/gists{/gist_id},https://api.github.com/users/EmergentOrder/starred{/owner}{/repo},https://api.github.com/users/EmergentOrder/subscriptions,https://api.github.com/users/EmergentOrder/orgs,https://api.github.com/users/EmergentOrder/repos,https://api.github.com/users/EmergentOrder/events{/privacy},https://api.github.com/users/EmergentOrder/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20478,https://github.com/apache/spark/pull/20478,https://github.com/apache/spark/pull/20478.diff,https://github.com/apache/spark/pull/20478.patch
453,https://api.github.com/repos/apache/spark/issues/20418,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20418/labels{/name},https://api.github.com/repos/apache/spark/issues/20418/comments,https://api.github.com/repos/apache/spark/issues/20418/events,https://github.com/apache/spark/pull/20418,292249823,MDExOlB1bGxSZXF1ZXN0MTY1NTkzOTQ2,20418,[SPARK-790][MESOS] Implement reregistered callback for MesosScheduler,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2018-01-28T23:26:26Z,2019-09-16T19:23:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Implement reregistered callback for MesosScheduler.
There are few related issues open on Mesos, below. This implementation is based on how Kafka on Mesos currently does reregistering.
https://issues.apache.org/jira/browse/MESOS-4659
https://issues.apache.org/jira/browse/MESOS-1719

## How was this patch tested?
Existing tests.
",spark,apache,rekhajoshm,5987836,MDQ6VXNlcjU5ODc4MzY=,https://avatars3.githubusercontent.com/u/5987836?v=4,,https://api.github.com/users/rekhajoshm,https://github.com/rekhajoshm,https://api.github.com/users/rekhajoshm/followers,https://api.github.com/users/rekhajoshm/following{/other_user},https://api.github.com/users/rekhajoshm/gists{/gist_id},https://api.github.com/users/rekhajoshm/starred{/owner}{/repo},https://api.github.com/users/rekhajoshm/subscriptions,https://api.github.com/users/rekhajoshm/orgs,https://api.github.com/users/rekhajoshm/repos,https://api.github.com/users/rekhajoshm/events{/privacy},https://api.github.com/users/rekhajoshm/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20418,https://github.com/apache/spark/pull/20418,https://github.com/apache/spark/pull/20418.diff,https://github.com/apache/spark/pull/20418.patch
454,https://api.github.com/repos/apache/spark/issues/20345,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20345/labels{/name},https://api.github.com/repos/apache/spark/issues/20345/comments,https://api.github.com/repos/apache/spark/issues/20345/events,https://github.com/apache/spark/pull/20345,290295789,MDExOlB1bGxSZXF1ZXN0MTY0MTg4ODgz,20345,[SPARK-23172][SQL] Expand the ReorderJoin rule to handle Project nodes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,89,2018-01-21T18:52:21Z,2019-12-16T04:35:11Z,,MEMBER,"## What changes were proposed in this pull request?
The current `ReorderJoin` optimizer rule cannot flatten a pattern `Join -> Project -> Join` because `ExtractFiltersAndInnerJoins` doesn't handle `Project` nodes. So, the current master cannot reorder joins in a query below;
```
val df1 = spark.range(100).selectExpr(""id % 10 AS k0"", s""id % 10 AS k1"", s""id % 10 AS k2"", ""id AS v1"")
val df2 = spark.range(10).selectExpr(""id AS k0"", ""id AS v2"")
val df3 = spark.range(10).selectExpr(""id AS k1"", ""id AS v3"")
val df4 = spark.range(10).selectExpr(""id AS k2"", ""id AS v4"")
df1.join(df2, ""k0"").join(df3, ""k1"").join(df4, ""k2"").explain(true)

== Analyzed Logical Plan ==
k2: bigint, k1: bigint, k0: bigint, v1: bigint, v2: bigint, v3: bigint, v4: bigint
Project [k2#5L, k1#4L, k0#3L, v1#6L, v2#16L, v3#24L, v4#32L]
+- Join Inner, (k2#5L = k2#31L)
   :- Project [k1#4L, k0#3L, k2#5L, v1#6L, v2#16L, v3#24L]
   :  +- Join Inner, (k1#4L = k1#23L)
   :     :- Project [k0#3L, k1#4L, k2#5L, v1#6L, v2#16L]
   :     :  +- Join Inner, (k0#3L = k0#15L)
   :     :     :- Project [(id#0L % cast(10 as bigint)) AS k0#3L, (id#0L % cast(10 as bigint)) AS k1#4L, (id#0L % cast(10 as bigint)) AS k2#5L, id#0
L AS v1#6L]
   :     :     :  +- Range (0, 100, step=1, splits=Some(4))
   :     :     +- Project [id#12L AS k0#15L, id#12L AS v2#16L]
   :     :        +- Range (0, 10, step=1, splits=Some(4))
   :     +- Project [id#20L AS k1#23L, id#20L AS v3#24L]
   :        +- Range (0, 10, step=1, splits=Some(4))
   +- Project [id#28L AS k2#31L, id#28L AS v4#32L]
      +- Range (0, 10, step=1, splits=Some(4))
```
To reorder the query, this pr added code to handle `Project` in `ExtractFiltersAndInnerJoins`.

This pr also fixed an output attribute reorder problem when joins reordered; it checks if a join reordered plan and an original plan have the same output attribute order with each other. If not, `ReorderJoin` adds `Project` in the top of the join reordered plan.

## How was this patch tested?
This pr added new tests in `JoinOptimizationSuite` and modified some existing tests in `StarJoinReorderSuite` to check if `ReorderJoin` can handle `Project` nodes correctly. Also, it modified the existing tests in `JoinReorderSuite` for the output attribute reorder issue.",spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20345,https://github.com/apache/spark/pull/20345,https://github.com/apache/spark/pull/20345.diff,https://github.com/apache/spark/pull/20345.patch
455,https://api.github.com/repos/apache/spark/issues/20206,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20206/labels{/name},https://api.github.com/repos/apache/spark/issues/20206/comments,https://api.github.com/repos/apache/spark/issues/20206/events,https://github.com/apache/spark/pull/20206,287144431,MDExOlB1bGxSZXF1ZXN0MTYxOTEyOTI3,20206,[SPARK-19256][SQL] Remove ordering enforcement from `FileFormatWriter` and let planner do that,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2018-01-09T16:14:17Z,2019-09-16T19:24:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This is as per discussion in https://github.com/apache/spark/pull/19483#issuecomment-336337516 . Enforcing `Sort` at right places in the tree is something that `EnsureRequirements` should take care of. This PR removes `SORT` node insertion done inside `FileFormatWriter`.

## How was this patch tested?

- Existing unit tests
- Looked at the query plan for bucketed insert. `Sort` was added in the plan.

```
scala> hc.sql("" desc formatted test1  "").collect.foreach(println)
.....
[Num Buckets,8,]
[Bucket Columns,[`j`, `k`],]
[Sort Columns,[`j`, `k`],]


scala> hc.sql("" EXPLAIN INSERT OVERWRITE TABLE test1 SELECT * FROM test2 "").collect.foreach(println)
[== Physical Plan ==
Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/warehouse/test1, false, 8 buckets, bucket columns: [j, k], sort columns: [j, k], ...
+- *Sort [pmod(hash(j#56, k#57, 42), 8) ASC NULLS FIRST, j#56 ASC NULLS FIRST, k#57 ASC NULLS FIRST], false, 0
   +- *FileScan orc default.test2[i#55,j#56,k#57] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/warehouse/test2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i:int,j:int,k:string>]
```
  ",spark,apache,tejasapatil,3230057,MDQ6VXNlcjMyMzAwNTc=,https://avatars1.githubusercontent.com/u/3230057?v=4,,https://api.github.com/users/tejasapatil,https://github.com/tejasapatil,https://api.github.com/users/tejasapatil/followers,https://api.github.com/users/tejasapatil/following{/other_user},https://api.github.com/users/tejasapatil/gists{/gist_id},https://api.github.com/users/tejasapatil/starred{/owner}{/repo},https://api.github.com/users/tejasapatil/subscriptions,https://api.github.com/users/tejasapatil/orgs,https://api.github.com/users/tejasapatil/repos,https://api.github.com/users/tejasapatil/events{/privacy},https://api.github.com/users/tejasapatil/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20206,https://github.com/apache/spark/pull/20206,https://github.com/apache/spark/pull/20206.diff,https://github.com/apache/spark/pull/20206.patch
456,https://api.github.com/repos/apache/spark/issues/20184,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20184/labels{/name},https://api.github.com/repos/apache/spark/issues/20184/comments,https://api.github.com/repos/apache/spark/issues/20184/events,https://github.com/apache/spark/pull/20184,286641745,MDExOlB1bGxSZXF1ZXN0MTYxNTU5Mzc5,20184,[SPARK-22987][Core] UnsafeExternalSorter cases OOM when invoking `getIterator` function.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2018-01-08T04:36:47Z,2019-09-16T18:25:40Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

ChainedIterator.UnsafeExternalSorter remains a Queue of UnsafeSorterIterator. When call `getIterator` function of UnsafeExternalSorter, UnsafeExternalSorter passes an ArrayList of UnsafeSorterSpillReader to the constructor of UnsafeExternalSorter. But, UnsafeSorterSpillReader maintains a byte array as buffer, witch capacity is more than 1 MB. When spilling frequently, this case maybe causes OOM.

In this PR, I try to change buffer allocation in UnsafeSorterSpillReader lazily.

## How was this patch tested?

Existing tests.
",spark,apache,liutang123,17537020,MDQ6VXNlcjE3NTM3MDIw,https://avatars3.githubusercontent.com/u/17537020?v=4,,https://api.github.com/users/liutang123,https://github.com/liutang123,https://api.github.com/users/liutang123/followers,https://api.github.com/users/liutang123/following{/other_user},https://api.github.com/users/liutang123/gists{/gist_id},https://api.github.com/users/liutang123/starred{/owner}{/repo},https://api.github.com/users/liutang123/subscriptions,https://api.github.com/users/liutang123/orgs,https://api.github.com/users/liutang123/repos,https://api.github.com/users/liutang123/events{/privacy},https://api.github.com/users/liutang123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20184,https://github.com/apache/spark/pull/20184,https://github.com/apache/spark/pull/20184.diff,https://github.com/apache/spark/pull/20184.patch
457,https://api.github.com/repos/apache/spark/issues/20124,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20124/labels{/name},https://api.github.com/repos/apache/spark/issues/20124/comments,https://api.github.com/repos/apache/spark/issues/20124/events,https://github.com/apache/spark/pull/20124,285246010,MDExOlB1bGxSZXF1ZXN0MTYwNjEzMTky,20124,[WIP][SPARK-22126][ML] Fix model-specific optimization support for ML tuning.,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2017-12-31T08:15:06Z,2019-09-16T19:25:06Z,,MEMBER,"## What changes were proposed in this pull request?

Support model-specific optimizations for CrossValidator and TrainValidationSplit by grouping `ParamMap`s so that param groups can fit models in parallel, but still allow `Estimator`s to optimally fit a sequence of models themselves.  This PR adds a new API to `Estimator` that can be overridden to indicate optimized params, and additional functions in `ParamGridBuilder` to group `ParamMap` arrays that can then be used by the meta-algorithms.

## How was this patch tested?

WIP, need to add tests",spark,apache,BryanCutler,4534389,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20124,https://github.com/apache/spark/pull/20124,https://github.com/apache/spark/pull/20124.diff,https://github.com/apache/spark/pull/20124.patch
458,https://api.github.com/repos/apache/spark/issues/20086,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/20086/labels{/name},https://api.github.com/repos/apache/spark/issues/20086/comments,https://api.github.com/repos/apache/spark/issues/20086/events,https://github.com/apache/spark/pull/20086,284635598,MDExOlB1bGxSZXF1ZXN0MTYwMTg0ODU4,20086,[SPARK-22903]Fix already being created exception in stage retry caused by wrong at‚Ä¶,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2017-12-27T02:25:39Z,2019-09-16T18:25:27Z,,NONE,"‚Ä¶temptNumber

## What changes were proposed in this pull request?

This PR fix the wrong attemptNumber in stage retry, it will solve the probem of AlreadyBeingCreatedException thrown by executor when failedStages already created the taskAttemptPath.
Details see: https://issues.apache.org/jira/browse/SPARK-22903

(Please fill in changes proposed in this fix)

## How was this patch tested?

manual
(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,liupc,6747355,MDQ6VXNlcjY3NDczNTU=,https://avatars2.githubusercontent.com/u/6747355?v=4,,https://api.github.com/users/liupc,https://github.com/liupc,https://api.github.com/users/liupc/followers,https://api.github.com/users/liupc/following{/other_user},https://api.github.com/users/liupc/gists{/gist_id},https://api.github.com/users/liupc/starred{/owner}{/repo},https://api.github.com/users/liupc/subscriptions,https://api.github.com/users/liupc/orgs,https://api.github.com/users/liupc/repos,https://api.github.com/users/liupc/events{/privacy},https://api.github.com/users/liupc/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/20086,https://github.com/apache/spark/pull/20086,https://github.com/apache/spark/pull/20086.diff,https://github.com/apache/spark/pull/20086.patch
459,https://api.github.com/repos/apache/spark/issues/19832,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19832/labels{/name},https://api.github.com/repos/apache/spark/issues/19832/comments,https://api.github.com/repos/apache/spark/issues/19832/events,https://github.com/apache/spark/pull/19832,277347454,MDExOlB1bGxSZXF1ZXN0MTU1MDQzMzg0,19832,[SPARK-22628][CORE]Some situationsÔºå  the assignment of executors on workers is not what we expected when `spark.deploy.spreadOut=true`.,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2017-11-28T11:18:08Z,2019-09-27T03:28:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

For example, cluster has 3 workers(workA, workB, workC), workA has 1 core left, workB has 1 core left, workC has no cores left.
User requests 3 executors (spark.cores.max = 3, spark.executor.cores = 1), obviously, workA will be assigned one executor ,and workB will be assigned one executor.
After a moment,if some apps release cores, and workB has 3 core left, workC has 2 core left, we should assign one executor on workC,not workB.
Especially for dynamic executors allocation in standalone mode, this problem is more serious.

This PR reorders by another key for `usableWorkers` to  solve this problem.


## How was this patch tested?
Manual test

",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19832,https://github.com/apache/spark/pull/19832,https://github.com/apache/spark/pull/19832.diff,https://github.com/apache/spark/pull/19832.patch
460,https://api.github.com/repos/apache/spark/issues/19810,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19810/labels{/name},https://api.github.com/repos/apache/spark/issues/19810/comments,https://api.github.com/repos/apache/spark/issues/19810/events,https://github.com/apache/spark/pull/19810,276537031,MDExOlB1bGxSZXF1ZXN0MTU0NDg5Mjg5,19810,[SPARK-22599][SQL] In-Memory Table Pruning without Extra Reading,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2017-11-24T07:42:49Z,2019-09-16T19:25:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

In the current implementation of Spark, InMemoryTableExec read all data in a cached table, filter CachedBatch according to stats and pass data to the downstream operators. This implementation makes it inefficient to reside the whole table in memory to serve various queries against different partitions of the table, which occupies a certain portion of our users' scenarios.

design doc: https://docs.google.com/document/d/1DSiP3ej7Wd2cWUPVrgqAtvxbSlu5_1ZZB6m_2t8_95Q/edit?usp=sharing

The following is an example of such a use case:

store_sales is a 1TB-sized table in cloud storage, which is partitioned by 'location'. The first query, Q1, wants to output several metrics A, B, C for all stores in all locations. After that, a small team of 3 data scientists wants to do some causal analysis for the sales in different locations. To avoid unnecessary I/O and parquet/orc parsing overhead, they want to cache the whole table in memory in Q1.

With the current implementation, even any one of the data scientists is only interested in one out of three locations, the queries they submit to Spark cluster is still reading 1TB data completely.

The reason behind the extra reading operation is that we implement CachedBatch as

```scala
case class CachedBatch(numRows: Int, buffers: Array[Array[Byte]], stats: InternalRow)
```

where the stats is a part of every CachedBatch, so we can only filter batches for output of InMemoryTableExec operator by reading all data in in-memory table as input. The extra reading would be even more unacceptable when some of the table's data is evicted to disks.

We propose to introduce a new type of block, metadata block, for the partitions of RDD representing data in the cached table. Every metadata block contains stats info for all columns in a partition and is saved to BlockManager when executing compute() method for the partition. To minimize the number of bytes to read,

## How was this patch tested?

1. unit test: add 3 new unit tests

2. performance test: 

Environment: 6 Executors, each of which has 16 cores 90G memory

dataset: 1T TPCDS data

queries: tested 4 queries (Q19, Q46, Q34, Q27) in https://github.com/databricks/spark-sql-perf/blob/c2224f37e50628c5c8691be69414ec7f5a3d919a/src/main/scala/com/databricks/spark/sql/perf/tpcds/ImpalaKitQueries.scala

results: https://docs.google.com/spreadsheets/d/1A20LxqZzAxMjW7ptAJZF4hMBaHxKGk3TBEQoAJXfzCI/edit?usp=sharing

",spark,apache,CodingCat,678008,MDQ6VXNlcjY3ODAwOA==,https://avatars1.githubusercontent.com/u/678008?v=4,,https://api.github.com/users/CodingCat,https://github.com/CodingCat,https://api.github.com/users/CodingCat/followers,https://api.github.com/users/CodingCat/following{/other_user},https://api.github.com/users/CodingCat/gists{/gist_id},https://api.github.com/users/CodingCat/starred{/owner}{/repo},https://api.github.com/users/CodingCat/subscriptions,https://api.github.com/users/CodingCat/orgs,https://api.github.com/users/CodingCat/repos,https://api.github.com/users/CodingCat/events{/privacy},https://api.github.com/users/CodingCat/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19810,https://github.com/apache/spark/pull/19810,https://github.com/apache/spark/pull/19810.diff,https://github.com/apache/spark/pull/19810.patch
461,https://api.github.com/repos/apache/spark/issues/19773,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19773/labels{/name},https://api.github.com/repos/apache/spark/issues/19773/comments,https://api.github.com/repos/apache/spark/issues/19773/events,https://github.com/apache/spark/pull/19773,274848197,MDExOlB1bGxSZXF1ZXN0MTUzMjg1MTcz,19773,[SPARK-22546][SQL] Supporting for changing column dataType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2017-11-17T12:21:40Z,2019-09-16T19:25:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Support user to change column dataType in hive table and datasource table, also make sure the new changed data type can work with all data sources.

- [x] DDL support for `ALTER TABLE CHANGE COLUMN`
- [ ] Support in parquet vectorized reader
- [ ] Support in parquet row reader
- [ ] Support in orc  vectorized reader
- [ ] Support in orc  row reader

## How was this patch tested?

Add test case in `DDLSuite.scala` and `SQLQueryTestSuite.scala`",spark,apache,xuanyuanking,4833765,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19773,https://github.com/apache/spark/pull/19773,https://github.com/apache/spark/pull/19773.diff,https://github.com/apache/spark/pull/19773.patch
462,https://api.github.com/repos/apache/spark/issues/19716,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19716/labels{/name},https://api.github.com/repos/apache/spark/issues/19716/comments,https://api.github.com/repos/apache/spark/issues/19716/events,https://github.com/apache/spark/pull/19716,272884005,MDExOlB1bGxSZXF1ZXN0MTUxODcxMTc5,19716,[SPARK-18755][MRG][ML] Random search implementation using RandomParamGridBuilder,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2017-11-10T10:17:56Z,2019-09-16T18:25:12Z,,NONE,"## What changes were proposed in this pull request?

Python `sklearn` has an implementation of random search to complement grid search. This usually allows for more efficient hyperparameter tuning.

I have developed an alternative class to `ParamGridBuilder` called `RandomParamGridBuilder` which will facilitate random search in Spark. This works by creating the same data structure as the output of `ParamGridBuilder.build()` (and so is compatible with the existing `CrossValidator` class), through random sampling.

The main methods by which the random sampling is used in the `sklearn` implementation are as follows:

- Sampling through options e.g. `[0.01, 0.001, 0.0001]` 
  - This is handled using `addUniformChoice`
- Sampling and integer/long/float/double between bounds 
  - `RandomParamGrid` has the `addUniformDistribution` method for this
- Boolean sampling
  - `RandomParamGrid.addUniformDistribution` supports this as well
- Or sampling over a more exotic distribution (e.g. beta or Cauchy)
  - Here the user can implement their own function which when called returns a value from the intended distribution, and add that using `addDistribution` thereby allowing full flexibility.

## How was this patch tested?

Several unit tests have been created for the `RandomParamGridBuilder` in `RandomParamGridBuilderSuite`. 
In `CrossValidatorSuite`, two tests were changed to run with param maps created by `RandomParamGridBuilder` as well as those from `ParamGridBuilder`. One additional test was added there as well (altered version of a `ParamGridBuilder` test). ",spark,apache,gnsiva,4519814,MDQ6VXNlcjQ1MTk4MTQ=,https://avatars3.githubusercontent.com/u/4519814?v=4,,https://api.github.com/users/gnsiva,https://github.com/gnsiva,https://api.github.com/users/gnsiva/followers,https://api.github.com/users/gnsiva/following{/other_user},https://api.github.com/users/gnsiva/gists{/gist_id},https://api.github.com/users/gnsiva/starred{/owner}{/repo},https://api.github.com/users/gnsiva/subscriptions,https://api.github.com/users/gnsiva/orgs,https://api.github.com/users/gnsiva/repos,https://api.github.com/users/gnsiva/events{/privacy},https://api.github.com/users/gnsiva/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19716,https://github.com/apache/spark/pull/19716,https://github.com/apache/spark/pull/19716.diff,https://github.com/apache/spark/pull/19716.patch
463,https://api.github.com/repos/apache/spark/issues/19690,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19690/labels{/name},https://api.github.com/repos/apache/spark/issues/19690/comments,https://api.github.com/repos/apache/spark/issues/19690/events,https://github.com/apache/spark/pull/19690,272098125,MDExOlB1bGxSZXF1ZXN0MTUxMzEwMzQz,19690,[SPARK-22467]Added a config to support whether `stdout_stream` and `stderr_stream` output to disk,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2017-11-08T07:25:00Z,2019-09-16T19:25:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

We should add a switch to control the `stdout_stream` and `stdout_stream` output to disk.
In my environment,due to disk I/O blocking, the `stdout_stream` output is very slow, so it can not be timely cleaningÔºåand this leads the executor process to be blocked.

## How was this patch tested?
Added a unit test
",spark,apache,10110346,24688163,MDQ6VXNlcjI0Njg4MTYz,https://avatars2.githubusercontent.com/u/24688163?v=4,,https://api.github.com/users/10110346,https://github.com/10110346,https://api.github.com/users/10110346/followers,https://api.github.com/users/10110346/following{/other_user},https://api.github.com/users/10110346/gists{/gist_id},https://api.github.com/users/10110346/starred{/owner}{/repo},https://api.github.com/users/10110346/subscriptions,https://api.github.com/users/10110346/orgs,https://api.github.com/users/10110346/repos,https://api.github.com/users/10110346/events{/privacy},https://api.github.com/users/10110346/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19690,https://github.com/apache/spark/pull/19690,https://github.com/apache/spark/pull/19690.diff,https://github.com/apache/spark/pull/19690.patch
464,https://api.github.com/repos/apache/spark/issues/19659,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19659/labels{/name},https://api.github.com/repos/apache/spark/issues/19659/comments,https://api.github.com/repos/apache/spark/issues/19659/events,https://github.com/apache/spark/pull/19659,271192283,MDExOlB1bGxSZXF1ZXN0MTUwNjc2NzM0,19659,[SPARK-19668][ML] Multiple NGram sizes,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2017-11-04T13:30:57Z,2019-09-16T19:26:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

[Jira ticket](https://issues.apache.org/jira/browse/SPARK-19668):
- implements extraction of multiple sizes of ngrams with `feature.NGram`

## How was this patch tested?

- unit tests were added for the implementation (`multiSliding` function)
- test cases were added to `NGramSuite` and `Word2VecSuite`

----

Benchmark of the implementation of the `multiSliding` function against a simpler, but naive implementation (tested on 1 machine):

```Scala
scala> def multiSlidingRepeated[A](x: Seq[A], min: Int, max: Int): Seq[Seq[A]] = (min to max).flatMap(x.sliding)
multiSlidingRepeated: [A](x: Seq[A], min: Int, max: Int)Seq[Seq[A]]

scala> time(multiSliding(1 to 10000, 5, 20)).toMillis
res5: Long = 66

scala> time(multiSlidingRepeated(1 to 10000, 5, 20)).toMillis
res6: Long = 434

scala> time(multiSliding(1 to 50000, 5, 20)).toMillis
res7: Long = 595

scala> time(multiSlidingRepeated(1 to 50000, 5, 20)).toMillis
java.lang.OutOfMemoryError: GC overhead limit exceeded
  ...

scala> time(multiSliding(1 to 10000, 5, 30)).toMillis
res9: Long = 91

scala> time(multiSlidingRepeated(1 to 10000, 5, 30)).toMillis
res11: Long = 428
```
",spark,apache,mpetruska,6648274,MDQ6VXNlcjY2NDgyNzQ=,https://avatars3.githubusercontent.com/u/6648274?v=4,,https://api.github.com/users/mpetruska,https://github.com/mpetruska,https://api.github.com/users/mpetruska/followers,https://api.github.com/users/mpetruska/following{/other_user},https://api.github.com/users/mpetruska/gists{/gist_id},https://api.github.com/users/mpetruska/starred{/owner}{/repo},https://api.github.com/users/mpetruska/subscriptions,https://api.github.com/users/mpetruska/orgs,https://api.github.com/users/mpetruska/repos,https://api.github.com/users/mpetruska/events{/privacy},https://api.github.com/users/mpetruska/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19659,https://github.com/apache/spark/pull/19659,https://github.com/apache/spark/pull/19659.diff,https://github.com/apache/spark/pull/19659.patch
465,https://api.github.com/repos/apache/spark/issues/19635,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19635/labels{/name},https://api.github.com/repos/apache/spark/issues/19635/comments,https://api.github.com/repos/apache/spark/issues/19635/events,https://github.com/apache/spark/pull/19635,270447777,MDExOlB1bGxSZXF1ZXN0MTUwMTQyNjY1,19635,[SPARK-22413][SQL] Type coercion for IN is not coherent between Literals and subquery,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,27,2017-11-01T20:50:03Z,2019-12-04T23:06:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Now, type coercion for IN is not coherent between Literals and subquery. This PR changes the behavior for the case with literals and makes it coherent with the case of the subquery and also with the binary comparisons.

Before the patch, when IN is used with literals, we are using `findWiderCommonType` to determine the type to cast all the elements in the list and the value attribute of the In operator. This is not consistent with the behavior In has when there is a subquery, where we are using `findCommonTypeForBinaryComparison`.

The PR changes In type coercion with Literals to make it coherent to the one with subqueries (which is also the one used in other places, like simple comparisons).


## How was this patch tested?

Added UT
",spark,apache,mgaido91,8821783,MDQ6VXNlcjg4MjE3ODM=,https://avatars1.githubusercontent.com/u/8821783?v=4,,https://api.github.com/users/mgaido91,https://github.com/mgaido91,https://api.github.com/users/mgaido91/followers,https://api.github.com/users/mgaido91/following{/other_user},https://api.github.com/users/mgaido91/gists{/gist_id},https://api.github.com/users/mgaido91/starred{/owner}{/repo},https://api.github.com/users/mgaido91/subscriptions,https://api.github.com/users/mgaido91/orgs,https://api.github.com/users/mgaido91/repos,https://api.github.com/users/mgaido91/events{/privacy},https://api.github.com/users/mgaido91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19635,https://github.com/apache/spark/pull/19635,https://github.com/apache/spark/pull/19635.diff,https://github.com/apache/spark/pull/19635.patch
466,https://api.github.com/repos/apache/spark/issues/19633,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19633/labels{/name},https://api.github.com/repos/apache/spark/issues/19633/comments,https://api.github.com/repos/apache/spark/issues/19633/events,https://github.com/apache/spark/pull/19633,270436816,MDExOlB1bGxSZXF1ZXN0MTUwMTM0NDM1,19633,[SPARK-22411][SQL] Disable the heuristic to calculate max partition size when  dynamic allocation is enabled and use the value specified by the property spark.sql.files.maxPartitionBytes instead,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2017-11-01T20:13:26Z,2019-09-17T00:23:08Z,,NONE,"## What changes were proposed in this pull request?

The heuristic to calculate the maxSplitSize in DataSourceScanExec is as follows:
https://github.com/apache/spark/blob/d28d5732ae205771f1f443b15b10e64dcffb5ff0/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L431

bytesPerCore is calculated based on the default parallelism. Default parallelism in this case is the number of total cores of all the registered executors for this application. The heuristic works well with static allocation but with dynamic allocation enabled, defaultParallelism is usually one (with default config of min and initial executors as zero) at the time of split calculation. This heuristic was introduced in SPARK-14582.

With dynamic allocation it is confusing to tune the split size with this heuristic. It is better to ignore bytesPerCore and use the values of 'spark.sql.files.maxPartitionBytes' as the max split size.

## How was this patch tested?
Tested manually.
",spark,apache,vgankidi,4732316,MDQ6VXNlcjQ3MzIzMTY=,https://avatars0.githubusercontent.com/u/4732316?v=4,,https://api.github.com/users/vgankidi,https://github.com/vgankidi,https://api.github.com/users/vgankidi/followers,https://api.github.com/users/vgankidi/following{/other_user},https://api.github.com/users/vgankidi/gists{/gist_id},https://api.github.com/users/vgankidi/starred{/owner}{/repo},https://api.github.com/users/vgankidi/subscriptions,https://api.github.com/users/vgankidi/orgs,https://api.github.com/users/vgankidi/repos,https://api.github.com/users/vgankidi/events{/privacy},https://api.github.com/users/vgankidi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19633,https://github.com/apache/spark/pull/19633,https://github.com/apache/spark/pull/19633.diff,https://github.com/apache/spark/pull/19633.patch
467,https://api.github.com/repos/apache/spark/issues/19583,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19583/labels{/name},https://api.github.com/repos/apache/spark/issues/19583/comments,https://api.github.com/repos/apache/spark/issues/19583/events,https://github.com/apache/spark/pull/19583,268940622,MDExOlB1bGxSZXF1ZXN0MTQ5MDY3NTQ4,19583,[WIP][SPARK-22339] [CORE] [NETWORK-SHUFFLE] Push epoch updates to executors on fetch failure to avoid fetch retries for missing executors,"[{'id': 1406605327, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MzI3', 'url': 'https://api.github.com/repos/apache/spark/labels/SHUFFLE', 'name': 'SHUFFLE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2017-10-26T23:14:06Z,2019-09-16T18:26:45Z,,NONE,"## What changes were proposed in this pull request?
When a task finishes with error due to a fetch error, then DAGScheduler unregisters the shuffle blocks hosted by the serving executor (or even all the executors in the failing host, with external shuffle and spark.files.fetchFailure.unRegisterOutputOnHost enabled) in the shuffle block directory stored by MapOutputTracker, that then increments its epoch as a result. This event is only signaled to the other executors when a new task with a new epoch starts in each executor. This means that other executors reading from the failed executors will retry fetching shuffle blocks from them, even though the driver already knows those executors are lost and those blocks are now unavailable at those locations. This impacts job runtime, specially for long shuffles and executor failures at the end of a stage, when the only pending tasks are shuffle reads.
This could be improved by pushing the epoch update to the executors without having to wait for a new task. In the attached patch I sketch a possible solution that sends the updated epoch from the driver to the executors by piggybacking on the executor heartbeat response. ShuffleBlockFetcherIterator, RetryingBlockFetcher and BlockFetchingListener are modified so blocks locations are checked on each fetch retry. This doesn't introduce additional traffic, as MapOutputTrackerWorker.mapStatuses is shared by all tasks running on the same Executor, and the lookup of the new shuffle blocks directory was going to happen anyway when the new epoch is detected during the start of the next task.
I would like to know the opinion of the community on this approach.

## How was this patch tested?
Unit tests, tests in Yarn cluster",spark,apache,juanrh,895341,MDQ6VXNlcjg5NTM0MQ==,https://avatars3.githubusercontent.com/u/895341?v=4,,https://api.github.com/users/juanrh,https://github.com/juanrh,https://api.github.com/users/juanrh/followers,https://api.github.com/users/juanrh/following{/other_user},https://api.github.com/users/juanrh/gists{/gist_id},https://api.github.com/users/juanrh/starred{/owner}{/repo},https://api.github.com/users/juanrh/subscriptions,https://api.github.com/users/juanrh/orgs,https://api.github.com/users/juanrh/repos,https://api.github.com/users/juanrh/events{/privacy},https://api.github.com/users/juanrh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19583,https://github.com/apache/spark/pull/19583,https://github.com/apache/spark/pull/19583.diff,https://github.com/apache/spark/pull/19583.patch
468,https://api.github.com/repos/apache/spark/issues/19568,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19568/labels{/name},https://api.github.com/repos/apache/spark/issues/19568/comments,https://api.github.com/repos/apache/spark/issues/19568/events,https://github.com/apache/spark/pull/19568,268199797,MDExOlB1bGxSZXF1ZXN0MTQ4NTIzMjA2,19568,SPARK-22345: Fix sort-merge joins with conditions and codegen.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2017-10-24T21:55:56Z,2019-09-16T19:26:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This adds a joined row to sort-merge join codegen. That joined row is used to generate code for filter expressions, which may fall back to using the result row. Previously, the right side of the join was used, which is incorrect (the non-codegen implementations use a joined row).

## How was this patch tested?

Current tests.",spark,apache,rdblue,87915,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19568,https://github.com/apache/spark/pull/19568,https://github.com/apache/spark/pull/19568.diff,https://github.com/apache/spark/pull/19568.patch
469,https://api.github.com/repos/apache/spark/issues/19472,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19472/labels{/name},https://api.github.com/repos/apache/spark/issues/19472/comments,https://api.github.com/repos/apache/spark/issues/19472/events,https://github.com/apache/spark/pull/19472,264531200,MDExOlB1bGxSZXF1ZXN0MTQ1OTA4ODk1,19472,"[WIP][SPARK-22246][SQL] Improve performance of UnsafeRow, UnsafeArrayData, UnsafeMapData, and SpecificUnsafeRowJoiner by using MemoryBlock ","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2017-10-11T10:16:50Z,2019-09-16T19:26:07Z,,MEMBER,"## What changes were proposed in this pull request?

Waiting for merging #19222

This PR tries to use `MemoryBlock` in `UnsafeRow`, `UnsafeArray`, `UnsafeMap`, `SpecificUnsafeRowJoiner`. There are two advantages to use `MemoryBlock`
1. Increase flexibility of choosing memory type (e.g `byte[]`, `int[]`, `long[]`, or `DirectBuffer (offheap)`).
2. Improve runtime performance of memory access instead of using `Object`.

This PR can achieve **1.9x performance improvement** of `Read UnsafeArrayData` in [`UnsafeArrayDataBenchmark`](https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/UnsafeArrayDataBenchmark.scala#L42).

Without this PR
```
OpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic
Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz
Read UnsafeArrayData:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
Int                                            845 /  863        993.3           1.0       1.0X
Double                                         935 / 1003        897.0           1.1       0.9X
```
With this PR
```
OpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic
Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz
Read UnsafeArrayData:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
Int                                            477 /  486       1760.3           0.6       1.0X
Double                                         824 /  835       1018.0           1.0       0.6X
```


## How was this patch tested?

Use existing testsuites",spark,apache,kiszk,1315079,MDQ6VXNlcjEzMTUwNzk=,https://avatars2.githubusercontent.com/u/1315079?v=4,,https://api.github.com/users/kiszk,https://github.com/kiszk,https://api.github.com/users/kiszk/followers,https://api.github.com/users/kiszk/following{/other_user},https://api.github.com/users/kiszk/gists{/gist_id},https://api.github.com/users/kiszk/starred{/owner}{/repo},https://api.github.com/users/kiszk/subscriptions,https://api.github.com/users/kiszk/orgs,https://api.github.com/users/kiszk/repos,https://api.github.com/users/kiszk/events{/privacy},https://api.github.com/users/kiszk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19472,https://github.com/apache/spark/pull/19472,https://github.com/apache/spark/pull/19472.diff,https://github.com/apache/spark/pull/19472.patch
470,https://api.github.com/repos/apache/spark/issues/19433,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19433/labels{/name},https://api.github.com/repos/apache/spark/issues/19433/comments,https://api.github.com/repos/apache/spark/issues/19433/events,https://github.com/apache/spark/pull/19433,262951215,MDExOlB1bGxSZXF1ZXN0MTQ0NzkxNDM4,19433,[SPARK-3162] [MLlib] Add local tree training for decision tree regressors,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2017-10-04T22:30:57Z,2019-09-16T19:27:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

### Overview
This PR adds local tree training for decision tree regressors as a first step for addressing [SPARK-3162](https://issues.apache.org/jira/browse/SPARK-3162): train decision trees locally when possible.

See [this design doc](https://docs.google.com/document/d/1baU5KeorrmLpC4EZoqLuG-E8sUJqmdELLbr8o6wdbVM/edit) (in particular the [local tree training section](https://docs.google.com/document/d/1baU5KeorrmLpC4EZoqLuG-E8sUJqmdELLbr8o6wdbVM/edit#heading=h.eiteb09wxsjf)) for detailed discussion of the proposed changes.

Distributed training logic has been refactored but only minimally modified; the local tree training implementation leverages existing distributed training logic for computing impurities and splits. This shared logic has been refactored into `...Utils` objects (e.g. `SplitUtils.scala`, `ImpurityUtils.scala`). 

### How to Review

Each commit in this PR adds non-overlapping functionality, so the PR can be reviewed commit-by-commit.

Changes introduced by each commit:
1. Adds new data structures for local tree training (`FeatureVector`, `TrainingInfo`)
2. Adds shared utility methods for computing splits/impurities (`SplitUtils`, `ImpurityUtils`, `AggUpdateUtils`), largely copied from existing distributed training code in `RandomForest.scala`.
3. Unit tests for split/impurity utility methods (`TreeSplitUtilsSuite`)
4. Updates distributed training code in `RandomForest.scala` to depend on the utility methods introduced in 2.
5. Adds local tree training logic (`LocalDecisionTree`) 
6. Local tree unit/integration tests (`LocalTreeUnitSuite`, `LocalTreeIntegrationSuite`)

## How was this patch tested?
No existing tests were modified. The following new tests were added (also described above):
* Unit tests for new data structures specific to local tree training (`LocalTreeDataSuite`, `LocalTreeUtilsSuite`)
* Unit tests for impurity/split utility methods (`TreeSplitUtilsSuite`)
* Unit tests for local tree training logic (`LocalTreeUnitSuite`)
* Integration tests verifying that local & distributed tree training produce the same trees (`LocalTreeIntegrationSuite`)",spark,apache,smurching,2358483,MDQ6VXNlcjIzNTg0ODM=,https://avatars1.githubusercontent.com/u/2358483?v=4,,https://api.github.com/users/smurching,https://github.com/smurching,https://api.github.com/users/smurching/followers,https://api.github.com/users/smurching/following{/other_user},https://api.github.com/users/smurching/gists{/gist_id},https://api.github.com/users/smurching/starred{/owner}{/repo},https://api.github.com/users/smurching/subscriptions,https://api.github.com/users/smurching/orgs,https://api.github.com/users/smurching/repos,https://api.github.com/users/smurching/events{/privacy},https://api.github.com/users/smurching/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19433,https://github.com/apache/spark/pull/19433,https://github.com/apache/spark/pull/19433.diff,https://github.com/apache/spark/pull/19433.patch
471,https://api.github.com/repos/apache/spark/issues/19410,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19410/labels{/name},https://api.github.com/repos/apache/spark/issues/19410/comments,https://api.github.com/repos/apache/spark/issues/19410/events,https://github.com/apache/spark/pull/19410,262118056,MDExOlB1bGxSZXF1ZXN0MTQ0MjA3ODM0,19410,[SPARK-22184][CORE][GRAPHX] GraphX fails in case of insufficient memory and checkpoints enabled,"[{'id': 1406607243, 'node_id': 'MDU6TGFiZWwxNDA2NjA3MjQz', 'url': 'https://api.github.com/repos/apache/spark/labels/GRAPHX', 'name': 'GRAPHX', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2017-10-02T15:08:27Z,2019-09-16T18:26:42Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Fix for [SPARK-22184](https://issues.apache.org/jira/browse/SPARK-22184) JIRA issue (and also includes the related #19373).

In case of GraphX jobs, when checkpoints are enabled, GraphX can fail with `FileNotFoundException`.

The failure can happen during Pregel iterations or when Pregel completes only in cases of insufficient memory when checkpointed RDDs are evicted from memory and have to be read from disk (but already removed from there).

This PR proposes to preserve all the checkpoints the last one (checkpoint) of `messages` and `graph` depends on during the iterations, and all the checkpoints of `messages` and `graph` the resulting `graph` depends at the end of Pregel iterations.

## How was this patch tested?

Unit tests as well as manually in production jobs which previously were failing until this PR applied.",spark,apache,szhem,1523889,MDQ6VXNlcjE1MjM4ODk=,https://avatars1.githubusercontent.com/u/1523889?v=4,,https://api.github.com/users/szhem,https://github.com/szhem,https://api.github.com/users/szhem/followers,https://api.github.com/users/szhem/following{/other_user},https://api.github.com/users/szhem/gists{/gist_id},https://api.github.com/users/szhem/starred{/owner}{/repo},https://api.github.com/users/szhem/subscriptions,https://api.github.com/users/szhem/orgs,https://api.github.com/users/szhem/repos,https://api.github.com/users/szhem/events{/privacy},https://api.github.com/users/szhem/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19410,https://github.com/apache/spark/pull/19410,https://github.com/apache/spark/pull/19410.diff,https://github.com/apache/spark/pull/19410.patch
472,https://api.github.com/repos/apache/spark/issues/19408,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19408/labels{/name},https://api.github.com/repos/apache/spark/issues/19408/comments,https://api.github.com/repos/apache/spark/issues/19408/events,https://github.com/apache/spark/pull/19408,261921843,MDExOlB1bGxSZXF1ZXN0MTQ0MDczOTQw,19408,[SPARK-22180][CORE] Allow IPv6 address in org.apache.spark.util.Utils.parseHostPort,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2017-10-01T14:55:34Z,2019-09-16T18:26:40Z,,NONE,"External applications like Apache Cassandra are able to deal with IPv6 addresses. Libraries like spark-cassandra-connector combine Apache Cassandra with Apache Spark.
This combination is very useful IMHO.

One problem is that `org.apache.spark.util.Utils.parseHostPort(hostPort:` `String)` takes the last colon to sepperate the port from host path. This conflicts with literal IPv6 addresses.

I think we can take `hostPort` as literal IPv6 address if it contains tow ore more colons. If IPv6 addresses are enclosed in square brackets port definition is still possible.
",spark,apache,obermeier,1632575,MDQ6VXNlcjE2MzI1NzU=,https://avatars3.githubusercontent.com/u/1632575?v=4,,https://api.github.com/users/obermeier,https://github.com/obermeier,https://api.github.com/users/obermeier/followers,https://api.github.com/users/obermeier/following{/other_user},https://api.github.com/users/obermeier/gists{/gist_id},https://api.github.com/users/obermeier/starred{/owner}{/repo},https://api.github.com/users/obermeier/subscriptions,https://api.github.com/users/obermeier/orgs,https://api.github.com/users/obermeier/repos,https://api.github.com/users/obermeier/events{/privacy},https://api.github.com/users/obermeier/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19408,https://github.com/apache/spark/pull/19408,https://github.com/apache/spark/pull/19408.diff,https://github.com/apache/spark/pull/19408.patch
473,https://api.github.com/repos/apache/spark/issues/19373,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19373/labels{/name},https://api.github.com/repos/apache/spark/issues/19373/comments,https://api.github.com/repos/apache/spark/issues/19373/events,https://github.com/apache/spark/pull/19373,261130310,MDExOlB1bGxSZXF1ZXN0MTQzNTA5ODIy,19373,[SPARK-22150][CORE] PeriodicCheckpointer fails in case of dependent RDDs,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2017-09-27T21:54:57Z,2019-09-16T18:26:36Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Fix for [SPARK-22150](https://issues.apache.org/jira/browse/SPARK-22150) JIRA issue.

In case of checkpointing RDDs which depend on previously checkpointed RDDs (for example in iterative algorithms) PeriodicCheckpointer removes already checkpointed materialized RDDs too early leading to FileNotFoundExceptions.

Consider the following snippet

    // create a periodic checkpointer with interval of 2
    val checkpointer = new PeriodicRDDCheckpointer[Double](2, sc)
    
    val rdd1 = createRDD(sc)
    checkpointer.update(rdd1)
    // on the second update rdd1 is checkpointed
    checkpointer.update(rdd1)
    // on action checkpointed rdd is materialized and its lineage is truncated
    rdd1.count() 
    
    // rdd2 depends on rdd1
    val rdd2 = rdd1.filter(_ => true)
    checkpointer.update(rdd2)
    // on the second update rdd2 is checkpointed and checkpoint files of rdd1 are deleted
    checkpointer.update(rdd2)
    // on action it's necessary to read already removed checkpoint files of rdd1
    rdd2.count()

This PR proposes to preserve all the checkpoints the last one depends on to be able to evaluate the final RDD even if the last checkpoint (the final RDD depends on) is not yet materialized.

## How was this patch tested?

Unit tests as well as manually in production jobs which previously were failing until this PR applied.",spark,apache,szhem,1523889,MDQ6VXNlcjE1MjM4ODk=,https://avatars1.githubusercontent.com/u/1523889?v=4,,https://api.github.com/users/szhem,https://github.com/szhem,https://api.github.com/users/szhem/followers,https://api.github.com/users/szhem/following{/other_user},https://api.github.com/users/szhem/gists{/gist_id},https://api.github.com/users/szhem/starred{/owner}{/repo},https://api.github.com/users/szhem/subscriptions,https://api.github.com/users/szhem/orgs,https://api.github.com/users/szhem/repos,https://api.github.com/users/szhem/events{/privacy},https://api.github.com/users/szhem/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19373,https://github.com/apache/spark/pull/19373,https://github.com/apache/spark/pull/19373.diff,https://github.com/apache/spark/pull/19373.patch
474,https://api.github.com/repos/apache/spark/issues/19194,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19194/labels{/name},https://api.github.com/repos/apache/spark/issues/19194/comments,https://api.github.com/repos/apache/spark/issues/19194/events,https://github.com/apache/spark/pull/19194,256771441,MDExOlB1bGxSZXF1ZXN0MTQwMzg0MDYz,19194,[SPARK-20589] Allow limiting task concurrency per stage,"[{'id': 1406606875, 'node_id': 'MDU6TGFiZWwxNDA2NjA2ODc1', 'url': 'https://api.github.com/repos/apache/spark/labels/SCHEDULER', 'name': 'SCHEDULER', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2017-09-11T16:50:28Z,2019-09-16T19:27:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This change allows the user to specify the maximum no. of tasks running in a given job group. (Kindly see the jira comments section for more context on why this is implemented at a job group level rather than a stage level). This change is beneficial where the user wants to avoid having a DoS while trying to access an external service from multiple executors without having the need to repartition or coalesce existing RDDs.

This code change introduces a new user level configuration: `spark.job.[userJobGroup].maxConcurrentTasks` which is used to set the active no. of tasks executing at a given point in time.

The user can use the feature by setting the appropriate jobGroup and passing the conf:

```
conf.set(""spark.job.group1.maxConcurrentTasks"", ""10"")
...
sc.setJobGroup(""group1"", """", false)
sc.parallelize(1 to 100000, 10).map(x => x + 1).count
sc.clearJobGroup
```

#### changes proposed in this fix 
This change limits the no. of tasks (in turn also the no. of executors to be acquired) than can run simultaneously in a given job group and its subsequent job/s and stage/s if the appropriate job group and max concurrency configs are set.

## How was this patch tested?
Ran unit tests and multiple manual tests with various combinations of:
- single/multiple/no job groups
- executors with single/multi cores
- dynamic allocation on/off
",spark,apache,dhruve,7732317,MDQ6VXNlcjc3MzIzMTc=,https://avatars1.githubusercontent.com/u/7732317?v=4,,https://api.github.com/users/dhruve,https://github.com/dhruve,https://api.github.com/users/dhruve/followers,https://api.github.com/users/dhruve/following{/other_user},https://api.github.com/users/dhruve/gists{/gist_id},https://api.github.com/users/dhruve/starred{/owner}{/repo},https://api.github.com/users/dhruve/subscriptions,https://api.github.com/users/dhruve/orgs,https://api.github.com/users/dhruve/repos,https://api.github.com/users/dhruve/events{/privacy},https://api.github.com/users/dhruve/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19194,https://github.com/apache/spark/pull/19194,https://github.com/apache/spark/pull/19194.diff,https://github.com/apache/spark/pull/19194.patch
475,https://api.github.com/repos/apache/spark/issues/19054,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/19054/labels{/name},https://api.github.com/repos/apache/spark/issues/19054/comments,https://api.github.com/repos/apache/spark/issues/19054/events,https://github.com/apache/spark/pull/19054,252932765,MDExOlB1bGxSZXF1ZXN0MTM3Njk1Nzk5,19054,[SPARK-18067] Avoid shuffling child if join keys are superset of child's partitioning keys,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2017-08-25T15:14:16Z,2019-09-16T19:27:06Z,,CONTRIBUTOR,"Jira : https://issues.apache.org/jira/browse/SPARK-18067

## What problem is being addressed in this PR ?

Currently shuffle based joins require its children to be shuffled over all the columns in the join condition. In case the child node is already distributed over a subset of columns in the join condition, this shuffle is not needed (eg. if the input is bucketed, if the input is output of a subquery). Avoiding the shuffle makes the join run faster and more stably as its single stage.

To dive deeper, lets look at this example. Both input tables `table1` and `table2` are bucketed on columns `i` and `j` and have 8 buckets. The query is joining the 2 tables over `i,j,k`. With bucketing, all the rows with the same values of `i` and `j` should reside in the same bucket of both the inputs. So, if we simply sort the corresponding buckets over the join columns and perform the join, that would suffice the requirements.

| partitions  | table1 (i,j,k) values | table2 (i,j,k) values  |
| ------------- | ------------- | ------------- |
| bucket 0  | (0,0,1)  (0,0,2)  (1,0,4) | (0,0,1)  (0,0,3) |
| bucket 1  | (1,0,2) (1,1,1) | (1,0,1) (1,0,2) (1,1,2) |
| bucket 2  | (0,1,8) (0,1,6) | (0,1,1) |

## What changes were proposed in this pull request?

Both shuffled hash join and sort merge join would not keep track of `which keys should the children be distributed on ?`. To start off, this is same as the join keys. The rule `ReorderJoinPredicates` is modified to detect if the child's output partitioning is over a subset of join keys and based on that the distribution keys for the join operator are revised.

## How was this patch tested?

- Added unit test.
- manual test:

Query:
```
-- both the input tables are bucketed over columns `i` and `j`
SELECT * FROM table1 a JOIN table2 b ON a.i = b.i AND a.j = b.j AND a.k = b.k
```

BEFORE
```
SortMergeJoin [i#5, j#6, k#7], [i#8, j#9, k#10], Inner
:- *Sort [i#5 ASC NULLS FIRST, j#6 ASC NULLS FIRST, k#7 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(i#5, j#6, k#7, 200)
:     +- *FileScan orc default.table1[i#5,j#6,k#7] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:warehouse/table1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i:int,j:int,k:string>
+- *Sort [i#8 ASC NULLS FIRST, j#9 ASC NULLS FIRST, k#10 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(i#8, j#9, k#10, 200)
      +- *FileScan orc default.table2[i#8,j#9,k#10] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:warehouse/table2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i:int,j:int,k:string>
```

AFTER
```
SortMergeJoin [i#5, j#6, k#7], [i#8, j#9, k#10], [i#5, j#6], [i#8, j#9], Inner
:- *Sort [i#5 ASC NULLS FIRST, j#6 ASC NULLS FIRST, k#7 ASC NULLS FIRST], false, 0
:  +- *FileScan orc default.table1[i#5,j#6,k#7] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:warehouse/table1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i:int,j:int,k:string>
+- *Sort [i#8 ASC NULLS FIRST, j#9 ASC NULLS FIRST, k#10 ASC NULLS FIRST], false, 0
   +- *FileScan orc default.table2[i#8,j#9,k#10] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:warehouse/table2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i:int,j:int,k:string>
```
",spark,apache,tejasapatil,3230057,MDQ6VXNlcjMyMzAwNTc=,https://avatars1.githubusercontent.com/u/3230057?v=4,,https://api.github.com/users/tejasapatil,https://github.com/tejasapatil,https://api.github.com/users/tejasapatil/followers,https://api.github.com/users/tejasapatil/following{/other_user},https://api.github.com/users/tejasapatil/gists{/gist_id},https://api.github.com/users/tejasapatil/starred{/owner}{/repo},https://api.github.com/users/tejasapatil/subscriptions,https://api.github.com/users/tejasapatil/orgs,https://api.github.com/users/tejasapatil/repos,https://api.github.com/users/tejasapatil/events{/privacy},https://api.github.com/users/tejasapatil/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/19054,https://github.com/apache/spark/pull/19054,https://github.com/apache/spark/pull/19054.diff,https://github.com/apache/spark/pull/19054.patch
476,https://api.github.com/repos/apache/spark/issues/18994,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18994/labels{/name},https://api.github.com/repos/apache/spark/issues/18994/comments,https://api.github.com/repos/apache/spark/issues/18994/events,https://github.com/apache/spark/pull/18994,251300936,MDExOlB1bGxSZXF1ZXN0MTM2NTIxMzAy,18994,[SPARK-21784][SQL] Adds support for defining informational primary key and foreign key constraints using ALTER TABLE DDL.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2017-08-18T16:48:55Z,2019-11-16T09:56:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This PR implements ALTER TABLE DDL ADD CONSTRAINT  to  add  informational primary key and foreign key (referential integrity) constraints in Spark. These constraints will be used in query optimization and you can find more details about this in the spec in  [SPARK-19842](https://issues.apache.org/jira/browse/SPARK-19842)
 
The proposed syntax of the constraints DDL is similar to the Hive  2.1 referential integrity constraints  support (https://issues.apache.org/jira/browse/HIVE-13076) which is aligned to Oracle's semantics.
 
Syntax:
```sql
ALTER TABLE [db_name.]table_name ADD [CONSTRAINT constraintName]
  (PRIMARY KEY (col_names) |
  FOREIGN KEY (col_names) REFERENCES [db_name.]table_name [(col_names)])
  [VALIDATE | NOVALIDATE] [RELY | NORELY]
```
Examples :
 ```sql
ALTER TABLE employee ADD CONSTRANT pk  PRIMARY KEY(empno) VALIDATE RELY
ALTER TABLE department ADD CONSTRAINT emp_fk FOREIGN KEY (mgrno) REFERENCES employee(empno) NOVALIDATE NORELY
ALTER TABLE department ADD PRIMARY KEY(deptno) VALIDATE RELY
ALTER TABLE employee ADD FOREIGN KEY (workdept) REFERENCES department(deptno) VALIDATE RELY;
 ```
The constraint information is stored in the table properties as JSON string for each constraint. 
One of the advantages of  storing constraints in the table properties is that this functionality  will work in all the supported Hive metastore versions. 
 
An alternative approach  that we considered was to store the constraints information using the hive metastore API that stores the constraints in a separate table.  The problem with this approach is this feature will only work in Spark installations that use **Hive 2.1 metastore**, and also this version is NOT the current  spark default.  More details are in the spec document.
 
**This PR implements the  ALTER TABLE constraint DDL using table properties because it is important to work with default hive metastore version of the spark.** 
 
The syntax to define the  constraints as part of _create table_  definition will be implemented in a follow-up Jira.


## How was this patch tested?
Added  new unit test cases to HiveDDLSuite, and SparkSqlParserSuite
",spark,apache,sureshthalamati,13815184,MDQ6VXNlcjEzODE1MTg0,https://avatars0.githubusercontent.com/u/13815184?v=4,,https://api.github.com/users/sureshthalamati,https://github.com/sureshthalamati,https://api.github.com/users/sureshthalamati/followers,https://api.github.com/users/sureshthalamati/following{/other_user},https://api.github.com/users/sureshthalamati/gists{/gist_id},https://api.github.com/users/sureshthalamati/starred{/owner}{/repo},https://api.github.com/users/sureshthalamati/subscriptions,https://api.github.com/users/sureshthalamati/orgs,https://api.github.com/users/sureshthalamati/repos,https://api.github.com/users/sureshthalamati/events{/privacy},https://api.github.com/users/sureshthalamati/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18994,https://github.com/apache/spark/pull/18994,https://github.com/apache/spark/pull/18994.diff,https://github.com/apache/spark/pull/18994.patch
477,https://api.github.com/repos/apache/spark/issues/18986,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18986/labels{/name},https://api.github.com/repos/apache/spark/issues/18986/comments,https://api.github.com/repos/apache/spark/issues/18986/events,https://github.com/apache/spark/pull/18986,251128620,MDExOlB1bGxSZXF1ZXN0MTM2Mzk0NDU3,18986,[SPARK-21774][SQL] The rule PromoteStrings should cast a string to double type when compare with a int/long,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2017-08-18T03:16:54Z,2019-11-11T05:33:35Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

The rule PromoteStrings should cast a string to double type when compare with a int.

This PR fixed this.

## How was this patch tested?

Origin test cases updated.",spark,apache,stanzhai,1438757,MDQ6VXNlcjE0Mzg3NTc=,https://avatars0.githubusercontent.com/u/1438757?v=4,,https://api.github.com/users/stanzhai,https://github.com/stanzhai,https://api.github.com/users/stanzhai/followers,https://api.github.com/users/stanzhai/following{/other_user},https://api.github.com/users/stanzhai/gists{/gist_id},https://api.github.com/users/stanzhai/starred{/owner}{/repo},https://api.github.com/users/stanzhai/subscriptions,https://api.github.com/users/stanzhai/orgs,https://api.github.com/users/stanzhai/repos,https://api.github.com/users/stanzhai/events{/privacy},https://api.github.com/users/stanzhai/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18986,https://github.com/apache/spark/pull/18986,https://github.com/apache/spark/pull/18986.diff,https://github.com/apache/spark/pull/18986.patch
478,https://api.github.com/repos/apache/spark/issues/18906,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18906/labels{/name},https://api.github.com/repos/apache/spark/issues/18906/comments,https://api.github.com/repos/apache/spark/issues/18906/events,https://github.com/apache/spark/pull/18906,249384008,MDExOlB1bGxSZXF1ZXN0MTM1MTUzMDk4,18906,[SPARK-21692][PYSPARK][SQL] Add nullability support to PythonUDF.,"[{'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,61,2017-08-10T15:22:59Z,2019-12-15T14:03:49Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
When registering a Python UDF, a user may know whether the function can return null values or not. PythonUDF and all related classes should handle nullability.

## How was this patch tested?
Existing tests and a few new tests.
",spark,apache,ptkool,20175793,MDQ6VXNlcjIwMTc1Nzkz,https://avatars2.githubusercontent.com/u/20175793?v=4,,https://api.github.com/users/ptkool,https://github.com/ptkool,https://api.github.com/users/ptkool/followers,https://api.github.com/users/ptkool/following{/other_user},https://api.github.com/users/ptkool/gists{/gist_id},https://api.github.com/users/ptkool/starred{/owner}{/repo},https://api.github.com/users/ptkool/subscriptions,https://api.github.com/users/ptkool/orgs,https://api.github.com/users/ptkool/repos,https://api.github.com/users/ptkool/events{/privacy},https://api.github.com/users/ptkool/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18906,https://github.com/apache/spark/pull/18906,https://github.com/apache/spark/pull/18906.diff,https://github.com/apache/spark/pull/18906.patch
479,https://api.github.com/repos/apache/spark/issues/18898,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18898/labels{/name},https://api.github.com/repos/apache/spark/issues/18898/comments,https://api.github.com/repos/apache/spark/issues/18898/events,https://github.com/apache/spark/pull/18898,249200155,MDExOlB1bGxSZXF1ZXN0MTM1MDE5MDU4,18898,[SPARK-21245][ML] Resolve code duplication for classification/regression summarizers,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2017-08-10T00:33:16Z,2019-10-18T22:48:05Z,,CONTRIBUTOR,"## Why the change?

In several places (LogReg, LinReg, SVC) in Spark ML, we collect summary information about training data using `MultivariateOnlineSummarizer` and `MulticlassSummarizer`. We have the same code appearing in several places (including test suites). We can eliminate this by creating a common implementation.

## What changes were proposed in this pull request?

1. A new class `ml.stat.Summarizers.scala` with `def getRegressionSummarizers` and `def getClassificationSummarizers` that provides a pair of feature and label summarizers.
This centralizes the duplicated code in: `LinearRegression`, `LinearSVC`, `LogisticRegression` and `DifferentiableLossAggregatorSuite`.
2. Moves `MultiClassSummarizer.scala`(and testSuite) out of `LogisticRegression.scala` to new file `ml.stat.MultiClassSummarizer.scala`, because it is also used by `LinearSVC` and can be generalized.

## How was this patch tested?

`ml.stat.SummarizersSuite.scala`",spark,apache,jiayue-zhang,15841739,MDQ6VXNlcjE1ODQxNzM5,https://avatars2.githubusercontent.com/u/15841739?v=4,,https://api.github.com/users/jiayue-zhang,https://github.com/jiayue-zhang,https://api.github.com/users/jiayue-zhang/followers,https://api.github.com/users/jiayue-zhang/following{/other_user},https://api.github.com/users/jiayue-zhang/gists{/gist_id},https://api.github.com/users/jiayue-zhang/starred{/owner}{/repo},https://api.github.com/users/jiayue-zhang/subscriptions,https://api.github.com/users/jiayue-zhang/orgs,https://api.github.com/users/jiayue-zhang/repos,https://api.github.com/users/jiayue-zhang/events{/privacy},https://api.github.com/users/jiayue-zhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18898,https://github.com/apache/spark/pull/18898,https://github.com/apache/spark/pull/18898.diff,https://github.com/apache/spark/pull/18898.patch
480,https://api.github.com/repos/apache/spark/issues/18729,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18729/labels{/name},https://api.github.com/repos/apache/spark/issues/18729/comments,https://api.github.com/repos/apache/spark/issues/18729/events,https://github.com/apache/spark/pull/18729,245253407,MDExOlB1bGxSZXF1ZXN0MTMyMTczODUx,18729,[SPARK-21526] [MLlib] Add support to ML LogisticRegression for setting initial model,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2017-07-25T00:37:25Z,2019-12-05T17:12:06Z,,NONE,"## What changes were proposed in this pull request?

* Change ML LogisticRegression setInitialModel from private to public.
* Add getInitialModel method.

## How was this patch tested?

Running `./dev/run-tests`.
",spark,apache,JohnHBrock,3038603,MDQ6VXNlcjMwMzg2MDM=,https://avatars0.githubusercontent.com/u/3038603?v=4,,https://api.github.com/users/JohnHBrock,https://github.com/JohnHBrock,https://api.github.com/users/JohnHBrock/followers,https://api.github.com/users/JohnHBrock/following{/other_user},https://api.github.com/users/JohnHBrock/gists{/gist_id},https://api.github.com/users/JohnHBrock/starred{/owner}{/repo},https://api.github.com/users/JohnHBrock/subscriptions,https://api.github.com/users/JohnHBrock/orgs,https://api.github.com/users/JohnHBrock/repos,https://api.github.com/users/JohnHBrock/events{/privacy},https://api.github.com/users/JohnHBrock/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18729,https://github.com/apache/spark/pull/18729,https://github.com/apache/spark/pull/18729.diff,https://github.com/apache/spark/pull/18729.patch
481,https://api.github.com/repos/apache/spark/issues/18666,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18666/labels{/name},https://api.github.com/repos/apache/spark/issues/18666/comments,https://api.github.com/repos/apache/spark/issues/18666/events,https://github.com/apache/spark/pull/18666,243589380,MDExOlB1bGxSZXF1ZXN0MTMwOTk1NDk0,18666,[SPARK-21449][SQL][Hive]Close HiveClient's SessionState to delete residual dirs,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2017-07-18T03:27:59Z,2019-09-16T19:28:06Z,,CONTRIBUTOR,"
## What changes were proposed in this pull request?

When sparkSession.stop() is called, close the hive client too.

## How was this patch tested?

manully",spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18666,https://github.com/apache/spark/pull/18666,https://github.com/apache/spark/pull/18666.diff,https://github.com/apache/spark/pull/18666.patch
482,https://api.github.com/repos/apache/spark/issues/18542,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18542/labels{/name},https://api.github.com/repos/apache/spark/issues/18542/comments,https://api.github.com/repos/apache/spark/issues/18542/events,https://github.com/apache/spark/pull/18542,240662454,MDExOlB1bGxSZXF1ZXN0MTI4OTcxNTE5,18542,[SPARK-21317][SQL] Avoid sorting on bucket expression if data is already bucketed,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2017-07-05T14:06:33Z,2019-09-16T18:27:25Z,,NONE,"## What changes were proposed in this pull request?

When saving bucketed data, each task will sort it's partition by the partition id produced by the HashPartitioning of the bucket column. If the input data is already partitioned by the bucket partitioning, then this value will be constant, making the sort unnecessary.

## How was this patch tested?
Manual tests + intellij debugger. Not sure what the easiest way to plug into verifying the plan inside the writer would have a sort or not.",spark,apache,pwoody,11271328,MDQ6VXNlcjExMjcxMzI4,https://avatars2.githubusercontent.com/u/11271328?v=4,,https://api.github.com/users/pwoody,https://github.com/pwoody,https://api.github.com/users/pwoody/followers,https://api.github.com/users/pwoody/following{/other_user},https://api.github.com/users/pwoody/gists{/gist_id},https://api.github.com/users/pwoody/starred{/owner}{/repo},https://api.github.com/users/pwoody/subscriptions,https://api.github.com/users/pwoody/orgs,https://api.github.com/users/pwoody/repos,https://api.github.com/users/pwoody/events{/privacy},https://api.github.com/users/pwoody/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18542,https://github.com/apache/spark/pull/18542,https://github.com/apache/spark/pull/18542.diff,https://github.com/apache/spark/pull/18542.patch
483,https://api.github.com/repos/apache/spark/issues/18423,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18423/labels{/name},https://api.github.com/repos/apache/spark/issues/18423/comments,https://api.github.com/repos/apache/spark/issues/18423/events,https://github.com/apache/spark/pull/18423,238628574,MDExOlB1bGxSZXF1ZXN0MTI3NTMzMDQ4,18423,SPARK-21158: Fix case sensitive of temp table/view name,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2017-06-26T18:24:02Z,2019-09-16T18:27:23Z,,NONE,"## What changes were proposed in this pull request?

Most changes have been made to SessionCatalog source file. Shouldn't convert table/view name into lowercase. The hash-table should always contain the exact input name of table. 

## How was this patch tested?

This patch wast tested around containLocalTable and containGlobalTable functions manually for case-sensitive and in-sensitive. They are very simple functions

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,cammachusa,28015823,MDQ6VXNlcjI4MDE1ODIz,https://avatars1.githubusercontent.com/u/28015823?v=4,,https://api.github.com/users/cammachusa,https://github.com/cammachusa,https://api.github.com/users/cammachusa/followers,https://api.github.com/users/cammachusa/following{/other_user},https://api.github.com/users/cammachusa/gists{/gist_id},https://api.github.com/users/cammachusa/starred{/owner}{/repo},https://api.github.com/users/cammachusa/subscriptions,https://api.github.com/users/cammachusa/orgs,https://api.github.com/users/cammachusa/repos,https://api.github.com/users/cammachusa/events{/privacy},https://api.github.com/users/cammachusa/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18423,https://github.com/apache/spark/pull/18423,https://github.com/apache/spark/pull/18423.diff,https://github.com/apache/spark/pull/18423.patch
484,https://api.github.com/repos/apache/spark/issues/18410,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18410/labels{/name},https://api.github.com/repos/apache/spark/issues/18410/comments,https://api.github.com/repos/apache/spark/issues/18410/events,https://github.com/apache/spark/pull/18410,238268225,MDExOlB1bGxSZXF1ZXN0MTI3Mjk5ODAw,18410,[SPARK-20971][SS] purge metadata log in FileStreamSource,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2017-06-23T22:31:38Z,2019-09-16T19:29:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Currently, there is no cleanup mechanism for FileStreamSource's metadata log so that the data is growing infinitely

This PR purges the log which is out of the retaining windowing

## How was this patch tested?

existing tests",spark,apache,CodingCat,678008,MDQ6VXNlcjY3ODAwOA==,https://avatars1.githubusercontent.com/u/678008?v=4,,https://api.github.com/users/CodingCat,https://github.com/CodingCat,https://api.github.com/users/CodingCat/followers,https://api.github.com/users/CodingCat/following{/other_user},https://api.github.com/users/CodingCat/gists{/gist_id},https://api.github.com/users/CodingCat/starred{/owner}{/repo},https://api.github.com/users/CodingCat/subscriptions,https://api.github.com/users/CodingCat/orgs,https://api.github.com/users/CodingCat/repos,https://api.github.com/users/CodingCat/events{/privacy},https://api.github.com/users/CodingCat/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18410,https://github.com/apache/spark/pull/18410,https://github.com/apache/spark/pull/18410.diff,https://github.com/apache/spark/pull/18410.patch
485,https://api.github.com/repos/apache/spark/issues/18406,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18406/labels{/name},https://api.github.com/repos/apache/spark/issues/18406/comments,https://api.github.com/repos/apache/spark/issues/18406/events,https://github.com/apache/spark/pull/18406,238150845,MDExOlB1bGxSZXF1ZXN0MTI3MjEzNDgw,18406,[SPARK-21195] Automatically register new metrics from sources and wire default registry,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2017-06-23T13:59:55Z,2019-12-06T13:42:20Z,,NONE,"## What changes were proposed in this pull request?
Registers metric listeners on sources metrics that forward actions to metricssystem metric registry. Hooks into default metric registry for easier integration with non spark specific libraries

## How was this patch tested?
Added tests
",spark,apache,robert3005,512084,MDQ6VXNlcjUxMjA4NA==,https://avatars0.githubusercontent.com/u/512084?v=4,,https://api.github.com/users/robert3005,https://github.com/robert3005,https://api.github.com/users/robert3005/followers,https://api.github.com/users/robert3005/following{/other_user},https://api.github.com/users/robert3005/gists{/gist_id},https://api.github.com/users/robert3005/starred{/owner}{/repo},https://api.github.com/users/robert3005/subscriptions,https://api.github.com/users/robert3005/orgs,https://api.github.com/users/robert3005/repos,https://api.github.com/users/robert3005/events{/privacy},https://api.github.com/users/robert3005/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18406,https://github.com/apache/spark/pull/18406,https://github.com/apache/spark/pull/18406.diff,https://github.com/apache/spark/pull/18406.patch
486,https://api.github.com/repos/apache/spark/issues/18306,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18306/labels{/name},https://api.github.com/repos/apache/spark/issues/18306/comments,https://api.github.com/repos/apache/spark/issues/18306/events,https://github.com/apache/spark/pull/18306,236000968,MDExOlB1bGxSZXF1ZXN0MTI1Njg2Mzg1,18306,[SPARK-21029][SS] All StreamingQuery should be stopped when the SparkSession is stopped,"[{'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2017-06-14T20:32:11Z,2019-09-16T19:29:05Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Adds method to `StreamingQueryManager` that stops all active queries. Calls this method when `SparkSession.stop` is called.

## How was this patch tested?

Two unit tests.",spark,apache,aray,981982,MDQ6VXNlcjk4MTk4Mg==,https://avatars1.githubusercontent.com/u/981982?v=4,,https://api.github.com/users/aray,https://github.com/aray,https://api.github.com/users/aray/followers,https://api.github.com/users/aray/following{/other_user},https://api.github.com/users/aray/gists{/gist_id},https://api.github.com/users/aray/starred{/owner}{/repo},https://api.github.com/users/aray/subscriptions,https://api.github.com/users/aray/orgs,https://api.github.com/users/aray/repos,https://api.github.com/users/aray/events{/privacy},https://api.github.com/users/aray/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18306,https://github.com/apache/spark/pull/18306,https://github.com/apache/spark/pull/18306.diff,https://github.com/apache/spark/pull/18306.patch
487,https://api.github.com/repos/apache/spark/issues/18193,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18193/labels{/name},https://api.github.com/repos/apache/spark/issues/18193/comments,https://api.github.com/repos/apache/spark/issues/18193/events,https://github.com/apache/spark/pull/18193,233416388,MDExOlB1bGxSZXF1ZXN0MTIzODY3OTAz,18193,[SPARK-15616] [SQL] CatalogRelation should fallback to HDFS size of partitions that are involved in Query for JoinSelection.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2017-06-04T07:04:21Z,2019-09-17T07:28:46Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
Currently if some partitions of a partitioned table are used in join operation we rely on Metastore returned size of table to calculate if we can convert the operation to Broadcast join. 
if Filter can prune some partitions, Hive can prune partition before determining to use broadcast joins according to HDFS size of partitions that are involved in Query.So sparkSQL needs it that can improve join's performance for partitioned table.

## How was this patch tested?
add unit tests.
",spark,apache,lianhuiwang,545478,MDQ6VXNlcjU0NTQ3OA==,https://avatars1.githubusercontent.com/u/545478?v=4,,https://api.github.com/users/lianhuiwang,https://github.com/lianhuiwang,https://api.github.com/users/lianhuiwang/followers,https://api.github.com/users/lianhuiwang/following{/other_user},https://api.github.com/users/lianhuiwang/gists{/gist_id},https://api.github.com/users/lianhuiwang/starred{/owner}{/repo},https://api.github.com/users/lianhuiwang/subscriptions,https://api.github.com/users/lianhuiwang/orgs,https://api.github.com/users/lianhuiwang/repos,https://api.github.com/users/lianhuiwang/events{/privacy},https://api.github.com/users/lianhuiwang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18193,https://github.com/apache/spark/pull/18193,https://github.com/apache/spark/pull/18193.diff,https://github.com/apache/spark/pull/18193.patch
488,https://api.github.com/repos/apache/spark/issues/18123,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/18123/labels{/name},https://api.github.com/repos/apache/spark/issues/18123/comments,https://api.github.com/repos/apache/spark/issues/18123/events,https://github.com/apache/spark/pull/18123,231707415,MDExOlB1bGxSZXF1ZXN0MTIyNzAwMzA2,18123,[SPARK-20903] [ML] Word2Vec Skip-Gram + Negative Sampling,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2017-05-26T18:51:03Z,2019-09-16T18:25:45Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This enhances [CBOW + Negative Sampling](https://github.com/apache/spark/pull/17673) to be able to estimate Skip-Gram with negative sampling as well.

## How was this patch tested?

Patch was tested using unit tests being contributed as a part of the PR

",spark,apache,shubhamchopra,6588487,MDQ6VXNlcjY1ODg0ODc=,https://avatars3.githubusercontent.com/u/6588487?v=4,,https://api.github.com/users/shubhamchopra,https://github.com/shubhamchopra,https://api.github.com/users/shubhamchopra/followers,https://api.github.com/users/shubhamchopra/following{/other_user},https://api.github.com/users/shubhamchopra/gists{/gist_id},https://api.github.com/users/shubhamchopra/starred{/owner}{/repo},https://api.github.com/users/shubhamchopra/subscriptions,https://api.github.com/users/shubhamchopra/orgs,https://api.github.com/users/shubhamchopra/repos,https://api.github.com/users/shubhamchopra/events{/privacy},https://api.github.com/users/shubhamchopra/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/18123,https://github.com/apache/spark/pull/18123,https://github.com/apache/spark/pull/18123.diff,https://github.com/apache/spark/pull/18123.patch
489,https://api.github.com/repos/apache/spark/issues/17822,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17822/labels{/name},https://api.github.com/repos/apache/spark/issues/17822/comments,https://api.github.com/repos/apache/spark/issues/17822/events,https://github.com/apache/spark/pull/17822,225532355,MDExOlB1bGxSZXF1ZXN0MTE4NDQ1MTA0,17822,[SPARK-20454] [GraphX] Two Improvements of ShortestPaths in GraphX,"[{'id': 1406607243, 'node_id': 'MDU6TGFiZWwxNDA2NjA3MjQz', 'url': 'https://api.github.com/repos/apache/spark/labels/GRAPHX', 'name': 'GRAPHX', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2017-05-01T21:29:18Z,2019-09-27T18:27:07Z,,NONE,"## What changes were proposed in this pull request?
I made two improvements as follows, we can have broader usage on
shortest paths.
(1) Output multiple shortest paths if there are. It computes shortest
paths from a given source vertex to all other vertices. If several
paths have the same shortest distance between two vertices, all the
paths will be outputted.
(2) Support both weighted and unweighted graphs. The generalized
problem is weighted graph, and it is what my code works on.  You just
need to input the weighted graph as ‚Äôtrue‚Äô in the parameters. It can
also address the unweighted graph. If you input an unweighted graph, I
just set the weights of each edge as 1.

## How was this patch tested?
Unit tests.",spark,apache,daijidj,7347517,MDQ6VXNlcjczNDc1MTc=,https://avatars0.githubusercontent.com/u/7347517?v=4,,https://api.github.com/users/daijidj,https://github.com/daijidj,https://api.github.com/users/daijidj/followers,https://api.github.com/users/daijidj/following{/other_user},https://api.github.com/users/daijidj/gists{/gist_id},https://api.github.com/users/daijidj/starred{/owner}{/repo},https://api.github.com/users/daijidj/subscriptions,https://api.github.com/users/daijidj/orgs,https://api.github.com/users/daijidj/repos,https://api.github.com/users/daijidj/events{/privacy},https://api.github.com/users/daijidj/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17822,https://github.com/apache/spark/pull/17822,https://github.com/apache/spark/pull/17822.diff,https://github.com/apache/spark/pull/17822.patch
490,https://api.github.com/repos/apache/spark/issues/17520,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17520/labels{/name},https://api.github.com/repos/apache/spark/issues/17520/comments,https://api.github.com/repos/apache/spark/issues/17520/events,https://github.com/apache/spark/pull/17520,218995081,MDExOlB1bGxSZXF1ZXN0MTEzOTc0Njkx,17520,[WIP][SPARK-19712][SQL] Move PullupCorrelatedPredicates and RewritePredicateSubquery after OptimizeSubqueries,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2017-04-03T16:49:04Z,2019-09-16T19:31:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This commit moves two rules right next to the rule OptimizeSubqueries.

1. PullupCorrelatedPredicates: the rewrite of [Not] Exists and [Not] In (ListQuery) to PredicateSubquery
2. RewritePredicateSubquery: the rewrite of PredicateSubquery to LeftSemi/LeftAnti
  
With this change, [Not] Exists/In subquery is now rewritten to LeftSemi/LeftAnti at the beginning of Optimizer. 
One Todo is to merge the two-stage rewrite in rule PullupCorrelatedPredicates and rule RewritePredicateSubquery into a single stage rewrite rule.

## How was this patch tested?
Unit tests with test cases in SQLQueryTestSuite under the directory ./sql/core/src/test/resources/sql-tests/inputs/subquery. ",spark,apache,nsyca,18035343,MDQ6VXNlcjE4MDM1MzQz,https://avatars0.githubusercontent.com/u/18035343?v=4,,https://api.github.com/users/nsyca,https://github.com/nsyca,https://api.github.com/users/nsyca/followers,https://api.github.com/users/nsyca/following{/other_user},https://api.github.com/users/nsyca/gists{/gist_id},https://api.github.com/users/nsyca/starred{/owner}{/repo},https://api.github.com/users/nsyca/subscriptions,https://api.github.com/users/nsyca/orgs,https://api.github.com/users/nsyca/repos,https://api.github.com/users/nsyca/events{/privacy},https://api.github.com/users/nsyca/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17520,https://github.com/apache/spark/pull/17520,https://github.com/apache/spark/pull/17520.diff,https://github.com/apache/spark/pull/17520.patch
491,https://api.github.com/repos/apache/spark/issues/17365,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17365/labels{/name},https://api.github.com/repos/apache/spark/issues/17365/comments,https://api.github.com/repos/apache/spark/issues/17365/events,https://github.com/apache/spark/pull/17365,215552909,MDExOlB1bGxSZXF1ZXN0MTExNjQ4OTcz,17365,[SPARK-19962] [MLlib]  add DictVectorizer to ml.feature,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2017-03-20T20:57:09Z,2019-09-16T18:28:24Z,,NONE,"## What changes were proposed in this pull request?
add a new estimator`DictVectorizer` and transformer `DictVectorizerModel` for dataframe.

## How was this patch tested?

unit test in `mllib/src/test/scala/org/apache/spark/ml/feature/DictVectorizerSuite.scala`

",spark,apache,yupbank,741544,MDQ6VXNlcjc0MTU0NA==,https://avatars1.githubusercontent.com/u/741544?v=4,,https://api.github.com/users/yupbank,https://github.com/yupbank,https://api.github.com/users/yupbank/followers,https://api.github.com/users/yupbank/following{/other_user},https://api.github.com/users/yupbank/gists{/gist_id},https://api.github.com/users/yupbank/starred{/owner}{/repo},https://api.github.com/users/yupbank/subscriptions,https://api.github.com/users/yupbank/orgs,https://api.github.com/users/yupbank/repos,https://api.github.com/users/yupbank/events{/privacy},https://api.github.com/users/yupbank/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17365,https://github.com/apache/spark/pull/17365,https://github.com/apache/spark/pull/17365.diff,https://github.com/apache/spark/pull/17365.patch
492,https://api.github.com/repos/apache/spark/issues/17360,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17360/labels{/name},https://api.github.com/repos/apache/spark/issues/17360/comments,https://api.github.com/repos/apache/spark/issues/17360/events,https://github.com/apache/spark/pull/17360,215375564,MDExOlB1bGxSZXF1ZXN0MTExNTIwMTQ4,17360,[SPARK-20029][ML] ML LinearRegression supports bound constrained optimization.,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2017-03-20T10:12:57Z,2019-09-16T19:32:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
MLlib ```LinearRegression``` should support bound constrained optimization. Users can add bound constraints to coefficients to make the solver produce solution in the specified range.
Under the hood, we call breeze [```L-BFGS-B```](https://github.com/scalanlp/breeze/blob/master/math/src/main/scala/breeze/optimize/LBFGSB.scala) as the solver for bound constrained optimization. And we only support L2 regularization currently.

Todo:
- [ ] Support set bound for intercept.

## How was this patch tested?
Unit tests.
",spark,apache,yanboliang,1962026,MDQ6VXNlcjE5NjIwMjY=,https://avatars1.githubusercontent.com/u/1962026?v=4,,https://api.github.com/users/yanboliang,https://github.com/yanboliang,https://api.github.com/users/yanboliang/followers,https://api.github.com/users/yanboliang/following{/other_user},https://api.github.com/users/yanboliang/gists{/gist_id},https://api.github.com/users/yanboliang/starred{/owner}{/repo},https://api.github.com/users/yanboliang/subscriptions,https://api.github.com/users/yanboliang/orgs,https://api.github.com/users/yanboliang/repos,https://api.github.com/users/yanboliang/events{/privacy},https://api.github.com/users/yanboliang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17360,https://github.com/apache/spark/pull/17360,https://github.com/apache/spark/pull/17360.diff,https://github.com/apache/spark/pull/17360.patch
493,https://api.github.com/repos/apache/spark/issues/17332,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17332/labels{/name},https://api.github.com/repos/apache/spark/issues/17332/comments,https://api.github.com/repos/apache/spark/issues/17332/events,https://github.com/apache/spark/pull/17332,214936140,MDExOlB1bGxSZXF1ZXN0MTExMjMzMzk5,17332,[SPARK-10764][ML] Add optional caching to Pipelines,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2017-03-17T08:24:17Z,2019-09-16T18:28:20Z,,NONE,"
## What changes were proposed in this pull request?
This PR is to allow users to persist the output of a particular `PipelineStage` in the `Pipeline`. It also allows users to persist only a subset of output that is actually needed for the next stage instead of the whole big data frame.

For example, while running LDA a pipeline may look something like `Tokenizer` -> `StopWordRemover` -> `CountVectorizer` -> `LDA`. Here a user might want to `persist` the data frame coming out of `CountVectorizer` so that the LDA iterations work on this persisted data. Also, we need the flexibility to persist only the subset of columns (say docId, and features) that are needed for `LDA` rather than all extra columns that may have been added by the previous stages.

Also, another issue is that once the pipeline is fit on a data, for the `transform` on the same data it has to pass through all the previous stages again. In many cases this can be optimized if we already had access to data cached in the intermediate stage. In the example above, it will be useful if the data previously persisted in `CountVectorizer` can be reused to get `topicDistributions` from the `LDA` on the train data set itself. Otherwise, the data needs to *again* pass through all the stages.

For this, the `PipelineStage` exposes following methods
* `persist(level: StorageLevel, colExprs: String*)` - where the user can specify storage level and subset of the output dataframe of the stage to be persisted.
* `persist(colExprs: String*)` - uses default storage level.
* `getPersistedDf(): Option[DataFrame]` - to retrieve the cached data frame for a stage. This will be useful when we only need to run the pipeline from a specific stage. For instance, `pipelineModel.transform(countVectorizer.getPersistedDf.get)` should only run the final `LDA` stage of the pipeline. The persisted dataframe s only available once the `Pipeline.fit` has been called.

Changes in `Pipeline`
* The `fit` method checks whether a particular stage is marked to persist its output. If yes, it marks the output data frame as persistent according to the storage level and column subset. For `Executors` in the stage, it associates the persistant dataframe with both the `Executor` as well as the resultant `Transformer`.
* The `transformSchema` methods is changes so that the column pruning as result of specifying subset columns for persisting is taken into account.

Changes in `PipelineModel`
* `transform(dataset)` method now checks if the dataset argument was persisted in any of the stages during `Pipeline.fit`. If yes, it ensures only the stages after that stage are run (else all the stages are run). These two cases are handled for `transformSchema`.

## How was this patch tested?
* Existing tests
* Added new test in `PipelineSuite` to test above scenarios.
",spark,apache,sachintyagi22,10532956,MDQ6VXNlcjEwNTMyOTU2,https://avatars3.githubusercontent.com/u/10532956?v=4,,https://api.github.com/users/sachintyagi22,https://github.com/sachintyagi22,https://api.github.com/users/sachintyagi22/followers,https://api.github.com/users/sachintyagi22/following{/other_user},https://api.github.com/users/sachintyagi22/gists{/gist_id},https://api.github.com/users/sachintyagi22/starred{/owner}{/repo},https://api.github.com/users/sachintyagi22/subscriptions,https://api.github.com/users/sachintyagi22/orgs,https://api.github.com/users/sachintyagi22/repos,https://api.github.com/users/sachintyagi22/events{/privacy},https://api.github.com/users/sachintyagi22/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17332,https://github.com/apache/spark/pull/17332,https://github.com/apache/spark/pull/17332.diff,https://github.com/apache/spark/pull/17332.patch
494,https://api.github.com/repos/apache/spark/issues/17280,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17280/labels{/name},https://api.github.com/repos/apache/spark/issues/17280/comments,https://api.github.com/repos/apache/spark/issues/17280/events,https://github.com/apache/spark/pull/17280,213856636,MDExOlB1bGxSZXF1ZXN0MTEwNDY4Mzgw,17280,[SPARK-19939] [ML] Add support for association rules in ML,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2017-03-13T18:26:55Z,2019-10-23T17:54:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
jira: https://issues.apache.org/jira/browse/SPARK-19939
Adding another essential characteristic for the Association Rule in Spark ml.fpm.

Support is an indication of how frequently the itemset of an association rule appears in the database and suggests if the rules are generally applicable to the dateset. refer to [wiki](https://en.wikipedia.org/wiki/Association_rule_learning) for more details.

Before adding support:

 rules | confidence 
---------|----------------
 beer -> soda | 0.5 
 pecan -> milk | 0.6 

After adding support: 

 rules  | confidence | support 
-------------------|----------------|-----------
 beer -> soda | 0.5 | 0.3 
 pecan -> milk | 0.6 | 0.01 

Thus to allow a better understanding for the generated association rules. This is a new feature and was not included in the original function parity PR.

## How was this patch tested?

existing and new unit test
",spark,apache,hhbyyh,7981698,MDQ6VXNlcjc5ODE2OTg=,https://avatars3.githubusercontent.com/u/7981698?v=4,,https://api.github.com/users/hhbyyh,https://github.com/hhbyyh,https://api.github.com/users/hhbyyh/followers,https://api.github.com/users/hhbyyh/following{/other_user},https://api.github.com/users/hhbyyh/gists{/gist_id},https://api.github.com/users/hhbyyh/starred{/owner}{/repo},https://api.github.com/users/hhbyyh/subscriptions,https://api.github.com/users/hhbyyh/orgs,https://api.github.com/users/hhbyyh/repos,https://api.github.com/users/hhbyyh/events{/privacy},https://api.github.com/users/hhbyyh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17280,https://github.com/apache/spark/pull/17280,https://github.com/apache/spark/pull/17280.diff,https://github.com/apache/spark/pull/17280.patch
495,https://api.github.com/repos/apache/spark/issues/17234,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17234/labels{/name},https://api.github.com/repos/apache/spark/issues/17234/comments,https://api.github.com/repos/apache/spark/issues/17234/events,https://github.com/apache/spark/pull/17234,213248405,MDExOlB1bGxSZXF1ZXN0MTEwMDYzNzc0,17234,[SPARK-19892][MLlib] Implement findAnalogies method for Word2VecModel,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2017-03-10T05:29:49Z,2019-09-16T18:28:18Z,,NONE,"## What changes were proposed in this pull request?

Added findAnalogies method to Word2VecModel for performing vector-algebra-based queries (e.g. King + Woman - Man).

## How was this patch tested?

Followed the contributor's guide for Spark and ran the run-tests. Compiled and tested functionality in spark-shell.

This is an original work that I license to the project under the project's open source license.",spark,apache,benradford,1372632,MDQ6VXNlcjEzNzI2MzI=,https://avatars0.githubusercontent.com/u/1372632?v=4,,https://api.github.com/users/benradford,https://github.com/benradford,https://api.github.com/users/benradford/followers,https://api.github.com/users/benradford/following{/other_user},https://api.github.com/users/benradford/gists{/gist_id},https://api.github.com/users/benradford/starred{/owner}{/repo},https://api.github.com/users/benradford/subscriptions,https://api.github.com/users/benradford/orgs,https://api.github.com/users/benradford/repos,https://api.github.com/users/benradford/events{/privacy},https://api.github.com/users/benradford/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17234,https://github.com/apache/spark/pull/17234,https://github.com/apache/spark/pull/17234.diff,https://github.com/apache/spark/pull/17234.patch
496,https://api.github.com/repos/apache/spark/issues/17174,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17174/labels{/name},https://api.github.com/repos/apache/spark/issues/17174/comments,https://api.github.com/repos/apache/spark/issues/17174/events,https://github.com/apache/spark/pull/17174,212045275,MDExOlB1bGxSZXF1ZXN0MTA5MjA0MjAx,17174,[SPARK-19145][SQL] Timestamp to String casting is slowing the query s‚Ä¶,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2017-03-06T07:39:55Z,2019-09-16T18:28:16Z,,NONE,"‚Ä¶ignificantly

If BinaryComparison has expression with timestamp and string datatype then cast string to timestamp if string type expression is foldable. This results in order of magnitude performance improvement in query execution

## What changes were proposed in this pull request?
If BinaryComparison has expression with timestamp and string datatype then cast string to timestamp if string type expression is foldable. This results in order of magnitude performance improvement in query execution

## How was this patch tested?

Added new unit tests to conver the functionality

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,tanejagagan,8993562,MDQ6VXNlcjg5OTM1NjI=,https://avatars2.githubusercontent.com/u/8993562?v=4,,https://api.github.com/users/tanejagagan,https://github.com/tanejagagan,https://api.github.com/users/tanejagagan/followers,https://api.github.com/users/tanejagagan/following{/other_user},https://api.github.com/users/tanejagagan/gists{/gist_id},https://api.github.com/users/tanejagagan/starred{/owner}{/repo},https://api.github.com/users/tanejagagan/subscriptions,https://api.github.com/users/tanejagagan/orgs,https://api.github.com/users/tanejagagan/repos,https://api.github.com/users/tanejagagan/events{/privacy},https://api.github.com/users/tanejagagan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17174,https://github.com/apache/spark/pull/17174,https://github.com/apache/spark/pull/17174.diff,https://github.com/apache/spark/pull/17174.patch
497,https://api.github.com/repos/apache/spark/issues/17123,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17123/labels{/name},https://api.github.com/repos/apache/spark/issues/17123/comments,https://api.github.com/repos/apache/spark/issues/17123/events,https://github.com/apache/spark/pull/17123,211114091,MDExOlB1bGxSZXF1ZXN0MTA4NTYwOTM2,17123,[SPARK-19781][ML] Handle NULLs as well as NaNs in Bucketizer when handleInvalid is on,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2017-03-01T15:26:02Z,2019-09-16T18:28:15Z,,NONE,"## What changes were proposed in this pull request?

The original Bucketizer can put NaNs into a special bucket when handleInvalid is on. but leave NULLs untouched.
This PR unify behaviours of processing of NULLs and NaNs.

BTW, this is my first commit to Spark code. I'm not sure whether my code or the way of doing things is appropriate. Plz point it out if I'm doing anything wrong. :-)

## How was this patch tested?

new unit tests",spark,apache,crackcell,131737,MDQ6VXNlcjEzMTczNw==,https://avatars2.githubusercontent.com/u/131737?v=4,,https://api.github.com/users/crackcell,https://github.com/crackcell,https://api.github.com/users/crackcell/followers,https://api.github.com/users/crackcell/following{/other_user},https://api.github.com/users/crackcell/gists{/gist_id},https://api.github.com/users/crackcell/starred{/owner}{/repo},https://api.github.com/users/crackcell/subscriptions,https://api.github.com/users/crackcell/orgs,https://api.github.com/users/crackcell/repos,https://api.github.com/users/crackcell/events{/privacy},https://api.github.com/users/crackcell/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17123,https://github.com/apache/spark/pull/17123,https://github.com/apache/spark/pull/17123.diff,https://github.com/apache/spark/pull/17123.patch
498,https://api.github.com/repos/apache/spark/issues/17035,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17035/labels{/name},https://api.github.com/repos/apache/spark/issues/17035/comments,https://api.github.com/repos/apache/spark/issues/17035/events,https://github.com/apache/spark/pull/17035,209686449,MDExOlB1bGxSZXF1ZXN0MTA3NTcwNjgz,17035,[SPARK-19705][SQL] Preferred location supporting HDFS cache for FileS‚Ä¶,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2017-02-23T07:39:03Z,2019-09-16T18:28:13Z,,NONE,"‚Ä¶canRDD

Added support of HDFS cache using TaskLocation.inMemoryLocationTag
NewHadoopRDD and HadoopRDD both support HDFS cache using TaskLocation.inMemoryLocationTag
where ""hdfs_cache_"" is added to hostname which is then interpretted by scheduler
With this enhacement same tag (""hdfs_cache_"") will be added to hostname if FilePartition only contains single file and the file is cached on one or more host
Current implementation would not cased where FilePartition would have multiple files as preferredLocation calculation is more complex.

## What changes were proposed in this pull request?

Added support of HDFS cache using TaskLocation.inMemoryLocationTag
NewHadoopRDD and HadoopRDD both support HDFS cache using TaskLocation.inMemoryLocationTag
where ""hdfs_cache_"" is added to hostname which is then interpretted by scheduler
With this enhacement same tag (""hdfs_cache_"") will be added to hostname if FilePartition only contains single file and the file is cached on one or more host
Current implementation would not work where FilePartition would have multiple files as preferredLocation calculation is more complex.

## How was this patch tested?

Add unit tests at FileSourceStrategySuite.scala 

Please review http://spark.apache.org/contributing.html before opening a pull request.
",spark,apache,tanejagagan,8993562,MDQ6VXNlcjg5OTM1NjI=,https://avatars2.githubusercontent.com/u/8993562?v=4,,https://api.github.com/users/tanejagagan,https://github.com/tanejagagan,https://api.github.com/users/tanejagagan/followers,https://api.github.com/users/tanejagagan/following{/other_user},https://api.github.com/users/tanejagagan/gists{/gist_id},https://api.github.com/users/tanejagagan/starred{/owner}{/repo},https://api.github.com/users/tanejagagan/subscriptions,https://api.github.com/users/tanejagagan/orgs,https://api.github.com/users/tanejagagan/repos,https://api.github.com/users/tanejagagan/events{/privacy},https://api.github.com/users/tanejagagan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17035,https://github.com/apache/spark/pull/17035,https://github.com/apache/spark/pull/17035.diff,https://github.com/apache/spark/pull/17035.patch
499,https://api.github.com/repos/apache/spark/issues/17000,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/17000/labels{/name},https://api.github.com/repos/apache/spark/issues/17000/comments,https://api.github.com/repos/apache/spark/issues/17000/events,https://github.com/apache/spark/pull/17000,208900158,MDExOlB1bGxSZXF1ZXN0MTA3MDE5Nzc3,17000,[SPARK-18946][ML] sliceAggregate which is a new aggregate operator for high-dimensional data,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2017-02-20T14:47:03Z,2019-11-02T21:52:04Z,,NONE,"In many machine learning cases, driver has to aggregate high-dimensional vectors/arrays from executors.
TreeAggregate is good solution for aggregating vectors to driver, and you can increase depth of tree when data is large.
However, treeAggregate would still failed, when the parition number of RDD and the dimension of vector grows up.

We propose a new operator of RDD, named sliceAggregate, which split the vector into **_n_** slices and each slice is assigned a key(from 0 to **_n-1_**). The RDD[key, slice] will be transform to RDD[slice] by using reduceByKey operator.
Finally driver will collect and compose the **_n_** slices to obtain result.

![qq 20170220214746](https://cloud.githubusercontent.com/assets/4936059/23129541/0f462474-f7be-11e6-997e-e494b98eee69.png)


I run an experiment which calculate the statistic values of features.
The number of samples is 1000. The feature dimension ranges from 10k to 20m, the comparition of time cost between treeAggregate and sliceAggregate is shown as follows. When feature dimension reach 20 million, treeAggregate was failed.

The table of time cost(ms) between sliceAggregate and treeAggregate.

| feature dimension |	sliceAggregate |	treeAggregate |
| ---- | ---- |---- | 
| 10K |	617 | 	607 |
|100K |	1470  |     967  | 
|1M    |    4019 |	4731 |
|2.5M |	7679 |	13348 |
|5M  |	14722 |	22858 |
|7.5M |	20821 |	36472 |
|10M |	28526 |	50184 |
|20M | 	 47014 |  | |
 
![image](https://cloud.githubusercontent.com/assets/4936059/23129580/32e50260-f7be-11e6-9152-badc6423c356.png)

The code relate to this experiment is [here](https://github.com/ZunwenYou/spark/blob/slice-aggregate-experiment/mllib/src/main/scala/org/apache/spark/ml/classification/SliceAggregate.scala) .

JIRA Issue: https://issues.apache.org/jira/browse/SPARK-18946",spark,apache,ZunwenYou,4936059,MDQ6VXNlcjQ5MzYwNTk=,https://avatars3.githubusercontent.com/u/4936059?v=4,,https://api.github.com/users/ZunwenYou,https://github.com/ZunwenYou,https://api.github.com/users/ZunwenYou/followers,https://api.github.com/users/ZunwenYou/following{/other_user},https://api.github.com/users/ZunwenYou/gists{/gist_id},https://api.github.com/users/ZunwenYou/starred{/owner}{/repo},https://api.github.com/users/ZunwenYou/subscriptions,https://api.github.com/users/ZunwenYou/orgs,https://api.github.com/users/ZunwenYou/repos,https://api.github.com/users/ZunwenYou/events{/privacy},https://api.github.com/users/ZunwenYou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/17000,https://github.com/apache/spark/pull/17000,https://github.com/apache/spark/pull/17000.diff,https://github.com/apache/spark/pull/17000.patch
500,https://api.github.com/repos/apache/spark/issues/16966,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/16966/labels{/name},https://api.github.com/repos/apache/spark/issues/16966/comments,https://api.github.com/repos/apache/spark/issues/16966/events,https://github.com/apache/spark/pull/16966,208267533,MDExOlB1bGxSZXF1ZXN0MTA2NjI4MTc0,16966,[SPARK-18409][ML]LSH approxNearestNeighbors should use approxQuantile instead of sort,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2017-02-16T22:26:46Z,2019-09-18T08:48:57Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
In previous implementation of LSH approxNearestNeighbors, we have used sorting to get hashThreshold. By moving to approxQuantile, we can get as good results as the sort-based implementation while improving the running time.

## How was this patch tested?
By running unit tests BucketedRandomProjectionLSHSuite and MinHashLSHSuite

Also I tested on WEX dataset following [this doc](https://docs.google.com/document/d/19BXg-67U83NVB3M0I84HVBVg3baAVaESD_mrg_-vLro/edit). Concretely, I use 2 executors to get 40 neighbors from a 2.8GB dataset with the numberHashTables set to 3. The results are as follows:

|   | Time | Result |
|  ------ | ------ | ------ |
|  Sort-based Multi-Probe | 16.85s | 40 rows |
|  Quantile-based Multi-Probe (Error = 0.05) | 12.38s | same as sort-based (40 rows) |
|  Quantile-based Multi-Probe (Error = 0.25) | 11.18s | same as single probe (14 rows) |
|  Single-probe | 3.25s | 14 rows |",spark,apache,Yunni,6089987,MDQ6VXNlcjYwODk5ODc=,https://avatars1.githubusercontent.com/u/6089987?v=4,,https://api.github.com/users/Yunni,https://github.com/Yunni,https://api.github.com/users/Yunni/followers,https://api.github.com/users/Yunni/following{/other_user},https://api.github.com/users/Yunni/gists{/gist_id},https://api.github.com/users/Yunni/starred{/owner}{/repo},https://api.github.com/users/Yunni/subscriptions,https://api.github.com/users/Yunni/orgs,https://api.github.com/users/Yunni/repos,https://api.github.com/users/Yunni/events{/privacy},https://api.github.com/users/Yunni/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/16966,https://github.com/apache/spark/pull/16966,https://github.com/apache/spark/pull/16966.diff,https://github.com/apache/spark/pull/16966.patch
501,https://api.github.com/repos/apache/spark/issues/16755,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/16755/labels{/name},https://api.github.com/repos/apache/spark/issues/16755/comments,https://api.github.com/repos/apache/spark/issues/16755/events,https://github.com/apache/spark/pull/16755,204300988,MDExOlB1bGxSZXF1ZXN0MTAzOTM4Nzcw,16755,[SPARK-19606][MESOS] Support constraints in spark-dispatcher,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2017-01-31T13:48:13Z,2019-09-16T19:34:11Z,,CONTRIBUTOR,"The `MesosClusterScheduler` doesn't handle the `spark.mesos.constraints`
setting (as opposed to `MesosCoarseGrainedSchedulerBackend`).

## What changes were proposed in this pull request?

This commit introduces the necessary changes to handle the offer
constraints.

## How was this patch tested?

unit test",spark,apache,philipphoffmann,1279634,MDQ6VXNlcjEyNzk2MzQ=,https://avatars0.githubusercontent.com/u/1279634?v=4,,https://api.github.com/users/philipphoffmann,https://github.com/philipphoffmann,https://api.github.com/users/philipphoffmann/followers,https://api.github.com/users/philipphoffmann/following{/other_user},https://api.github.com/users/philipphoffmann/gists{/gist_id},https://api.github.com/users/philipphoffmann/starred{/owner}{/repo},https://api.github.com/users/philipphoffmann/subscriptions,https://api.github.com/users/philipphoffmann/orgs,https://api.github.com/users/philipphoffmann/repos,https://api.github.com/users/philipphoffmann/events{/privacy},https://api.github.com/users/philipphoffmann/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/16755,https://github.com/apache/spark/pull/16755,https://github.com/apache/spark/pull/16755.diff,https://github.com/apache/spark/pull/16755.patch
502,https://api.github.com/repos/apache/spark/issues/16478,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/16478/labels{/name},https://api.github.com/repos/apache/spark/issues/16478/comments,https://api.github.com/repos/apache/spark/issues/16478/events,https://github.com/apache/spark/pull/16478,198954012,MDExOlB1bGxSZXF1ZXN0MTAwMjg4NTk3,16478,[SPARK-7768][SQL] Revise user defined types (UDT),"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,50,2017-01-05T13:18:57Z,2019-10-02T15:40:11Z,,MEMBER,"## What changes were proposed in this pull request?

This patch is going to revise the current API for User Defined Types (UDTs).

Currently the API of `UserDefinedType` asks developers to implement the method that fills user data into SparkSQL's internal format such as `GenericInternalRow`, `UnsafeArrayData`, etc.

One target of this patch is to simplify the way to write UDTs. Developers can just use Scala types, instead of internal types such as ArrayData, when writing UDTs. `UserDefinedType` will use SparkSQL's encoder to convert user data to internal format.

For example, the following is the serialization method of `MatrixUDT`.

Before this patch:

    override def serialize(obj: Matrix): InternalRow = {
      val row = new GenericInternalRow(7)
      obj match {
        case sm: SparseMatrix =>
          row.setByte(0, 0)
          row.setInt(1, sm.numRows)
          row.setInt(2, sm.numCols)
          row.update(3, UnsafeArrayData.fromPrimitiveArray(sm.colPtrs))
          row.update(4, UnsafeArrayData.fromPrimitiveArray(sm.rowIndices))
          row.update(5, UnsafeArrayData.fromPrimitiveArray(sm.values))
          row.setBoolean(6, sm.isTransposed)

        case dm: DenseMatrix =>
          row.setByte(0, 1)
          row.setInt(1, dm.numRows)
          row.setInt(2, dm.numCols)
          row.setNullAt(3)
          row.setNullAt(4)
          row.update(5, UnsafeArrayData.fromPrimitiveArray(dm.values))
          row.setBoolean(6, dm.isTransposed)
      }
      row
    }


After this patch:

    override def writeRow(obj: Matrix): Row = {
      obj match {
        case sm: SparseMatrix =>
          Row(0.toByte, sm.numRows, sm.numCols, sm.colPtrs, sm.rowIndices, sm.values, sm.isTransposed)

        case dm: DenseMatrix =>
          Row(1.toByte, dm.numRows, dm.numCols, null, null, dm.values, dm.isTransposed)
      }
    }

Developers now manipulate external row `Row` and Scala types. Encoder will take care of converting the data to SparkSQL's internal format.


### Main API change

In the past, two main methods are needed to be implemented in developers' UDTs which extend `UserDefinedType`.

    /** Convert the user type to a SQL datum */
    def serialize(obj: UserType): Any

    /** Convert a SQL datum to the user type */
    def deserialize(datum: Any): UserType

Developers put/get the data of user class into/from an internal type of Spark SQL.

Now developers after this patch, are changed to implement two methods which put/get user data into/from an external row of SparkSQL.

    /** Convert the object of user type to an external row. Must be implemented in subclasses. */
    def writeRow(obj: UserType): Row

    /** Convert the external row to an object of user type. Must be implemented in subclasses. */
    def readRow(row: Row): UserType

### Benchmark

Ran a benchmark against previous `UserDefinedType`.

Code:

    private val random = new Random(100)
    private lazy val pointsRDD = (0 to 1000).map { i =>
      val features = (0 to 100).map { _ =>
        random.nextDouble()
      }
      MyLabeledPoint(i % 10, new UDT.MyDenseVector(features.toArray))
    }.toDF()

    test(""serialize MyLabeledPoint"") {
      val N = 10L << 5
      runBenchmark(""serialize MyLabeledPoint"", N) {
        pointsRDD.groupBy('label).agg(max('features)).collect()
      }
    }


Before this patch:

    Java HotSpot(TM) 64-Bit Server VM 1.8.0_102-b14 on Linux 4.4.39-moby
    Intel(R) Core(TM) i7-5557U CPU @ 3.10GHz
    serialize MyLabeledPoint:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
    ------------------------------------------------------------------------------------------------
    serialize MyLabeledPoint wholestage off        271 /  328          0.0      845767.7       1.0X
    serialize MyLabeledPoint wholestage on         131 /  261          0.0      408173.6       2.1X

After this patch:

    Java HotSpot(TM) 64-Bit Server VM 1.8.0_102-b14 on Linux 4.4.39-moby
    Intel(R) Core(TM) i7-5557U CPU @ 3.10GHz
    serialize MyLabeledPoint:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
    ------------------------------------------------------------------------------------------------
    serialize MyLabeledPoint wholestage off        221 /  331          0.0      692100.4       1.0X
    serialize MyLabeledPoint wholestage on         125 /  145          0.0      391120.1       1.8X

Basically, after this patch the serialization/deserialization of `UserDefinedType` can compete or be slightly better than previous one.

## How was this patch tested?

Existing Jenkins tests.

",spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/16478,https://github.com/apache/spark/pull/16478,https://github.com/apache/spark/pull/16478.diff,https://github.com/apache/spark/pull/16478.patch
503,https://api.github.com/repos/apache/spark/issues/16476,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/16476/labels{/name},https://api.github.com/repos/apache/spark/issues/16476/comments,https://api.github.com/repos/apache/spark/issues/16476/events,https://github.com/apache/spark/pull/16476,198927283,MDExOlB1bGxSZXF1ZXN0MTAwMjY5NDQz,16476,[SPARK-19084][SQL] Implement expression field ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2017-01-05T10:50:35Z,2019-09-16T18:27:59Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This is an implementation of expression `field` which is implemented as built-in function by Hive and MySQL.

field(expr, expr1, expr2, ... ) is a variable-length(>=2) function that returns the index of expr in (expr1, expr2, ...) list or 0 if not found.

 * It takes at least 2 parameters, and all parameters can be of any type.
 * Implicit cast will be done when at least 2 parameters have different types, and it will be based on the first parameter's type.
 * If the first parameter is of NumericType, all parameters will be implicitly cast to DoubleType, and those that can't be cast to DoubleType will be regarded as NULL.
 * If the first parameter is of any other type, all parameters will be implicitly cast to StringType and the comparison will follow String's comparing rules.
 * If the search expression is NULL, the return value is 0 because NULL fails equality comparison with any value.

## How was this patch tested?

Unit tests are in ConditionalExpressionSuite & ColumnExpressionSuite.",spark,apache,gczsjdy,7685352,MDQ6VXNlcjc2ODUzNTI=,https://avatars1.githubusercontent.com/u/7685352?v=4,,https://api.github.com/users/gczsjdy,https://github.com/gczsjdy,https://api.github.com/users/gczsjdy/followers,https://api.github.com/users/gczsjdy/following{/other_user},https://api.github.com/users/gczsjdy/gists{/gist_id},https://api.github.com/users/gczsjdy/starred{/owner}{/repo},https://api.github.com/users/gczsjdy/subscriptions,https://api.github.com/users/gczsjdy/orgs,https://api.github.com/users/gczsjdy/repos,https://api.github.com/users/gczsjdy/events{/privacy},https://api.github.com/users/gczsjdy/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/16476,https://github.com/apache/spark/pull/16476,https://github.com/apache/spark/pull/16476.diff,https://github.com/apache/spark/pull/16476.patch
504,https://api.github.com/repos/apache/spark/issues/16415,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/16415/labels{/name},https://api.github.com/repos/apache/spark/issues/16415/comments,https://api.github.com/repos/apache/spark/issues/16415/events,https://github.com/apache/spark/pull/16415,197658301,MDExOlB1bGxSZXF1ZXN0OTk0MTI3NzQ=,16415,[SPARK-19063][ML]Speedup and optimize the GradientBoostedTrees,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2016-12-27T07:11:25Z,2019-09-16T20:39:50Z,,NONE,"JIRA Issue: https://issues.apache.org/jira/browse/SPARK-19007

Speedup and optimize the GradientBoostedTrees in the ""data>memory"" scene  by persist the RDD named predError.

Test dataÔºö80G CTR training data from criteolabs(http://criteolabs.wpengine.com/downloads/download-terabyte-click-logs/ ) ,I used 1 of the 24 days' data.Some features needed to be repalced by new generated continuous featuresÔºåthe way to generate the new features refers to the way mentioned in the xgboost's paper.

Recource allocated: spark on yarn, 20 executors, 8G memory and 2 cores per executor.
I tested the GradientBoostedTrees algorithm in mllib using 80G CTR data mentioned above.
It totally costs 1.5 hour, and i found many task failures after 6 or 7 GBT rounds later.Without these task failures and task retry it can be much faster ,which can save about half the time. I think it's caused by the RDD named predError in the while loop of the boost method at GradientBoostedTrees.scala,because the lineage of the RDD named predError is growing after every GBT round, and then it caused failures like this :

(ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 10.2 GB of 10 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.).

I tried to boosting spark.yarn.executor.memoryOverhead but the meomry it needed is too much (even increase half the memory can't solve the problem) so i think it's not a proper method.

Although it can set the predCheckpoint Interval smaller to cut the line of the lineage but it increases IO cost a lot.

I tried another way to solve this problem.I persisted the RDD named predError every round and use pre_predError to record the previous RDD and unpersist it because it's useless anymore.
Finally it costs about 45 min after i tried my method and no task failure occured and no more memeory added.

So when the data is much larger than memory, my little improvement can speedup the GradientBoostedTrees 1~2 times.",spark,apache,zdh2292390,13797111,MDQ6VXNlcjEzNzk3MTEx,https://avatars0.githubusercontent.com/u/13797111?v=4,,https://api.github.com/users/zdh2292390,https://github.com/zdh2292390,https://api.github.com/users/zdh2292390/followers,https://api.github.com/users/zdh2292390/following{/other_user},https://api.github.com/users/zdh2292390/gists{/gist_id},https://api.github.com/users/zdh2292390/starred{/owner}{/repo},https://api.github.com/users/zdh2292390/subscriptions,https://api.github.com/users/zdh2292390/orgs,https://api.github.com/users/zdh2292390/repos,https://api.github.com/users/zdh2292390/events{/privacy},https://api.github.com/users/zdh2292390/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/16415,https://github.com/apache/spark/pull/16415,https://github.com/apache/spark/pull/16415.diff,https://github.com/apache/spark/pull/16415.patch
505,https://api.github.com/repos/apache/spark/issues/15970,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/15970/labels{/name},https://api.github.com/repos/apache/spark/issues/15970/comments,https://api.github.com/repos/apache/spark/issues/15970/events,https://github.com/apache/spark/pull/15970,190871045,MDExOlB1bGxSZXF1ZXN0OTQ3MjI1NTQ=,15970,[SPARK-18134][SQL] Comparable MapTypes [POC],"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2016-11-21T23:59:58Z,2019-09-16T19:34:11Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?
This is a small POC to see if we can make MapType orderable, and thus usable in aggregates and joins. The key idea in this PR is that there is a difference between an unordered and an ordered map (an ordered map is can be compared), and that `ordered` is a property of `MapType`.

A map can be converted from an unordered map to an ordered map by injecting a `SortMap` expression. The analyzer will inject `SortMap` expressions whenever we use a map in a binary comparison and when we use it in an aggregate. Note that the `SortMap` expression is far from optimized, it should however perform reasonable.

## How was this patch tested?
No tests yet. This will probably fail tests.",spark,apache,hvanhovell,9616802,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/15970,https://github.com/apache/spark/pull/15970,https://github.com/apache/spark/pull/15970.diff,https://github.com/apache/spark/pull/15970.patch
506,https://api.github.com/repos/apache/spark/issues/15496,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/15496/labels{/name},https://api.github.com/repos/apache/spark/issues/15496/comments,https://api.github.com/repos/apache/spark/issues/15496/events,https://github.com/apache/spark/pull/15496,183175265,MDExOlB1bGxSZXF1ZXN0ODk0NzMyNzk=,15496,[SPARK-17950] [Python] Match SparseVector behavior with DenseVector,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406590181, 'node_id': 'MDU6TGFiZWwxNDA2NTkwMTgx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYSPARK', 'name': 'PYSPARK', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2016-10-15T00:07:29Z,2019-09-16T18:27:04Z,,NONE,"## What changes were proposed in this pull request?

Simply added the `__getattr__` to SparseVector that DenseVector has, but calls to a SciPy sparse representation instead of storing a vector all the time in self.array

This allows for use of functions on the values of an entire SparseVector in the same direct way that users interact with DenseVectors.
 i.e. you can simply call SparseVector.mean() to average the values in the entire vector.

Note: The functions do have a slight bit of variance due to calling SciPy and not NumPy. However, the majority of useful functions (sums, means, max, etc.) are available to both packages anyways.
## How was this patch tested?

Manual testing on local machine.
Passed ./python/run-tests
No UI changes.
",spark,apache,itg-abby,8840022,MDQ6VXNlcjg4NDAwMjI=,https://avatars1.githubusercontent.com/u/8840022?v=4,,https://api.github.com/users/itg-abby,https://github.com/itg-abby,https://api.github.com/users/itg-abby/followers,https://api.github.com/users/itg-abby/following{/other_user},https://api.github.com/users/itg-abby/gists{/gist_id},https://api.github.com/users/itg-abby/starred{/owner}{/repo},https://api.github.com/users/itg-abby/subscriptions,https://api.github.com/users/itg-abby/orgs,https://api.github.com/users/itg-abby/repos,https://api.github.com/users/itg-abby/events{/privacy},https://api.github.com/users/itg-abby/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/15496,https://github.com/apache/spark/pull/15496,https://github.com/apache/spark/pull/15496.diff,https://github.com/apache/spark/pull/15496.patch
507,https://api.github.com/repos/apache/spark/issues/15326,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/15326/labels{/name},https://api.github.com/repos/apache/spark/issues/15326/comments,https://api.github.com/repos/apache/spark/issues/15326/events,https://github.com/apache/spark/pull/15326,180509570,MDExOlB1bGxSZXF1ZXN0ODc2MzI0OTU=,15326,[SPARK-17759] [CORE] Avoid adding duplicate schedulables,"[{'id': 1406606875, 'node_id': 'MDU6TGFiZWwxNDA2NjA2ODc1', 'url': 'https://api.github.com/repos/apache/spark/labels/SCHEDULER', 'name': 'SCHEDULER', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2016-10-02T12:17:14Z,2019-09-16T19:35:07Z,,MEMBER,"## What changes were proposed in this pull request?

If `spark.scheduler.allocation.file` has duplicate pools, all of them are created when `SparkContext` is initialized but just one of them is used and the other ones look **redundant**. This causes redundant pool creation and needs to be fixed. 

**Code to Reproduce** :

```
val conf = new SparkConf().setAppName(""spark-fairscheduler"").setMaster(""local"")
conf.set(""spark.scheduler.mode"", ""FAIR"")
conf.set(""spark.scheduler.allocation.file"", ""src/main/resources/fairscheduler-duplicate-pools.xml"")
val sc = new SparkContext(conf)
```

**fairscheduler-duplicate-pools.xml** :

The following sample just shows two **default** and **duplicate_pool1** but this also needs to be thought for **N** default and/or other duplicate pools.

```
<allocations>
    <pool name=""default"">
        <minShare>0</minShare>
        <weight>1</weight>
        <schedulingMode>FIFO</schedulingMode>
    </pool>
    <pool name=""default"">
        <minShare>0</minShare>
        <weight>1</weight>
        <schedulingMode>FIFO</schedulingMode>
    </pool>
    <pool name=""duplicate_pool1"">
        <minShare>1</minShare>
        <weight>1</weight>
        <schedulingMode>FAIR</schedulingMode>
    </pool>
    <pool name=""duplicate_pool1"">
        <minShare>2</minShare>
        <weight>2</weight>
        <schedulingMode>FAIR</schedulingMode>
    </pool>
</allocations>
```

**Debug Screenshot** :
The following screenshots show `Pool.schedulableQueue(ConcurrentLinkedQueue[Schedulable])` has **4** pools as 

> default, default, duplicate_pool1 and duplicate_pool1

 but `Pool.schedulableNameToSchedulable(ConcurrentHashMap[String, Schedulable])` has 

> default and duplicate_pool1

due to pool name as key so one of **default** and **duplicate_pool1** look **redundant** and live in `Pool.schedulableQueue`.

<img width=""250"" alt=""duplicate_pools"" src=""https://cloud.githubusercontent.com/assets/1437738/19020475/994fbcfc-88a1-11e6-9d11-102023461d3d.png"">
<img width=""250"" alt=""duplicate_pools2"" src=""https://cloud.githubusercontent.com/assets/1437738/19020476/9969e4d8-88a1-11e6-9732-1e91f942c570.png"">
## How was this patch tested?

Added new Unit Test case.
",spark,apache,erenavsarogullari,1437738,MDQ6VXNlcjE0Mzc3Mzg=,https://avatars0.githubusercontent.com/u/1437738?v=4,,https://api.github.com/users/erenavsarogullari,https://github.com/erenavsarogullari,https://api.github.com/users/erenavsarogullari/followers,https://api.github.com/users/erenavsarogullari/following{/other_user},https://api.github.com/users/erenavsarogullari/gists{/gist_id},https://api.github.com/users/erenavsarogullari/starred{/owner}{/repo},https://api.github.com/users/erenavsarogullari/subscriptions,https://api.github.com/users/erenavsarogullari/orgs,https://api.github.com/users/erenavsarogullari/repos,https://api.github.com/users/erenavsarogullari/events{/privacy},https://api.github.com/users/erenavsarogullari/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/15326,https://github.com/apache/spark/pull/15326,https://github.com/apache/spark/pull/15326.diff,https://github.com/apache/spark/pull/15326.patch
508,https://api.github.com/repos/apache/spark/issues/15297,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/15297/labels{/name},https://api.github.com/repos/apache/spark/issues/15297/comments,https://api.github.com/repos/apache/spark/issues/15297/events,https://github.com/apache/spark/pull/15297,180025814,MDExOlB1bGxSZXF1ZXN0ODcyOTYzMDc=,15297,[SPARK-9862]Handling data skew,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,38,2016-09-29T11:43:41Z,2019-09-16T18:29:11Z,,NONE,"## What changes were proposed in this pull request?

   Generally reducer need fetch all mapper block to deal„ÄÇBut in data skew caseÔºåhuge data in one reducer, make the running time is too long, or Out Of Memory.  So we need design a new feature to handling skew data join.  Since a reduce to deal with the data is too large, then scattered to a number of reducer for processing.
In join case. A join B equal to A1 join B + A2 join B + ‚Ä¶. An join B.  So when in join case the data skew occur. Don‚Äôt pull all skew data into a reducer do as A join B, but pull one mapper data and whole data about other join side do  as A1 join B + A2 join B + ‚Ä¶. An join B.

In spark sql, we use ShuffedRowRDD to fetch the map side generate date. ShuffedRowRDD only can read  a range of reduce IDs but fetches from all maps. Join operater generate two ShuffedRowRDD, them has the same partition num, and put the same partition id together to join. The ShuffedRowRDD partition  like this:
ShuffledRowRDD 1: [1,3,5,9]
ShuffledRowRDD 2: [1,3,5,9]

For handling the data skew case. We design a new RDD to read one mapper , we call it SkewShuffleRowRDD. The SkewShuffleRowRDD function getPartitions can generate partition to describe read one or a range of mappers data or read all mappers data.
The the two SkewShuffleRowRDD partition like this:
SkewShuffleRowRDD 1Ôºö
[(1, read all maps),(3, read map 1 to 3),(3,read map 3 to 5),(3,read map 5 to 7),(5, read all maps),(9,read all maps)]
SkewShuffleRowRDD 2Ôºö
[(1, read all maps),(3, read all maps),(3, read all maps),(3, read all maps),(5, read all maps),(9,read all maps)]

For determine which partition is data skew,  add a new configure parameter : spark.sql.adaptive.skewjoin.threshold.  when the skew threshold  > map out size. Determine the partition is data skew.
For achieve this feature, need add some code in ExchangeCoordinator.scala  to aware the skew partition. If is join operater and find some partition is skew, then generate the SkewShuffleRowRDD instead ShuffleRowRDD
## How was this patch tested?

Unit tests in ExchangeCoordinatorSuite

also can generate skew data and  manual test

Author: wangyuhuwangyuhu2002@126.com
",spark,apache,YuhuWang2002,21324650,MDQ6VXNlcjIxMzI0NjUw,https://avatars1.githubusercontent.com/u/21324650?v=4,,https://api.github.com/users/YuhuWang2002,https://github.com/YuhuWang2002,https://api.github.com/users/YuhuWang2002/followers,https://api.github.com/users/YuhuWang2002/following{/other_user},https://api.github.com/users/YuhuWang2002/gists{/gist_id},https://api.github.com/users/YuhuWang2002/starred{/owner}{/repo},https://api.github.com/users/YuhuWang2002/subscriptions,https://api.github.com/users/YuhuWang2002/orgs,https://api.github.com/users/YuhuWang2002/repos,https://api.github.com/users/YuhuWang2002/events{/privacy},https://api.github.com/users/YuhuWang2002/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/15297,https://github.com/apache/spark/pull/15297,https://github.com/apache/spark/pull/15297.diff,https://github.com/apache/spark/pull/15297.patch
509,https://api.github.com/repos/apache/spark/issues/14936,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/14936/labels{/name},https://api.github.com/repos/apache/spark/issues/14936/comments,https://api.github.com/repos/apache/spark/issues/14936/events,https://github.com/apache/spark/pull/14936,174743447,MDExOlB1bGxSZXF1ZXN0ODM3ODk5MjY=,14936,[SPARK-7877][MESOS] Allow configuration of framework timeout,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2016-09-02T11:59:26Z,2019-09-16T19:35:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This commit introduces a setting for configuring the Mesos framework failover timeout (`spark.mesos.failoverTimeout`). The default timeout is 10 seconds.

Before, the timeout was set to 0 (which causes all tasks to be killed by mesos) or Integer.MAX which kindof leaves them hanging forever, depending on how the driver was launched.
## How was this patch tested?

unit test
",spark,apache,philipphoffmann,1279634,MDQ6VXNlcjEyNzk2MzQ=,https://avatars0.githubusercontent.com/u/1279634?v=4,,https://api.github.com/users/philipphoffmann,https://github.com/philipphoffmann,https://api.github.com/users/philipphoffmann/followers,https://api.github.com/users/philipphoffmann/following{/other_user},https://api.github.com/users/philipphoffmann/gists{/gist_id},https://api.github.com/users/philipphoffmann/starred{/owner}{/repo},https://api.github.com/users/philipphoffmann/subscriptions,https://api.github.com/users/philipphoffmann/orgs,https://api.github.com/users/philipphoffmann/repos,https://api.github.com/users/philipphoffmann/events{/privacy},https://api.github.com/users/philipphoffmann/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/14936,https://github.com/apache/spark/pull/14936,https://github.com/apache/spark/pull/14936.diff,https://github.com/apache/spark/pull/14936.patch
510,https://api.github.com/repos/apache/spark/issues/14431,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/14431/labels{/name},https://api.github.com/repos/apache/spark/issues/14431/comments,https://api.github.com/repos/apache/spark/issues/14431/events,https://github.com/apache/spark/pull/14431,168548827,MDExOlB1bGxSZXF1ZXN0Nzk1MDIwNDc=,14431,[SPARK-16258][SparkR] Automatically append the grouping keys in SparkR's gapply,"[{'id': 1406606336, 'node_id': 'MDU6TGFiZWwxNDA2NjA2MzM2', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARKR', 'name': 'SPARKR', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2016-08-01T01:04:39Z,2019-09-16T19:37:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

The following pull request addresses the new feature request described in SPARK-16258.
It automatically('by default') appends grouping keys to output `DataFrame`.

I've also tried to solve the problem by adding an optional flag in `gapply` that states if the key is required or not. However, the optional flag needs to be passed as an argument through a number of methods which is not necessarily elegant and leads to some issues such as ""The number of parameters should not exceed 10"" in '..../logical/object.scala:290'

Since this pull request already appends the grouping key automatically, I was thinking if we really need to pass 'key' as R functions input argument - function(key, x) {....} Isn't it superfluous ?
I'd be happy to hear your thoughts on that.

Thanks!
## How was this patch tested?

Test cases in R.
",spark,apache,NarineK,12010601,MDQ6VXNlcjEyMDEwNjAx,https://avatars3.githubusercontent.com/u/12010601?v=4,,https://api.github.com/users/NarineK,https://github.com/NarineK,https://api.github.com/users/NarineK/followers,https://api.github.com/users/NarineK/following{/other_user},https://api.github.com/users/NarineK/gists{/gist_id},https://api.github.com/users/NarineK/starred{/owner}{/repo},https://api.github.com/users/NarineK/subscriptions,https://api.github.com/users/NarineK/orgs,https://api.github.com/users/NarineK/repos,https://api.github.com/users/NarineK/events{/privacy},https://api.github.com/users/NarineK/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/14431,https://github.com/apache/spark/pull/14431,https://github.com/apache/spark/pull/14431.diff,https://github.com/apache/spark/pull/14431.patch
511,https://api.github.com/repos/apache/spark/issues/14129,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/14129/labels{/name},https://api.github.com/repos/apache/spark/issues/14129/comments,https://api.github.com/repos/apache/spark/issues/14129/events,https://github.com/apache/spark/pull/14129,164746895,MDExOlB1bGxSZXF1ZXN0NzY4OTEwNTM=,14129,[SPARK-16280][SQL] Implement histogram_numeric SQL function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2016-07-11T01:56:50Z,2019-10-21T15:01:59Z,,NONE,"## What changes were proposed in this pull request?

This PR implements histogram_numeric according to the Algorithm I & II on the paper: Yael Ben-Haim and Elad Tom-Tov, ""A streaming parallel decision tree algorithm"", J. Machine Learning Research 11 (2010), pp. 849--872.
## How was this patch tested?

I add a test ""SPARK-16280: test histogram_numeric."" and ""SPARK-16280: skip null when computing histogram_numeric."" in DataFrameAggregateSuite.
",spark,apache,tilumi,734766,MDQ6VXNlcjczNDc2Ng==,https://avatars2.githubusercontent.com/u/734766?v=4,,https://api.github.com/users/tilumi,https://github.com/tilumi,https://api.github.com/users/tilumi/followers,https://api.github.com/users/tilumi/following{/other_user},https://api.github.com/users/tilumi/gists{/gist_id},https://api.github.com/users/tilumi/starred{/owner}{/repo},https://api.github.com/users/tilumi/subscriptions,https://api.github.com/users/tilumi/orgs,https://api.github.com/users/tilumi/repos,https://api.github.com/users/tilumi/events{/privacy},https://api.github.com/users/tilumi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/14129,https://github.com/apache/spark/pull/14129,https://github.com/apache/spark/pull/14129.diff,https://github.com/apache/spark/pull/14129.patch
512,https://api.github.com/repos/apache/spark/issues/13650,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/13650/labels{/name},https://api.github.com/repos/apache/spark/issues/13650/comments,https://api.github.com/repos/apache/spark/issues/13650/events,https://github.com/apache/spark/pull/13650,160050595,MDExOlB1bGxSZXF1ZXN0NzM2NDczOTA=,13650,[SPARK-9623] [ML] Provide conditional variance for RandomForestRegressor,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2016-06-13T21:36:13Z,2019-09-16T19:37:07Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

It is useful to get the variance of predictions from the `RandomForestRegressor` to plot confidence intervals on the predictions. I verified the formula from page 17 of this paper (http://arxiv.org/pdf/1211.0906v2.pdf)
The returned dataframe now has a column `variance` that can be set by `setVarianceCol` that calculates the variance.

The variance is calculated by summing up the mean of the variance of each decision tree + mean of the prediction^2 of each tree - square(mean of the prediction of each tree)

This pull request is highly inspired from https://github.com/apache/spark/pull/8866/files
## How was this patch tested?

I added a couple of tests to the RandomForestRegression test suite.
",spark,apache,MechCoder,1867024,MDQ6VXNlcjE4NjcwMjQ=,https://avatars3.githubusercontent.com/u/1867024?v=4,,https://api.github.com/users/MechCoder,https://github.com/MechCoder,https://api.github.com/users/MechCoder/followers,https://api.github.com/users/MechCoder/following{/other_user},https://api.github.com/users/MechCoder/gists{/gist_id},https://api.github.com/users/MechCoder/starred{/owner}{/repo},https://api.github.com/users/MechCoder/subscriptions,https://api.github.com/users/MechCoder/orgs,https://api.github.com/users/MechCoder/repos,https://api.github.com/users/MechCoder/events{/privacy},https://api.github.com/users/MechCoder/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/13650,https://github.com/apache/spark/pull/13650,https://github.com/apache/spark/pull/13650.diff,https://github.com/apache/spark/pull/13650.patch
513,https://api.github.com/repos/apache/spark/issues/13440,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/13440/labels{/name},https://api.github.com/repos/apache/spark/issues/13440/comments,https://api.github.com/repos/apache/spark/issues/13440/events,https://github.com/apache/spark/pull/13440,157939274,MDExOlB1bGxSZXF1ZXN0NzIyMDM1MDU=,13440,[SPARK-15699] [ML] Implement a Chi-Squared test statistic option for measuring split quality,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,51,2016-06-01T15:38:29Z,2019-09-16T19:37:06Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

Using test statistics as a measure of decision tree split quality is a useful split halting measure that can yield improved model quality. I am proposing to add the chi-squared test statistic as a new impurity option (in addition to ""gini"" and ""entropy"") for classification decision trees and ensembles.

https://issues.apache.org/jira/browse/SPARK-15699

http://erikerlandson.github.io/blog/2016/05/26/measuring-decision-tree-split-quality-with-test-statistic-p-values/
## How was this patch tested?

I added unit testing to verify that the chi-squared ""impurity"" measure functions as expected when used for decision tree training.
",spark,apache,erikerlandson,259898,MDQ6VXNlcjI1OTg5OA==,https://avatars0.githubusercontent.com/u/259898?v=4,,https://api.github.com/users/erikerlandson,https://github.com/erikerlandson,https://api.github.com/users/erikerlandson/followers,https://api.github.com/users/erikerlandson/following{/other_user},https://api.github.com/users/erikerlandson/gists{/gist_id},https://api.github.com/users/erikerlandson/starred{/owner}{/repo},https://api.github.com/users/erikerlandson/subscriptions,https://api.github.com/users/erikerlandson/orgs,https://api.github.com/users/erikerlandson/repos,https://api.github.com/users/erikerlandson/events{/privacy},https://api.github.com/users/erikerlandson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/13440,https://github.com/apache/spark/pull/13440,https://github.com/apache/spark/pull/13440.diff,https://github.com/apache/spark/pull/13440.patch
514,https://api.github.com/repos/apache/spark/issues/13379,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/13379/labels{/name},https://api.github.com/repos/apache/spark/issues/13379/comments,https://api.github.com/repos/apache/spark/issues/13379/events,https://github.com/apache/spark/pull/13379,157340802,MDExOlB1bGxSZXF1ZXN0NzE4MDMwNTk=,13379,[SPARK-12431][GraphX] Add local checkpointing to GraphX.,"[{'id': 1406607243, 'node_id': 'MDU6TGFiZWwxNDA2NjA3MjQz', 'url': 'https://api.github.com/repos/apache/spark/labels/GRAPHX', 'name': 'GRAPHX', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2016-05-28T14:18:02Z,2019-09-16T18:28:50Z,,NONE,"## What changes were proposed in this pull request?

Local checkpointing was added to RDD to speed up iterative spark jobs, but this capability hasn't been added to GraphX.
## How was this patch tested?

Manually tested.
",spark,apache,adeandrade,313191,MDQ6VXNlcjMxMzE5MQ==,https://avatars0.githubusercontent.com/u/313191?v=4,,https://api.github.com/users/adeandrade,https://github.com/adeandrade,https://api.github.com/users/adeandrade/followers,https://api.github.com/users/adeandrade/following{/other_user},https://api.github.com/users/adeandrade/gists{/gist_id},https://api.github.com/users/adeandrade/starred{/owner}{/repo},https://api.github.com/users/adeandrade/subscriptions,https://api.github.com/users/adeandrade/orgs,https://api.github.com/users/adeandrade/repos,https://api.github.com/users/adeandrade/events{/privacy},https://api.github.com/users/adeandrade/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/13379,https://github.com/apache/spark/pull/13379,https://github.com/apache/spark/pull/13379.diff,https://github.com/apache/spark/pull/13379.patch
515,https://api.github.com/repos/apache/spark/issues/13326,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/13326/labels{/name},https://api.github.com/repos/apache/spark/issues/13326/comments,https://api.github.com/repos/apache/spark/issues/13326/events,https://github.com/apache/spark/pull/13326,156950690,MDExOlB1bGxSZXF1ZXN0NzE1MzE4MTY=,13326,[SPARK-15560] [Mesos] Queued/Supervise drivers waiting for retry drivers disappear for kill command in Mesos mode,"[{'id': 1406625202, 'node_id': 'MDU6TGFiZWwxNDA2NjI1MjAy', 'url': 'https://api.github.com/repos/apache/spark/labels/MESOS', 'name': 'MESOS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2016-05-26T10:16:11Z,2019-09-27T18:44:21Z,,NONE,"## What changes were proposed in this pull request?

With the patch, it moves the drivers from Queued Drivers/Supervise drivers waiting for retry section to Finished Drivers section when they get killed.
## How was this patch tested?

I have verified it manually by checking the Mesos Dispatcher UI while simulating this scenario.
",spark,apache,devaraj-kavali,3174804,MDQ6VXNlcjMxNzQ4MDQ=,https://avatars0.githubusercontent.com/u/3174804?v=4,,https://api.github.com/users/devaraj-kavali,https://github.com/devaraj-kavali,https://api.github.com/users/devaraj-kavali/followers,https://api.github.com/users/devaraj-kavali/following{/other_user},https://api.github.com/users/devaraj-kavali/gists{/gist_id},https://api.github.com/users/devaraj-kavali/starred{/owner}{/repo},https://api.github.com/users/devaraj-kavali/subscriptions,https://api.github.com/users/devaraj-kavali/orgs,https://api.github.com/users/devaraj-kavali/repos,https://api.github.com/users/devaraj-kavali/events{/privacy},https://api.github.com/users/devaraj-kavali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/13326,https://github.com/apache/spark/pull/13326,https://github.com/apache/spark/pull/13326.diff,https://github.com/apache/spark/pull/13326.patch
516,https://api.github.com/repos/apache/spark/issues/12302,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/12302/labels{/name},https://api.github.com/repos/apache/spark/issues/12302/comments,https://api.github.com/repos/apache/spark/issues/12302/events,https://github.com/apache/spark/pull/12302,147495961,MDExOlB1bGxSZXF1ZXN0NjYwMzA0Mzk=,12302,[SPARK-14390][GraphX] Make initialization step in Pregel optional.,"[{'id': 1406607243, 'node_id': 'MDU6TGFiZWwxNDA2NjA3MjQz', 'url': 'https://api.github.com/repos/apache/spark/labels/GRAPHX', 'name': 'GRAPHX', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2016-04-11T17:22:49Z,2019-11-11T23:30:05Z,,NONE,"## What changes were proposed in this pull request?

Suppose a sendMsg function depends on the state of a edge's vertices to send messages, and those Pregel messages update such state. In this scenario, initialMsg will initially enforce the same message on all vertices, effectively removing a custom initialization one may have per vertex.

To deal with this situation, we must define a dummy initMsg (i.e. None), and all Pregel functions must be modified to handle this type of message. A simpler and less cumbersome solution is to make initMsg and the initialization step in Pregel optional.
## How was this patch tested?

No new tests were added. Some tests were updated. Previous functionality was kept.
",spark,apache,adeandrade,313191,MDQ6VXNlcjMxMzE5MQ==,https://avatars0.githubusercontent.com/u/313191?v=4,,https://api.github.com/users/adeandrade,https://github.com/adeandrade,https://api.github.com/users/adeandrade/followers,https://api.github.com/users/adeandrade/following{/other_user},https://api.github.com/users/adeandrade/gists{/gist_id},https://api.github.com/users/adeandrade/starred{/owner}{/repo},https://api.github.com/users/adeandrade/subscriptions,https://api.github.com/users/adeandrade/orgs,https://api.github.com/users/adeandrade/repos,https://api.github.com/users/adeandrade/events{/privacy},https://api.github.com/users/adeandrade/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/12302,https://github.com/apache/spark/pull/12302,https://github.com/apache/spark/pull/12302.diff,https://github.com/apache/spark/pull/12302.patch
517,https://api.github.com/repos/apache/spark/issues/11994,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/11994/labels{/name},https://api.github.com/repos/apache/spark/issues/11994/comments,https://api.github.com/repos/apache/spark/issues/11994/events,https://github.com/apache/spark/pull/11994,143926286,MDExOlB1bGxSZXF1ZXN0NjQzMDA4Mjc=,11994,[SPARK-14151][CORE] Expose metrics Source and Sink interface,"[{'id': 1405801482, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDgy', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20CORE', 'name': 'SPARK CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2016-03-28T08:27:31Z,2019-09-16T19:38:08Z,,CONTRIBUTOR,"## What changes were proposed in this pull request?

This patch tries to expose metrics Source and Sink interface for external developers, this will:
1. Let user to define their own Sink and Source implementation and plug into `MetricsSystem`.
2. Alleviate the maintenance overhead of Spark code base. Spark itself doesn't need to maintain several different sinks and sources.
## How was this patch tested?

Unit test is added and local integrated test is done.
",spark,apache,jerryshao,850797,MDQ6VXNlcjg1MDc5Nw==,https://avatars2.githubusercontent.com/u/850797?v=4,,https://api.github.com/users/jerryshao,https://github.com/jerryshao,https://api.github.com/users/jerryshao/followers,https://api.github.com/users/jerryshao/following{/other_user},https://api.github.com/users/jerryshao/gists{/gist_id},https://api.github.com/users/jerryshao/starred{/owner}{/repo},https://api.github.com/users/jerryshao/subscriptions,https://api.github.com/users/jerryshao/orgs,https://api.github.com/users/jerryshao/repos,https://api.github.com/users/jerryshao/events{/privacy},https://api.github.com/users/jerryshao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/11994,https://github.com/apache/spark/pull/11994,https://github.com/apache/spark/pull/11994.diff,https://github.com/apache/spark/pull/11994.patch
518,https://api.github.com/repos/apache/spark/issues/11336,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/11336/labels{/name},https://api.github.com/repos/apache/spark/issues/11336/comments,https://api.github.com/repos/apache/spark/issues/11336/events,https://github.com/apache/spark/pull/11336,135906733,MDExOlB1bGxSZXF1ZXN0NjA0MzM1NTE=,11336,[SPARK-9325][SPARK-R] head() and show() for Columns,"[{'id': 1406606336, 'node_id': 'MDU6TGFiZWwxNDA2NjA2MzM2', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARKR', 'name': 'SPARKR', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,85,2016-02-24T00:05:01Z,2019-09-16T19:38:08Z,,NONE,"# See attached design document

[SparkR collect (JIRA doc).pdf](https://github.com/apache/spark/files/143656/SparkR.collect.JIRA.doc.pdf)
",spark,apache,olarayej,13985649,MDQ6VXNlcjEzOTg1NjQ5,https://avatars1.githubusercontent.com/u/13985649?v=4,,https://api.github.com/users/olarayej,https://github.com/olarayej,https://api.github.com/users/olarayej/followers,https://api.github.com/users/olarayej/following{/other_user},https://api.github.com/users/olarayej/gists{/gist_id},https://api.github.com/users/olarayej/starred{/owner}{/repo},https://api.github.com/users/olarayej/subscriptions,https://api.github.com/users/olarayej/orgs,https://api.github.com/users/olarayej/repos,https://api.github.com/users/olarayej/events{/privacy},https://api.github.com/users/olarayej/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/11336,https://github.com/apache/spark/pull/11336,https://github.com/apache/spark/pull/11336.diff,https://github.com/apache/spark/pull/11336.patch
519,https://api.github.com/repos/apache/spark/issues/11218,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/11218/labels{/name},https://api.github.com/repos/apache/spark/issues/11218/comments,https://api.github.com/repos/apache/spark/issues/11218/events,https://github.com/apache/spark/pull/11218,134000059,MDExOlB1bGxSZXF1ZXN0NTk1MDA0NTE=,11218,"[SPARK-13340][ML] PolynomialExpansion, Normalizer and ElementwiseProduct should validate input type","[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2016-02-16T14:48:05Z,2019-09-16T18:29:50Z,,CONTRIBUTOR,,spark,apache,grzegorz-chilkiewicz,10269827,MDQ6VXNlcjEwMjY5ODI3,https://avatars3.githubusercontent.com/u/10269827?v=4,,https://api.github.com/users/grzegorz-chilkiewicz,https://github.com/grzegorz-chilkiewicz,https://api.github.com/users/grzegorz-chilkiewicz/followers,https://api.github.com/users/grzegorz-chilkiewicz/following{/other_user},https://api.github.com/users/grzegorz-chilkiewicz/gists{/gist_id},https://api.github.com/users/grzegorz-chilkiewicz/starred{/owner}{/repo},https://api.github.com/users/grzegorz-chilkiewicz/subscriptions,https://api.github.com/users/grzegorz-chilkiewicz/orgs,https://api.github.com/users/grzegorz-chilkiewicz/repos,https://api.github.com/users/grzegorz-chilkiewicz/events{/privacy},https://api.github.com/users/grzegorz-chilkiewicz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/11218,https://github.com/apache/spark/pull/11218,https://github.com/apache/spark/pull/11218.diff,https://github.com/apache/spark/pull/11218.patch
520,https://api.github.com/repos/apache/spark/issues/7842,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/7842/labels{/name},https://api.github.com/repos/apache/spark/issues/7842/comments,https://api.github.com/repos/apache/spark/issues/7842/events,https://github.com/apache/spark/pull/7842,98475595,MDExOlB1bGxSZXF1ZXN0NDEzODM2ODc=,7842,[SPARK-8542][MLlib]PMML export for Decision Trees,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,63,2015-07-31T21:40:17Z,2019-09-16T18:29:44Z,,NONE,"Added support for PMML export for Decision Trees
",spark,apache,JasmineGeorge,1110984,MDQ6VXNlcjExMTA5ODQ=,https://avatars0.githubusercontent.com/u/1110984?v=4,,https://api.github.com/users/JasmineGeorge,https://github.com/JasmineGeorge,https://api.github.com/users/JasmineGeorge/followers,https://api.github.com/users/JasmineGeorge/following{/other_user},https://api.github.com/users/JasmineGeorge/gists{/gist_id},https://api.github.com/users/JasmineGeorge/starred{/owner}{/repo},https://api.github.com/users/JasmineGeorge/subscriptions,https://api.github.com/users/JasmineGeorge/orgs,https://api.github.com/users/JasmineGeorge/repos,https://api.github.com/users/JasmineGeorge/events{/privacy},https://api.github.com/users/JasmineGeorge/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/7842,https://github.com/apache/spark/pull/7842,https://github.com/apache/spark/pull/7842.diff,https://github.com/apache/spark/pull/7842.patch
